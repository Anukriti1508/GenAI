{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Developing RAG system using Llama2 with Hugging Face"
      ],
      "metadata": {
        "id": "GQRsDF7kFJXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RFZmPiFFPP9",
        "outputId": "9548ea7a-de7c-4662-db28-3e3516004d40"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHu67aUOFY24",
        "outputId": "1a974948-4648-4d78-a203-95633da3abaf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m521.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Library for embeddings\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKr3wbAXHKGk",
        "outputId": "cebe55aa-79c6-4e5b-a638-dd9d259e0e42"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhlsxeDqH0K_",
        "outputId": "5d1bf393-4cd9-453b-83d1-9f4a8d718d5c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_index\n",
            "  Downloading llama_index-0.10.38-py3-none-any.whl (6.8 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama_index)\n",
            "  Downloading llama_index_agent_openai-0.2.5-py3-none-any.whl (13 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.38 (from llama_index)\n",
            "  Downloading llama_index_core-0.10.38.post1-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama_index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama_index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama_index)\n",
            "  Downloading llama_index_llms_openai-0.1.20-py3-none-any.whl (11 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama_index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl (5.8 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama_index)\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama_index)\n",
            "  Downloading llama_index_readers_file-0.1.22-py3-none-any.whl (36 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.3.0,>=0.1.4->llama_index)\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (0.6.6)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.38->llama_index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.38->llama_index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.38->llama_index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpath-ng (from llama-index-core<0.11.0,>=0.10.38->llama_index)\n",
            "  Downloading jsonpath_ng-1.6.1-py3-none-any.whl (29 kB)\n",
            "Collecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core<0.11.0,>=0.10.38->llama_index)\n",
            "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (2.31.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (3.7.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (8.3.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.38->llama_index)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (4.11.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.38->llama_index) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.2.0)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index)\n",
            "  Downloading llama_parse-0.4.3-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama_index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama_index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama_index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama_index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama_index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.38->llama_index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.38->llama_index) (2.7.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.38->llama_index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.38->llama_index) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.38->llama_index)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.38->llama_index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.38->llama_index) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.38->llama_index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.38->llama_index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.38->llama_index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.38->llama_index) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama_index) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.38->llama_index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.38->llama_index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.38->llama_index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.38->llama_index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.38->llama_index) (3.21.2)\n",
            "Collecting ply (from jsonpath-ng->llama-index-core<0.11.0,>=0.10.38->llama_index)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.38->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.38->llama_index) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.38->llama_index) (2024.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (6.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (3.4.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.38->llama_index) (1.2.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.38->llama_index) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.38->llama_index) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.38->llama_index) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->llama-index-core<0.11.0,>=0.10.38->llama_index) (1.1.1)\n",
            "Installing collected packages: striprtf, ply, dirtyjson, jsonpath-ng, h11, deprecated, tiktoken, httpcore, httpx, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
            "Successfully installed deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpath-ng-1.6.1 llama-index-agent-openai-0.2.5 llama-index-cli-0.1.12 llama-index-core-0.10.38.post1 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.20 llama-index-multi-modal-llms-openai-0.1.6 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.22 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.3 llama_index-0.10.38 llamaindex-py-client-0.1.19 openai-1.30.1 ply-3.11 striprtf-0.0.26 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-llms-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPQesCk_KEyk",
        "outputId": "c15f1d05-8dc7-4dd7-daec-c4ffc5ea3edb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-huggingface\n",
            "  Downloading llama_index_llms_huggingface-0.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.23.0)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.10.38.post1)\n",
            "Collecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface)\n",
            "  Downloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers[torch]<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (4.41.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.11.0)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.6.6)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-ng in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.6.1)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.30.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (9.4.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.7.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.3.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.14.1)\n",
            "Requirement already satisfied: pydantic<3,>2 in /usr/local/lib/python3.10/dist-packages (from text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.4.3)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.30.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (4.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.4.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.1.5)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.10/dist-packages (from jsonpath-ng->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2024.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (6.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.4.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.2.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.16.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.1.1)\n",
            "Installing collected packages: text-generation, llama-index-llms-huggingface\n",
            "Successfully installed llama-index-llms-huggingface-0.2.0 text-generation-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt"
      ],
      "metadata": {
        "id": "rz0jcAFsIBkW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs=SimpleDirectoryReader('/content/data').load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDXhyphKIoLT",
        "outputId": "70f02197-87a5-4e07-ad25-7543c7930d6a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf.generic._base:could not convert string to float: '0.00-39343833' : FloatObject (b'0.00-39343833') invalid; use 0.0 instead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVZXBhfTK9N2",
        "outputId": "f5bd289f-6295-44c7-f226-7e09e85a38a8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(id_='a559270c-5fff-4231-8749-3b5ec8c6fac6', embedding=None, metadata={'page_label': '1', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP\\nSHREYA GOYAL, Robert Bosch Centre for Data Science and AI, Indian Institute of Technology Madras, India\\nSUMANTH DODDAPANENI, Robert Bosch Centre for Data Science and AI, Indian Institute of Technology\\nMadras, India\\nMITESH M. KHAPRA, Robert Bosch Centre for Data Science and AI, Indian Institute of Technology Madras, India\\nBALARAMAN RAVINDRAN, Robert Bosch Centre for Data Science and AI, Indian Institute of Technology\\nMadras, India\\nIn the past few years, it has become increasingly evident that deep neural networks are not resilient enough to withstand adversarial\\nperturbations in input data, leaving them vulnerable to attack. Various authors have proposed strong adversarial attacks for computer\\nvision and Natural Language Processing (NLP) tasks. As a response, many defense mechanisms have also been proposed to prevent\\nthese networks from failing. The significance of defending neural networks against adversarial attacks lies in ensuring that the model’s\\npredictions remain unchanged even if the input data is perturbed. Several methods for adversarial defense in NLP have been proposed,\\ncatering to different NLP tasks such as text classification, named entity recognition, and natural language inference. Some of these\\nmethods not only defend neural networks against adversarial attacks but also act as a regularization mechanism during training,\\nsaving the model from overfitting. This survey aims to review the various methods proposed for adversarial defenses in NLP over the\\npast few years by introducing a novel taxonomy. The survey also highlights the fragility of advanced deep neural networks in NLP\\nand the challenges involved in defending them.\\nCCS Concepts: •Computer systems organization →Embedded systems ;Redundancy ; Robotics; •Networks→Network relia-\\nbility.\\nAdditional Key Words and Phrases: Adversarial attacks, Adversarial defenses, Perturbations, NLP\\nACM Reference Format:\\nShreya Goyal, Sumanth Doddapaneni, Mitesh M. Khapra, and Balaraman Ravindran . 2023. A Survey of Adversarial Defences and\\nRobustness in NLP. 1, 1 (April 2023), 43 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1 INTRODUCTION\\nRecently, there have been significant advancements in the field of Natural Lanaguage Processing (NLP) using deep\\nlearning algorithms. In fact, the proposed solutions for NLP have already surpassed human accuracy in some cases\\n[32,60,69]. By learning from vast amounts of available data, deep learning has revolutionized the field by providing a\\nrepresentation of language that can be utilized for a range of tasks. NLP involves the manipulation and processing of\\nhuman language, and deep neural networks have enabled NLP models to learn how to represent language and solve\\nAuthors’ addresses: Shreya Goyal, Robert Bosch Centre for Data Science and AI, Indian Institute of Technology Madras, Bhupat and Jyoti Mehta School\\nof Biosciences, Chennai, Tamil Nadu, India, 600036; Sumanth Doddapaneni, Robert Bosch Centre for Data Science and AI, Indian Institute of Technology\\nMadras, Bhupat and Jyoti Mehta School of Biosciences, Chennai, Tamil Nadu, India, 600036; Mitesh M. Khapra, Robert Bosch Centre for Data Science and\\nAI, Indian Institute of Technology Madras, Bhupat and Jyoti Mehta School of Biosciences, Chennai, Tamil Nadu, India, 600036; Balaraman Ravindran,\\nRobert Bosch Centre for Data Science and AI, Indian Institute of Technology Madras, Bhupat and Jyoti Mehta School of Biosciences, Chennai, Tamil\\nNadu, India, 600036.\\n©2023 Association for Computing Machinery.\\nManuscript submitted to ACM\\nManuscript submitted to ACM 1arXiv:2203.06414v4  [cs.CL]  18 Apr 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e1040fc9-ba4e-454e-a081-38e4a67941aa', embedding=None, metadata={'page_label': '2', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2 Shreya Goyal, et al.\\ntasks such as text classification, natural language inferencing, sentiment analysis, machine translation, named entity\\nrecognition, malware detection, reading comprehension, textual entailment with remarkable accuracy [ 60,96,128]. A\\ntypical NLP pipeline using deep neural networks learns word representation in text and contextual details in sentences,\\nand this type of language modeling can be utilized for tasks such as sentence classification, translation, and question\\nanswering, using Convolutional Neural Network (CNN) or Recurrent Neural Networks (RNN) based learning models.\\nFinding feature representations for natural language text is a crucial part of this pipeline, and various methods have been\\nproposed, including hand-crafted features and auto-encoded features using recent deep neural networks [68, 96, 128].\\nDespite the significant progress made, deep neural networks still suffer from a lack of interpretability and operate\\nas a \"black box\" [ 16]. Their high performance remains inexplicable, and there is limited understanding of how they\\nfunction [ 16,113]. Although they can achieve exceptional accuracy and human-like performance, these networks are\\nvulnerable to attacks and are highly sensitive to even the slightest perturbations in inputs, causing them to fail [ 2,32].\\nRecently, there has been a growing number of proposed adversarial attacks for deep neural networks in computer\\nvision and NLP, raising concerns about the robustness of these high-performing models [ 159,167]. Adversarial attacks\\ncan pose a significant security threat to applications such as malware and spam detection, as well as biometrics. In NLP,\\nthese attacks can take various forms, including substitution, insertion, deletion, and swapping of words/characters in a\\nsentence or finding a neighboring word embedding to introduce perturbations in the input [167].\\nThere are two main types of adversarial attacks, black box and white box, based on the attacker’s access to the model’s\\nparameters [ 167]. These attacks can be further categorized based on their design granularity, including character level,\\nword level, sentence level, and multi-level attacks [ 45,137,167]. Adversaries are generated by perturbing the input text\\nusing techniques such as insertion, deletion, flipping, swapping of characters or words, or paraphrasing the sentence in\\na way that preserves its original meaning but changes the wording. In white box attacks, the attacker has access to\\nthe model’s parameters and modifies the word embeddings of input text using gradient-based schemes. In contrast,\\nblack box attacks do not have access to the model’s parameters and generate a replica of the model by repeatedly\\nquerying the input and output. Once the parameters are acquired, they train a substitute model with perturbed data\\nand attack it [ 45,103,137,159,167]. Generating perturbations in textual data is more challenging than in images due to\\nthe discrete nature of the data [ 167]. The quality of adversarial examples generated for text data is determined by two\\nfactors, namely, the naturalness of the adversarial examples and the efficiency to generate these examples [ 71]. Some\\nresearchers have succeeded in detecting perturbations in text using simpler techniques like spell check and adversarial\\ntraining [ 102], while others who used word-level attacks failed to efficiently generate adversarial examples due to the\\nhigh-dimensional search space [ 160]. Thus, efficiently generating adversarial attacks in NLP poses unique challenges.\\nDespite the difficulty, stronger and imperceptible adversarial attacks have been proposed, which pose a significant\\nthreat to the security of deep neural networks [ 5,13]. Consequently, several defense mechanisms have been proposed\\nin recent years to counter adversarial attacks in NLP, and the considerable amount of work in adversarial defenses\\nhas provided good competition to the novel adversarial attack algorithms, substantially improving the robustness of\\nexisting deep learning models.\\nAdversarial defense strategies in NLP can be broadly classified into three categories: adversarial training-based,\\nperturbation control-based, and certification-based methods. The majority of the work in this field employs an adversarial\\ntraining approach and techniques are further subdivided based on the generation of adversarial instances or noise\\nin the defense pipeline. These methods include data augmentation-based adversarial training, adversarial training as\\na regularization technique, Generative Adversarial Network (GAN)-based adversarial training, Virtual Adversarial\\nTraining (VAT), and Human-In-The-Loop (HITL) approaches. Perturbation control-based methods are also categorized\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='730a8912-3ae6-48e9-beec-13984b1a738d', embedding=None, metadata={'page_label': '3', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 3\\ninto perturbation identification and correction and perturbation direction control. Certification-based techniques fall\\nunder the third category and provide certificates of robustness against adversarial attacks. A few methods do not fit\\ninto any of the aforementioned categories and are classified as miscellaneous. In the subsequent section, the objectives\\nof this survey paper are emphasized, distinguishing it from previous surveys.\\n1.1 Goals of this survey paper\\nIn this article, we reviewed numerous methods of adversarial defenses in NLP, proposed in recent years. The key goals\\nof this survey are as listed below:\\n•Providing a comprehensive review of adversarial defense schemes in NLP by covering schemes for different NLP\\ntasks and bringing the attention of the community to this emerging area.\\n•Proposing novel taxonomy for adversarial defense methods in NLP for various tasks.\\n•Accentuating the importance of defense methods for adversarial attacks and as a regularization scheme in deep\\nneural networks.\\n•Paving the path for future work in this area by highlighting the open issues.\\nNumerous survey papers have been published in the past discussing adversarial attacks on deep neural networks in\\nboth computer vision and NLP. For example, in [ 148], the authors conducted a comprehensive survey of adversarial\\nattacks on deep neural networks for images, text, and graphs, proposing a novel taxonomy to categorize a wide range of\\nmethods. Similarly, [ 2] proposed a taxonomy for different adversarial attacks and defenses on various computer vision\\nalgorithms, including image classification, image segmentation, object detection, robotic vision, and visual question\\nanswering. In addition, [ 97] presented a brief survey on general adversarial attack methods in deep learning, while [ 62]\\nbriefly reviewed attack and defense methods in images and text data. Furthermore, [ 17] and [ 11] discussed adversarial\\nattacks and defenses for various computer vision algorithms.\\nIn contrast to previous research, [ 164] proposed a novel taxonomy for universal adversarial attacks, which includes\\nuniversal perturbations for image classifiers, and briefly discussed attacks on text and audio classification models. While\\nthe previously discussed survey papers focused primarily on adversarial attacks on images and briefly discussed attack\\nalgorithms in NLP, [ 45,103,137,167] extensively reviewed adversarial attack algorithms for various NLP tasks while\\nbriefly discussing some defense methods. However, the importance of adversarial defense algorithms is self-evident,\\ngiven the large amount of work in this area in recent years. Therefore, this survey paper aims to address this gap and is\\ndifferent from previous survey papers in this area by focusing exclusively on adversarial defense methods in NLP. This\\npaper proposes a detailed and novel taxonomy for adversarial defense mechanisms in NLP, emphasizes the importance\\nof defense methods, and discusses open issues in this area while presenting future work to the community.\\nIn this paper, Section 2 discusses adversarial attacks in deep learning and categorizes adversarial attacks in NLP.\\nSection 3 presents a novel taxonomy for adversarial defense methods in NLP. Section 4 provides a detailed description\\nof adversarial training-based defenses in NLP, along with sub-categories. Section 5 discusses perturbation control-based\\nadversarial defense methods. Section 6 outlines certification-based adversarial defenses. Section 7 describes various\\nother adversarial defenses that do not fit into the previous categorization. Section 8 discusses different metrics used to\\nevaluate defense mechanisms. Section 9 describes the datasets and frameworks proposed for training and evaluating\\nadversarial defense methods. Section 10 provides suggestions for future research in adversarial defenses for NLP, and\\nfinally, Section 11 concludes the paper.\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='99e364ec-67c0-4f7f-ae73-e167177dc79b', embedding=None, metadata={'page_label': '4', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4 Shreya Goyal, et al.\\n2 A GENERAL OVERVIEW OF ADVERSARIAL ATTACKS\\nAn adversarial attack is a deliberate attempt to corrupt a deep neural network’s functionality by introducing distorted\\ninputs that cause the model to fail. These perturbations are designed to be subtle enough to evade human detection\\nbut effective enough to deceive a neural network. For instance, image classification models have been subjected to\\nexperiments with various input perturbations, including the addition of noise, the adjustment of pixels, the use of\\npatches, the addition of watermarks, and so on, which can go unnoticed by humans. In contrast, adversarial attacks\\nin NLP involve multiple proposed perturbations at the character, word, or sentence level through deletion, insertion,\\nswapping, flipping, use of synonyms, concatenation with characters or words, insertion of numeric or alphanumeric\\ncharacters, etc. However, it is more challenging to generate adversarial perturbations for text data than image data\\nbecause altering a character or word in a sentence is more perceptible to humans. Moreover, creating imperceptible\\nadversarial attacks is difficult in NLP since perturbations in textual data could result in less natural input data [ 71].\\nAdversarial attacks are classified into two categories based on motivation: targeted attacks and non-targeted attacks.\\nTargeted attacks aim to misclassify inputs to a specific class, while non-targeted attacks aim to push the classifier\\nboundary to cause the model to misclassify inputs. Based on access to the model’s parameters, adversarial attacks are\\nclassified as white-box and black-box attacks. In this section, we briefly review the state-of-the-art adversarial attacks\\nfor NLP tasks algorithms.\\n2.1 Adversarial attacks in deep learning\\nThe goal of an adversarial attack is to generate such perturbations for input xbelonging to class C1, such that, xis\\nwrongly classified to class C2with a high confidence value. For a multi-class classification algorithm, for 𝑘input classes\\n𝑖=𝐶1,𝐶2,...,𝐶𝑘, the perturbed input is x′andfiis the discriminant function which defines the classification boundaries,\\nwhere xbelongs to class Ci, and Ctarget is the target class after the attack, then:\\n𝑓𝑡𝑎𝑟𝑔𝑒𝑡(x′)>𝑓𝑖(x′) (1)\\nHence, adversarial attacks can be formally defined as an optimization problem for xas:\\nminimize𝑥(||𝑥′−𝑥||)\\n𝑠𝑢𝑏𝑗𝑒𝑐𝑡𝑡𝑜 max\\n𝑖≠𝑡𝑎𝑟𝑔𝑒𝑡{𝑓𝑖(x′)}−𝑓𝑡𝑎𝑟𝑔𝑒𝑡(x′)≤0(2)\\nThe inequality defined in 1 represents the goal of any adversarial attack which pushes the perturbed input x′to a\\ndesired target class, rather than its actual class. Hence Equation 2 defines the adversarial attack as an optimization\\nproblem, where the goal is to minimize the perturbation magnitude to make the perturbations less perceptible and\\nmake sure it gets classified to the target class, Ctarget [87, 97, 108, 148].\\n2.2 Adversarial attacks in NLP\\nIn the past few years, numerous methods for adversarial attacks have been introduced, which are specifically designed\\nfor NLP tasks. It is important to note that adversarial examples in computer vision cannot be directly applied to text\\nas they are fundamentally different. Therefore, several attack methods that modify the text data while maintaining\\nimperceptibility to humans have been proposed in literature. Typically, these methods alter the text data at the word,\\ncharacter, or sentence levels. The following section presents some of these attack methods in NLP.\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='212e9a0e-2c3d-406f-96a4-20e3d380d9fb', embedding=None, metadata={'page_label': '5', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 5\\nCharacter level adversarial attacks Character-level attacks perturb the input sequences at a character level. These\\noperations include insertion, deletion, and swapping of characters in a given input sequence. Despite the fact, these\\nattacks are quite effective, they can easily be detected with a spell-checker mechanism. One of the techniques used in\\ncharacter-level attacks is adding natural and synthetic noise to the inputs [ 8]. For natural noise authors collected natural\\nspelling mistakes and used them to replace words in inputs. For synthetic noise, they swap or randomized characters\\n(except the peripheral) and replace a character with its neighboring character on the keyboard. Adding punctuation\\nmarks, and increasing or removing the space between characters is another technique to add synthetic noise in the\\ntext inputs. For example, in DeepWordBug [ 33], in black-box setting they use a two-step process as they don’t have\\naccess to the gradients, parameters, or structure of the model. The first step involves finding the most important words\\nin the sentence which would be the target words to perturb. In the second stage, perturbations are added to these\\nselect words by the above-mentioned operations. Edit distance is further used in order to keep track of the readability\\nof the generated sentences. Another example proposed is, TextBugger [ 74] in both black-box and white-box settings\\nwhere the white-box attack is a two-step process. The first process involves finding the most important word with\\nthe help of the jacobian matrix ( JF(𝑥)) defined asJF(𝑥)=𝜕F(𝑥)\\n𝜕𝑥=h𝜕F𝑖(𝑥)\\n𝜕𝑥𝑖i\\n𝑖∈1...𝑁,𝑗∈1...𝐾, where𝑥𝑖is the𝑖𝑡ℎword\\nof the input text, 𝑁is the total number of words in the input text, and Fis the classifier, and later use 5different\\noptions to add bugs. These 5include insert, delete, swap, substitution with visually similar words, and substitution with\\na semantically similar words. In the black-box setting they propose a 3-step process where first the most important\\nsentence is identified, then they find the important words to generate 5 bugs and select the optimal from that. The best\\nadversary is chosen based on how optimal they are for reducing accuracy. Along the same line, [ 42] has shown that just\\nby adding extra \".\" (period), spaces between words, \"Perspective\" API created by Google gave lesser toxicity scores for\\nthe words perturbed in this fashion.\\nWord level adversarial attacks Word level attacks perturb the whole word instead of a few characters. Common\\noperations include insertion, deletion and replacement. Word level attacks can be classified into Gradient-based and\\nImportance based and replacement-based strategies on the basis of the perturbation schemes used:\\n•In gradient-based methods, the gradient is monitored for every input perturbation. Whenever the probability of\\nclassification is reversed that particular perturbation is chosen. This is inspired by the Fast Gradient Sign Method\\n(FGSM) [ 36] used for adversarial attacks in computer vision models. If the classification probability changes the\\nclass then the perturbation is considered effective. Another way of using gradient based method is to find the\\nimportant words using FGSM and then employ insertion, deletion and replacement strategies on top of them\\n[118]. [82] used a similar approach where they created adversaries by backpropagating for the cost gradients.\\n•In importance-based methods it is believed that words with the highest or lowest attention scores play an\\nimportant role in predictions of self-attention models. Hence these are chosen as the possible vulnerable words.\\nThese words are greedily perturbed until the attack is successful. One of the methods “Textfooler\" [ 52] uses\\na similar strategy where important words are greedily replaced with synonyms until the classification label\\nchanges. Another work in this direction [ 46] proposed TextExplanationFooler algorithm, which designed word\\nimportance-based attacks for explanation models in text classification problems. Working in a black box attack\\nsetting proposed attack attempted to alter outputs of widely used explanation methods while not changing the\\npredictions of the classifier.\\n•In replacement-based methods, words are randomly replaced with semantically and syntactically similar words.\\nHere the replacement for words is obtained by using word vectors like GloVe [ 99] or thought vectors. [ 64] used\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='38b33df9-5a89-42d0-8595-8a41762c158a', embedding=None, metadata={'page_label': '6', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6 Shreya Goyal, et al.\\nthought vectors to map sentences to vectors and replaced one word from it’s nearest neighbors which had best\\neffect on the objective function. [ 3] used GloVe vectors to randomly replace words that fit in context of sentence.\\nSentence level adversarial attacks These attacks can be considered as manipulation of a group of words together\\ninstead of individual words in the sentence. Moreover, these attacks are more flexible, as a perturbed sentence can be\\ninserted anywhere in the input, as long as it is grammatically correct. These attack strategies are commonly used in tasks\\nsuch as Natural Language Inferencing, Question-Answering, Neural Machine Translation, Reading Comprehension,\\ntext classification. For sentence-level attacks novel techniques such as ADDSENT, ADDANY are introduced in literature\\nin recent years with variants such as ADDONESENT, ADDCOMMON [ 49,141]. Some of the sentence based attacks are\\ncreated such that they don’t affect the original label of the input and used as a concatenation in the original text. In these\\ncases, the correct behavior of the model is to retain the original output and the attack is successful if the model changes\\nthe output/label. In another set of methods, GAN based sentence level adversaries are created which are grammatically\\ncorrect and semantically close to the input text [ 170]. Another example “AdvGen\" [ 19] is introduced which is an example\\nof gradient based white-box method and used in neural machine translation models. They used greedy search guided\\nwith the training loss to create the adversarial examples while retaining semantic meaning. Another work in this\\ndirection, [ 47] proposed syntactically controlled paraphrase networks (SCPNS) for adversarial example generation\\nwhere they used encoder-decoder network to generate examples with a particular syntactic structure.\\nMulti-level adversarial attacks Multi-level attack schemes consist of a mixture of some of the methods discussed\\nabove. These attacks are used to make the inputs more imperceptible to humans and to have a higher success rate.\\nHence, computationally more intensive and more complicated techniques such as FGSM have been used to create\\nadversarial examples. In one such method, they create hot training phrases and hot sample phrases. In this method, the\\ntraining phrases focus on what and where to insert, modify or delete by finding hot sample phrases in white and black\\nbox settings where deviation score is used to find the importance of the words [ 83]. Another example used \"HotFlip\"\\n[29] which is a character level white-box attack swapping characters based on the gradient computation. Similar to\\nmany other techniques, TextBugger [ 74] tries to find the most important word to perturb using a Jacobian matrix in a\\nwhite box setting. The important words after identification are used for creating adversaries by inserting, deleting and\\nswapping along with Reinforcement Learning methods with an encoder-decoder framework.\\n3 TAXONOMY OF ADVERSARIAL DEFENSES\\nIn this section, we will discuss the different types of defense methods used to protect deep learning models from\\nadversarial attacks. We will also highlight some of the recent research works that have shown promise in this area.\\nAdversarial defense strategies are methods used to prevent deep neural networks from failing due to adversarial attacks.\\nThese methods aim to increase the robustness of neural networks by training them in an environment that simulates\\nadversarial attacks, or by adding mechanisms to detect and handle adversarial inputs. Another approach to increase\\nrobustness is to create a perturbation-resistant region around the input space. Therefore, in NLP, there are three\\nmain strategies for designing adversarial defense methods: creating a similar environment during neural network\\ntraining, identifying malicious inputs during training and correcting them using specialized methods, and certifying\\nthe robustness of the input region for the network.\\nThe defense methods in NLP discussed in this paper are divided into three main categories: adversarial training\\nbased methods, perturbation control based methods, and certification based methods, along with some miscellaneous\\napproaches. Methods that do not fall under the first three categories are included in the miscellaneous category. The\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='abcc1223-8bce-4fb6-be7f-66463c9d5974', embedding=None, metadata={'page_label': '7', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 7\\nFig. 1. Taxonomy of adversarial defense methods in natural language processing\\nfirst set of techniques falls under the category of (i) Adversarial Training , which serves as a defense mechanism against\\nadversarial attacks. The various subcategories of methods included in this group are: Adversarial training through data\\naugmentation, Adversarial training as a regularization technique, Adversarial training using Generative Adversarial\\nNetworks (GANs), Virtual adversarial training, and Robustness through human intervention. The second set of methods\\nis based on (ii) perturbation control , and includes two subcategories: Perturbation identification and correction, and\\nPerturbation direction control. The third set of methods follows approaches that provide (iii) certification of the model’s\\nrobustness against adversarial attacks. The last set, (iv) miscellaneous , comprises a combination of various methods that\\ndo not fit into any of the aforementioned categories. Figure 1 illustrates the detailed taxonomy proposed in this survey\\narticle for adversarial defense techniques in NLP.\\n4 ADVERSARIAL TRAINING BASED DEFENSES\\nAdversarial training was first introduced in the work proposed in [ 35]. It is a method of defending against adversarial\\nattacks by introducing adversarial examples in the training data. The strength of adversarial examples decides the\\nfinal robustness and generalization achieved by the model. Adversarial training is further divided into sub-groups on\\nthe basis of the strategies used for augmenting the data such as word or character level modification, model-based\\ngeneration of adversarial examples or adversarial inputs generated by concatenation in the original data. While some\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f0d9ea0b-beca-4ba3-9a20-21cfdbf25ddf', embedding=None, metadata={'page_label': '8', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8 Shreya Goyal, et al.\\nof the methods performed adversarial training by generating a set of adversarial examples and inserting them in the\\ntraining dataset, other methods used adversarial training as regularizer by introducing perturbation within the network.\\nIn this section, some of the work in literature will be highlighted which proposes to use of data augmentation for\\ngenerating adversarial examples for adversarial training.\\n4.1 Adversarial training by data augmentation\\nAdversarial defense methods often use adversarial training as a basic technique for defending against adversarial attacks.\\nThis method involves generating a set of adversarial data using a perturbation scheme and incorporating it into the\\nmodel training process. Many techniques for generating adversarial examples using data augmentation have been\\nproposed in the literature. These techniques involve identifying the most important words, characters, or other parts\\nof the input text that affect the output the most and manipulating the input data by flipping, inserting, deleting, or\\nswapping those parts of the sentence. Concatenating a piece of text in the input by finding the most appropriate position\\nis another strategy used in the literature . There are also automatic methods for generating adversarial examples for\\nadversarial training, some of which will be discussed in the following sections.\\n4.1.1 Word level data augmentation. There are methods in literature that propose to modify words or word embeddings\\nin the input text for generating adversarial examples to augment data. In this line the work proposed in Textbugger [ 74]\\npresents adversarial training by data augmentation for text classification in both white box and black box settings by\\ngenerating utility-preserving adversarial examples. In the white box, important words for perturbations are found using\\nJacobian of classifier and then optimal perturbations are found by searching the nearest neighbor space in Word2Vec\\nembeddings. In black box setting, data is augmented by finding important words and sentences which contribute to the\\noutput the most and manipulating them. The work in [ 20] proposed novel data augmentation technique for adversarial\\ntraining in machine translation task, which reinforces the model to virtual data points around the observed examples\\nin training data. They proposed vicinity distribution for adversarial space (space of adversarial examples centered\\naround each training example), and sampled virtual adversarial samples from it using interpolated embeddings of\\nexisting training samples. In the work [ 78] adversarial perturbations are applied on word embedding layer of a CNN\\nfor text classification task to make the classification model robust towards the worst perturbations. Another work\\n[155] proposed a new method PQAT which perturbs the embedding matrix rather than the word vector for machine\\nreading comprehension task. Two additional independent embedding spaces for paragraph-question(PQ) are used, to\\ngive additional context for the same word used with different roles in paragraphs and questions. During training P-Q\\nembeddings are added to the original vector keeping the context from passage and question separate.\\nIn another work, [ 166], the authors proposed continuous bag-of-word (CBOW) embedding based perturbations for\\ngenerating human imperceptible adversaries for text classification task. Embedding space generated by CBOW is used\\nfor predicting the perturbation direction and tries to preserve the meaning of the sentence by placing a constraint\\non the perturbation direction. Authors generated adversarial examples without altering the semantic meaning of the\\nsentence and used these examples in adversarial training. In the same line, [ 84] proposed a solution to the Out-Of-\\nVocabulary (OOV) words problem faced by conventional character level defense methods leading to a poor performance\\nof models. They proposed adversarial stability training to overcome these challenges. Stability training is a technique\\nwhich makes the output of the neural networks significantly robust, while maintaining the original performance [ 171].\\nProposed Adversarial Stability Training (AST) is used with character level embeddings, to overcome OOV problems\\nand adversarial sentences are generated by perturbing each word, using character level embeddings representation\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='50c7e8b3-31eb-4303-9e95-9f57edb6fab5', embedding=None, metadata={'page_label': '9', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 9\\nto overcome the distribution problem. Another work in the same direction, [ 43] created adversarial examples using\\nseveral schemes such as, random word replacement, synonym replacement, finding weak spots in the input strings\\nwith greedy approach, by constraining the embeddings within 𝐿1distance, replacing the word on the basis of attention\\nscore (high attention score word to low score word) and demonstrate their results on sentiment analysis, textual\\nentailment, and machine translation tasks. They analyzed the robustness of RNNs, transformers and BERT-based models\\nand demonstrated that self-attentive models are more robust than RNNs. Work proposed in [ 9] evaluated adversarial\\ntraining with adversarial examples for eight datasets in NLP targeted for several purposes such as question answering,\\nreasoning, detection, sentiment analysis, and language detection using language models such as LSTM and GRU. They\\nused combinations of dropout and adversarial example for evaluation. Another work [ 123] proposed data augmentation\\nfor adversarial training for increasing the robustness of causal reasoning task. They proposed data augmentation by\\nsynonym substitution and by filtering out casually linked clauses in the larger dataset and used generative language\\nmodels to generate distractor sentences as potential adversarial examples. To improve the conventional adversarial\\ntraining methods [ 138] proposed to use gradient based approach for ranking the important words in the training dataset\\nand distilBERT similarity score for finding similarity between two-word embeddings for a faster and low-resource\\nrequirement-based adversarial training. Also they propose to use a percentage of training data for generating adversarial\\nexamples instead of converting all the training data for cheaper adversarial training.\\nIn [28], authors have proposed a white box based defense method by generating adversarial examples with flip,\\nswap, insert and delete at character level and used gradient based optimization method to rank the examples. The work\\n[165] proposed Metropolis-Hastings (MH) sampling [ 21] based adversarial example generator for text classification.\\nThree level word operations, replacement, insertion and deletion are performed with MH sampled words in a black box\\nsetting, while the gradient of the loss function is inserted in the pre-selection function of MH in case of white box attack.\\nFor the text classification task, in the work [ 26] adversarial training is used for cross lingual text classification and\\nrobustness enhancement by training it on English data and using the model to predict on non-English unlabelled data.\\nThe predicted outputs are used as adversarial examples for adversarial training. In the same line, [ 109], authors have\\nproposed a greedy algorithm probability weighted word saliency for adversary generation for the text classification\\ntask. Adversarial examples were generated with word synonym replacement and named entities with other named\\nentities using WordNet, picking up the words which cause maximum change in text classification probability. In the\\nwork [ 160], black box adversary generation is proposed which uses sememe (minimum semantic unit in linguistics)\\nbased word substitution that is more sememes per sense mean fewer substitute words that share the same sememes can\\nbe found, which negatively affects the adversarial attack success rate. Later they used Particle swarm optimization\\n[59] based algorithm to search the optimal adversarial example. In literature, frameworks and APIs have also been\\nproposed which provides a complete platform to the user with various kinds of attacks to generate adversarial examples\\nto be used for adversarial training for defense. In this line, the work [ 91] presented a python framework for attack\\ngeneration and defending the model, which augments the data using a word embedding, word swap, thesaurus word\\nswap, homoglyph character substitution, etc. They also provide a set of constraints on the generated perturbations\\nto keep them indistinguishable from the original such as, grammar check, POS tags etc. Along the same lines, [ 130],\\ngenerated imitation models for Google API of machine translation and generated adversarial samples using techniques\\nsuch as flipping, replacing malicious nonsense, substituting phrases that cause incorrect translation and attacking\\noriginal model in a black box manner. Later they used those adversarial examples to train the imitation model and\\ntransfer the examples to the victim model. In this work the authors aim towards, finding vulnerability in the victim\\nmodel to make it more robust, by having the victim model output a different high-accuracy translation. Apart from\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1b338cf9-b016-4539-857e-698fd57a1561', embedding=None, metadata={'page_label': '10', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10 Shreya Goyal, et al.\\nword level perturbations, there are some sentence level perturbations proposed to generate adversarial examples, which\\nare discussed in coming section. These adversarial examples are hand crafted and generated by identifying vulnerability\\nof model, and further used with adversarial training to defend model from such attacks. However, some of papers\\nproposed model based generation of adversarial examples are also discussed in coming sections.\\n4.1.2 Concatenation based data augmentation. Another approach to generating adversarial examples for data augmen-\\ntation involves using concatenation-based strategies and automatic generation of adversaries with language models. For\\ninstance, [ 50] developed concatenative adversarial perturbations, AddSent, to generate adversarial examples for reading\\ncomprehension systems. This method involves concatenating grammatically correct sentences to end of a paragraph,\\nwhich look like questions or other arbitrary sentences. They also replaced some answers with fake answers that have\\nthe same part of speech type and category. Similarly, [ 142] proposed AddSentDiverse, which generates adversarial\\nexamples with significantly higher variance by varying placement of perturbations. They also expanded set of fake\\nanswers used in AddSent and demonstrated the limitations of the original AddSent method.\\n4.1.3 Generation based data augmentation. Another approach to improving the robustness of machine learning models\\nis through generation-based adversarial examples. For example, [ 56] proposed using knowledge-guided rules and a\\nseq2seq model to generate new hypotheses from a given premise for textual entailment models. In a similar vein, [ 39]\\nproposed a defense strategy using adversarial training, using a seq2seq model to generate perturbations for structure\\nprediction tasks that involved predicting POS tags, parse-trees, and noun phrases. In [ 150], the authors proposed a\\ngrey box adversarial attack for sentiment analysis that uses a generator model for data augmentation. Adversarial\\ntraining was conducted by augmenting data using a static copy mask mechanism in the generator, and counter-fitted\\nword embeddings and label smoothing methods were used to better capture lexical relations and preserve the labels of\\nadversaries. [ 92] introduced the idea of injecting antonymy and synonymy constraints into vector space representations.\\nIn the same line, [ 134] proposed a new method of adversarial example generation by controlled adversarial text\\ngeneration where they aimed to perturb input for a given task by changing other controllable attributes of the dataset.\\nFor example, in the case of sentiment analysis task for product reviews, product category becomes a controllable attribute\\nthat cannot change the sentiment of a review. Their pretraining module consists of an encoder-decoder architecture,\\nwhich is used to teach the model to copy the input sentence S, assuming that it has the controllable attribute ( 𝑎) in\\nthe sentence. The decoder module is updated to generate a sentence containing attribute 𝑎′≠𝑎. In the optimizing\\nmodule the subspace of all 𝑎′is checked by computing the cross-entropy loss to find the highest perturbation. In similar\\nlines of developing robust NLP model, the work proposed in [ 65] studied a group of linguistic rules to demonstrate\\nlocal semantic robustness within a sentence and generated variations in input text using predefined template with\\nfixed label. These templates adhere to the linguistic rules discussed in the paper, where incoming variations will\\nnot be able to change the output label and further used them for training robust sentiment analysis models. In the\\ndirection of paraphrasing-based adversarial instance generation, [ 48] proposed syntactically controlled paraphrase\\nnetworks (SCPNs), which generate a paraphrase of the given sentence with the desired syntax in a controlled manner.\\nSCPNs use a bidirectional LSTM and a two-layer LSTM augmented with soft attention over the encoded states based\\nencoder-decoder architecture, utilizing a paraphrase pair and a target syntax tree as the inputs. Using the adversarial\\ninstances in adversarial training, they evaluated the proposed method on sentiment classification and textual entailment\\napplications.\\nTable 1 shows the summary of all the adversarial training methods with data augmentation. It summarizes the\\nstrategies used in the literature for perturbation generation, along with the granularity of the perturbations used,\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='57a1561f-fea0-4f97-9e91-c9b3604524f9', embedding=None, metadata={'page_label': '11', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 11\\nStrategy Work Granularity Application Threat Model\\n[74] Words & Sentences Sentiment Analysis White Box,\\nBlack Box\\n[20] Sentence level Machine Translation Black Box\\n[78] Words Text Classification White Box\\n[155] Embedding Matrix Reading Comprehension White Box\\n[92] Word embedding Dialogue state tracking White Box\\n[166] Word embeddings Text Classification White Box\\n[84] Character Embeddings Text Classification White Box\\nWord[43] Word Sentiment Analysis, Textual Entailment,\\nMachine TranslationWhite Box\\n[9] Word and Character\\nembeddingsQuestion Answering, Reasoning, Senti-\\nment Analysis, Language detectionWhite Box\\n[123] Word level Causal Relation classification White Box\\n[28] Character level Machine translation White Box\\n[165] Word level Text classification Black box\\n[109] Word level Text Classification White Box\\n[160] Word level Text classification Black Box\\n[130] Phrase level Machine Translation Black Box\\n[26] Word level Document & Intent classification White Box\\nConcatenation [50] Sentence Reading comprehension White Box\\n[142] Sentence Reading comprehension White Box\\n[56] Sentence generation Textual Entailment White Box\\n[39] Sentence generation Predicting POS tags, parse trees, NP Black Box\\nGenerative [150] Sentence generation Sentiment Analysis Grey Box\\n[134] Sentence generation Sentiment Analysis White Box\\n[65] Sentence generation Sentiment Analysis White Box\\n[48] Sentence generation Sentiment classification, Textual entail-\\nmentWhite Box\\nTable 1. Summary of the adversarial training methods by data augmentation\\ndemonstrated NLP applications, and the kind of the threat model they are defending. Many of the word-based methods\\nfor data augmentation involve operations like flipping, swapping, inserting, deleting, and synonym substitution with\\nwords to modify the original inputs and generate adversarial examples for use with adversarial training. Some methods\\nuse these operations with characters in the sentences instead of whole words. These adversarial perturbation generation\\nmethods aim to maintain the naturalness of the input text so that the perturbation is imperceptible to humans. Therefore,\\nsome of these methods use word embeddings for perturbations instead of words from the input text. Another set of\\nmethods in the literature use concatenation operations within input sentences by identifying the appropriate position\\nfor concatenation, and these adversarial examples are used in adversarial training. In addition to manual perturbations\\nand adversarial example generation, another direction of methods uses generative models to create adversarial examples\\nor perturbations. Along with the data augmentation methods, adversarial training methods introduce perturbations\\nwithin the training loss functions, which are discussed in the next section.\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='71ee9791-5084-4ad8-a9f8-bfa968c26267', embedding=None, metadata={'page_label': '12', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12 Shreya Goyal, et al.\\n4.2 Adversarial training as regularization technique\\nIn this section, another defense method based on adversarial training for NLP is discussed. In this style of adversarial\\ntraining, the input perturbations are incorporated as a part of model training, instead of training it with adversarial\\nexamples. In the work [ 35], authors proposed adding perturbations in input as a regularizer in the loss function. The\\nmodified optimization function based on the fast gradient sign method after adding perturbations is defined as:\\n𝐿𝑎𝑑𝑣(𝑥𝑙,𝜃)=𝐷[𝑞(𝑦|𝑥𝑙),𝑝(𝑦|𝑥𝑙+𝑟𝑎𝑑𝑣,𝜃)]\\n𝑟𝑎𝑑𝑣=argmax\\n𝑟;||𝑟||2≤𝜖𝐷[𝑞(𝑦|𝑥𝑙),𝑝(𝑦|𝑥𝑙+𝑟,𝜃)]\\nWhere,𝐿𝑎𝑑𝑣is the adversarial loss term, 𝑟𝑎𝑑𝑣is the adversarial perturbation, 𝑥𝑙is the labeled input data, 𝐷is the\\nnon-negative divergence measurement function between two probability distributions, 𝜖is the upper bound on the\\nperturbations, 𝑞(𝑦|𝑥𝑙)is the unknown true distribution of the output label. This loss function is designed to approximate\\nthe true distribution 𝑞(𝑦|𝑥𝑙)by a parametric model 𝑝(𝑦|𝑥𝑙,𝜃)which is robust against adversarial attack to the input\\ndata𝑥. Adversarial training using perturbations in the loss function has been shown to be a successful defense strategy\\nas it generates adversarial examples that are difficult to create manually by exploiting flaws in the optimization function\\nto improve model generalization. In the field of NLP, several techniques have been proposed to introduce perturbations\\nduring training or modify the loss function. Here, we describe some of the methods that have been proposed to introduce\\nperturbations during training.\\nIn the work [ 89], the authors included perturbations at each step of training and tried to minimize the loss function for\\ntext classification tasks. In another work [ 156], authors proposed adversarial training for POS tagging, by using character\\nlevel embeddings with BiLSTM models, where word-level embeddings are generated by concatenating character level\\nembeddings. Perturbations are added to the input at the character level embeddings in the direction which maximises\\nthe classifier loss function while training the model. Another method [ 146], introduced adversarial noise at embedding\\n(concatenation of word and characters) level for the task of relation extraction within the multi-instance multi-label\\nlearning framework. They proposed a joint model to entity recognition and relation extraction using adversarial\\ntraining as a regularization scheme where worst case perturbations are added to maximize the training loss. Along\\nthe same line, [ 133] authors improved the neural language modeling by using adversarial training as a regularization\\ntechnique. They injected an adversarial perturbation on the word embedding vectors in the softmax layer of the language\\nmodels while training the model. Authors have suggested new loss functions and diverse neural network optimization\\nmethods to train a model adversarially and improve its robustness. In this direction, the work [ 88] proposed adversarial\\ntraining for natural language inferencing, by reducing the adversarial example generation problem to combinatorial\\noptimization problem. They proposed a continuous inconsistency loss function that measures the degree to which\\na set of examples can cause a model to fail. By maximizing the inconsistency loss and constraining the perplexity\\nof the generated sentences, adversarial examples are generated, posing it as an optimization problem. In the same\\ndirection, [ 57] proposed defense mechanism, diversity training, for transfer based attacks for the ensemble of models.\\nThey proposed gradient alignment loss (GAL) which is used as regularizer to train an ensemble of diverse models with\\nmisaligned loss gradients. Another work [ 25] proposed a novel Adversarial Sparse Convex Combination (ASSC) method\\nto leverage regularization term for introducing perturbations. They modeled the word substitution attack space as a\\nconvex hull of word vectors and further proposed ASSC-defense for using these perturbations in adversarial training.\\nThere are methods in the literature that proposed novel regularization techniques for enhancing the robustness of a\\nlanguage model. In this line of work [ 132] proposed InfoBERT for increasing the robustness of BERT based models by\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a4c635be-799d-4c1f-b32b-8879bb3c4ce3', embedding=None, metadata={'page_label': '13', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 13\\nanalyzing language models from information-theoretic perspective. They presented two mutual information based\\nadversarial regularizers for adversarial training. Information Bottleneck regularizer which extracts minimal features for\\ndownstream tasks and removes noisy and vulnerable information for potential adversarial attacks. Anchored Feature\\nregularizer extracts strong local features which are not vulnerable while aligning local features with global features\\nto increase the robustness. Another work [ 175] proposed FreeLB, to use K-step PGD [ 4] for generating adversaries\\nin adversarial training and used multiple PGD iterations. In contrast with k-step PGD and freeAT [ 120] methods,\\nused multiple PGD iterations to create adversaries simultaneously accumulates the “free” parameter gradient. In this\\nmethod while optimizing the objective function, it replaces the batch of input with K times large batch which include\\nperturbations along with inputs. Improving the performance of this work [ 81] proposed FreeLB++, by extending the\\nsearch region to a larger 𝑙2-norm and increasing the number of search steps at the same time since FreeLB has a narrow\\nsearch space. They also bench-marked various defense methods in literature under standard constraints and settings\\nto have a fair comparison of these methods. Table 2 describes the summary of adversarial training as regularization\\ntechnique. In the coming section GAN based adversarial defense methods are discussed, where GAN is used as a\\ngenerator and discriminator model to build more robust models against adversarial attacks.\\nWork NLP task Granularity\\n[89] Text Classification Word embedding\\n[156] POS Tagging Character embeddings\\n[146] Relation Extraction Word and character embeddings\\n[133] Machine translation, language modeling Word embeddings\\n[88] Natural Language Inferencing Sentence Embedding\\n[25] Sentiment Analysis and Natural Language Inferencing Word embeddings\\n[132] Question Answering, Natural Language Inferencing BERT embeddings\\n[175] Natural Language Inferencing, Textual Entailment Word embeddings\\n[81] Sentiment analysis, Text Classification Word embeddings\\nTable 2. Summary of adversarial training as regularization technique\\n4.3 GAN based adversarial training\\nUsing GAN for adversarial training is another approach that has been used in literature for defending models from\\nadversarial attacks. In this approach a generator and discriminator are trained together in adversarial manner, where\\nthe generator is primarily used for generating adversarial examples. The discriminator is responsible for discriminating\\nbetween the clean data and the adversarial training samples to make model more robust towards adversarial attacks.\\nIn this line, the work proposed in [ 56] uses adversarial training withva GANs for the textual entailment task. The\\ngenerator and discriminator are trained in an end-to-end manner, where generator (seq2seq) is trained for generating\\nadversarial examples using external knowledge or handwritten rules. Discriminator is trained in the same manner to\\nlearn the textual entailment for the generated samples. In another work, [ 110], authors used a conditional variational\\nautoencoder which generates the adversarial examples for text classification task. They further used a discriminator and\\nGAN training framework for adversarial training and to make sure the generated adversaries are consistent with the\\nreal-world data. In the work [ 85] authors used multi-task learning for domain adaptation. They proposed a model which\\nhas separate units to discriminate between shared patterns and task specific patterns using GAN for creating adversarial\\nexamples and includes this loss in the final loss for optimization. They demonstrated the improved performance over 16\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2ac5f1f5-114d-4f93-914a-f74a0e10cd89', embedding=None, metadata={'page_label': '14', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14 Shreya Goyal, et al.\\ndatasets and the learned parameters in both shared and task-specific parts of the network. The work in [ 149] proposed\\na new adversarial training approach that mimics a GAN. The generator is used to create adversarial examples with\\nthe help of lexical knowledge base where a classifier is used to score the generated adversarial example. The model\\nis trained in a reinforcement learning fashion due to discrete-time generation of the generator model and the score\\nfrom the classifier is used as the reward for the generator. The generator generates examples by replacing words\\nin the input sentence with synonyms, neighboring words, and superior words. In another work, [ 22] try to defend\\nagainst an attacker who tries to take the encoded information to reconstruct the original input text. In the adversarial\\ntraining-based defense strategy they used GAN style training with 2 components in which the original one predicts the\\nclass of a given sentence and a binary classifier predicts the privacy element. The training style involves making the\\noutput prediction as correct as possible while at the same time creating more complex examples for the privacy element\\nclassifier. Table 3 describes the summary of GAN based adversarial training methods, where each row describes the\\nNLP task for which defense is designed and the specific strategy used in the proposed work.\\nWork NLP Task Strategy\\n[56] Textual Entailment Using external knowledge\\n[110] Text classification Using Conditional Variational Autoencoder (VAE)\\n[85] Text classification Adversarial shared-private model\\n[149] Sentiment Analysis Using lexical knowledge base\\n[22] Sentiment Analysis and Topic classification Privacy measurement of neural representations\\nTable 3. Summary of GAN based adversarial training methods\\n4.4 Virtual Adversarial Training (VAT)\\nVirtual Adversarial Training (VAT) is another variant of adversarial training-based defense methods first proposed by\\n[90]. VAT is found to be a very efficient method in the case of semi-supervised learning methods because it defines\\nthe adversarial direction without label information. In contrast to Adversarial training, VAT doesn’t need full label\\ninformation for generating perturbation. The intuition behind VAT is to add perturbation′𝑟′to input𝑥such that the\\ndivergence of their output space is maximum. Hence, training is done in a way to minimize the divergence term after\\nadding the perturbed input to make the model robust against adversarial attacks. The modified loss function for virtual\\nadversarial training is defined as:\\n𝐿𝐷𝑆(𝑥∗,𝜃)=𝐷[𝑝(𝑦|𝑥∗,ˆ𝜃),𝑝(𝑦|𝑥∗+𝑟𝑎𝑑𝑣,𝜃)]\\n𝑟𝑎𝑑𝑣=argmax\\n𝑟;||𝑟||2≤𝜖𝐷[𝑝(𝑦|𝑥∗,ˆ𝜃),𝑝(𝑦|𝑥∗+𝑟𝑎𝑑𝑣)]\\n𝑅𝑎𝑑𝑣(𝐷𝑙,𝐷𝑢𝑙,𝜃)=1\\n𝑁𝑙+𝑁𝑢𝑙∑︁\\n𝑥∗∈𝐷𝑙,𝐷𝑢𝑙𝐿𝐷𝑆(𝑥∗,𝜃)\\nWhere,𝑥∗are the “virtual\" labels which are probabilistically generated and 𝑟𝑎𝑑𝑣are virtual adversarial perturbations.\\nHere,𝑥∗is kept in the place of 𝑥, since label information is not available for all the input data. 𝐿𝐷𝑆 is the local smoothness\\nterm for the input data point 𝑥and𝑅𝑎𝑑𝑣is the final regularization term.\\nThe work proposed in [ 89] extended the notion of virtual adversarial training and adversarial training for text\\nclassification and sequence models proposing this technique as a regularization method. For defending the models, they\\nintroduced perturbations in word embeddings of the text inputs, while minimizing the KL divergence of VAT. In another\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='335c9a2b-c09c-4aa2-a7f5-1885fd53dd24', embedding=None, metadata={'page_label': '15', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 15\\nwork [ 100], authors proposed a VAT method by performing adversarial steps on those examples which are predicted as\\nwrong by the model and then regularise the model for this target direction in contrast with general adversarial training\\nmethods where the perturbation is done for all examples with variation from the gold label. In a targeted training\\nmanner, they try to steer the examples to a particular label 𝑦𝑡and presented a comparison with human-annotated\\ndata along with other adversarial training algorithms. In the same direction, the work [ 86] authors proposed a novel\\nadversarial robust model “Adversarial training for large neural LangUage Models(ALUM)\" for defending BERT-based\\npretraining language models. It is a general model for adversarial training in pretraining and fine-tuning which\\nregularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial\\nloss. The model is regularized using VAT. Experimenting with different word embeddings using VAT, [ 166] extended\\nthe adversarial training regularization for semi-supervised tasks. They used continuous bag of words (CBOW) model\\nfor generating word embeddings and restricted perturbation directions for creating adversaries. Targeting specifically\\nsequence labelling tasks in NLP, [ 18] proposed VAT for sequence labelling task combining CRF, making sequence\\nlabelling task more robust. They use CNN layer for extracting character and word embeddings, LSTM for sequence\\nencoding, and CRF decoder layer to incorporate the probabilities of label transition. Introducing more variations to\\nVAT, [ 77] proposed Token aware virtual adversarial training. In contrast with conventional virtual adversarial training,\\nTAVAT generated token aware perturbations instead of random perturbations to avoid unnecessary noise and take\\nimportant information carried by tokens into consideration. Table 4 describes the summary of Virtual Adversarial\\nTraining (VAT) based methods along with the specified NLP task for their design and granularity of the perturbation.\\nWork NLP task Granularity\\n[89] Text classification & Sequence modeling Word embeddings\\n[100] Natural Language Inferencing (NLI) Word embeddings\\n[86] Question answering, NLI, Named Entity Recognition(NER) BERT embeddings\\n[166] Sentiment classification Word embeddings\\n[18] Chunking, NER, slot filling Character and word embeddings\\n[77] NER, NLI, Textual Entailment, text classification Token level\\nTable 4. Summary of Virtual Adversarial Training (VAT) based defense methods\\nAlong with the several variants of adversarial training methods, there are a few schemes that utilizes Human-In-The-\\nLoop (HITL) framework, where human-level intervention is considered for adversarial training. Some of these methods\\nare discussed in the next section.\\n4.5 Robustness by human in the loop\\nHuman-In-The-Loop (HITL) is an idea of leveraging human intervention while training models or defending them\\nagainst adversarial attacks. The scheme is extensively used in various fields of artificial intelligence. It takes advantage\\nof both human and machine intelligence, for labeling the data, and training and validation of models. While it is proven\\nto be an efficient scheme in other areas of artificial intelligence, several authors have tried to use HITL for developing\\nalgorithms for adversarial defenses.\\nThe work [ 160] proposed a sememe-based word substitution method to generate perturbations. Apart from using\\nparticle swarm based optimization algorithm to search perturbation for data augmentation, they manually selected 692\\nvalid adversarial samples for adversarial training to further boost the performance. Also, authors in [ 94,144], created\\ndataset of adversarial examples, ANLI for natural language inferencing task by crowd-sourcing. Adversarial examples\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6b8f6cb4-1301-42ba-aa15-2d04cb1fc937', embedding=None, metadata={'page_label': '16', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16 Shreya Goyal, et al.\\nare written and verified by human annotators in 3stages in loop, while getting them tested from high performing NLI\\nmodels. They also presented error analysis and discuss annotation scheme and data collection process of ANLI. In the\\nwork [ 24] authors built a model for offensive language detection in dialogues using human and models in loop. They\\ntrained BERT based model on Wikipedia Toxic Comments dataset, and asked crowd workers for marking the messages\\nas offensive if they are wrongly marked safe by the system. This process is performed in multiple iterations to build a\\nrobust system. Work in [ 129] presented a defense method using Human-In-The-Loop by proposing human-computer\\nhybrid approach for evaluating the models. They presented a human verification of the question-answering system,\\nwhere human annotators authored adversarial examples to break a model-based QA system but still answerable by\\nhumans. This process is targeted towards building a robust question-answering system by inserting human-authored\\nadversarial examples. Table 5 depicts summary of the Human-In-The-Loop (HITL) based adversarial training methods\\nin NLP defense. Each row in table depicts the NLP application for which the defense method is designed and granularity\\nat which human interaction is proposed. Adversarial training-based defense methods are evidently the most popular\\nway to build robust models against adversarial attacks in NLP. It requires the generation of adversarial examples using\\nseveral techniques and using them for training the model. However, in contrast with training with adversarial examples,\\nthere are methods in the literature that are proposed for the detection of perturbations, their correction, and controlling\\ndirections of those perturbed instances. Some of these methods are discussed in the next section.\\nWork NLP Tasks Human interaction granularity\\n[160] Sentiment Classification, NLI Manual selection of word substitution\\nbased perturbations\\n[94] NLI Crowd Sourced and tested with models\\n[144] NLI Crowd Sourced and tested with models\\n[24] Offensive language detection BERT with crowd workers\\n[129] Question Answering Human annotators and verification\\nTable 5. Summary of the Human In The Loop (HITL) based adversarial training methods\\n5 PERTURBATION CONTROL BASED DEFENSES\\nThe adversarial defense methods proposed in previous sections, use data augmentation schemes, perturbation generation\\nwithin training for supervised and semi-supervised tasks, adversaries monitored by human and models in loop, defending\\nthe model in a generating and discriminating manner. However, all these schemes do not incorporate the idea of\\ninterpretable perturbations or reconstruction of generated perturbations. In literature, there are schemes that control\\ndirection of perturbations to make the perturbations more meaningful, indistinguishable and re-constructive and further\\nuse them in training. Also, another set of method try to identify the perturbed inputs and correct them to make the\\nmodels more robust. In this line, the following sections describe methods which have been proposed in this direction.\\n5.1 Perturbation identification and correction\\nIn literature perturbation, control-based method tries to identify malicious inputs after an attack and correct them.\\nSome of the methods only identify these inputs and filter the training data accordingly. While the other category of\\nmethods also corrects these inputs after their identification using methods such as spell checker or rule-based. Some of\\nthese methods are described below.\\n5.1.1 Perturbation Identification. Perturbations related to word modifications which included insertion, deletion,\\nsubstitution or swapping of words are identified in several ways. One of those methods is proposed in [ 139], where the\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='420d3f69-cc5d-49dc-a667-55ed095b79dd', embedding=None, metadata={'page_label': '17', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 17\\nauthors proposed defense mechanism, against synonym substitution, calling it “Synonym Encoding Method”(SEM).\\nThey essentially clustered all the synonyms in embedding space with their euclidean distances and then encoder is\\nlayered before input to train the model. Encoder is responsible for identifying all the synonym substitution-based\\nattacks in the model and maps all the synonyms to a unique encoding without adding extra data for training. Improving\\non this work [ 154] proposed a robust adversarial training method called Fast Triplet Metric Learning (FTML). This\\nmethod tries to cluster the similar embedding and pushes dissimilar embedding where SEM works directly with\\ninput texts and establishes no relation with non synonym clusters. FTML forces each word with a similar meaning to\\nhave the same representation in the feature space and pushes away the word with a different meaning. The method\\nincorporates a word level triplet loss which tries to minimize the distance between a word with its corresponding\\ngroup of synonyms and maximizes the distance with its non-synonym group. In another work in this direction, [ 173]\\nproposed Dirichlet Neighborhood Ensemble (DNE), a randomized smoothing method for training a robust model to\\ndefend substitution-based attacks. DNE forms virtual sentences by sampling embedding vectors for each word in an\\ninput sentence from a convex hull spanned by the word and its synonyms, and it augments them with the training data,\\n(mixing the embedding of the original word in the input sentence with its synonyms). The work in the same line [ 6]\\nintroduced frequency-aware randomization framework Anomaly Detection with Frequency Aware Randomization\\n(ADFAR) for defense against adversarial word substitution. They add an extra module to detect perturbation in the\\nsentences and apply ADFAR only on sentences that are identified as adversarial. This module is added to the language\\nmodel and use a multi-task learning procedure. They demonstrated that ADFAR works better than other defenses\\non 4 datasets - MR, SST-2, IMDb, MNLI. The work [ 140] proposed an adversarial defense scheme by perturbation\\ndetection for synonym substitution attacks. They proposed a novel method, namely, Randomized Substitution and\\nVote (RS&V). The proposed method calls an input text “adversarial example\", by randomly substituting some of its\\nwords by their synonyms and checking the consistency of the highest voted label for all perturbed examples. If the\\nlabel is found to be inconsistent with the original label then it is considered as an adversarial input. Working in a\\nsimilar direction, [ 127] proposed to use of random perturbations to defend sentiment analysis models. They inserted\\nrandom perturbations to the multiple copies of a randomly selected sentence from the reviews. These perturbations\\nincluded synonym substitution, dropping of a word or a spell check with correction if necessary. All these perturbed\\ncopies of the sentences are put together with the original review and a majority vote is taken for each sentence by the\\nsentiment classifier. By majority-based classification, the models are taken back to their original performance accuracy\\nbefore the attack. In the direction of perturbation identification, [ 147] proposed an extensive dataset, TCAB, for attack\\ndetection and labeling with over 1.5million instances of adversaries. To construct this dataset, authors used 6 datasets\\nfor text classification with 3 target classifiers and 12 different attacks from textAttack and openAttack and presented a\\nbenchmark for these attacks. They proposed an attack identifier/labeler, using three features of the input such as text\\nproperties including contextual embeddings, length of the input, token case, punctuation, non ASCII characters. Authors\\nuse language model’s properties as another feature that identifies the structure of the language such as phrasing or\\nungrammatical input text. They used target model’s properties as another feature that captured the changes in model’s\\ngradients, activation or saliency because of malicious inputs and trained a classifier with them. Utilizing the TCAB\\ndataset and the text, language, and classifier features, the detection of the perturbation and labeling the type of attack is\\nproposed. Another work [ 136] proposed a novel method called TextFirewall identification of adversarial inputs. They\\nused word importance to quantify the importance of a word in the input sentence to the final classification of the\\nmodel. The impact of each word in an input sentence is calculated to detect the perturbation, by summing the scores of\\neach model, and then the model’s output is compared with original ground truth to identify the perturbed input. In\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='57680717-f251-443e-9be1-1a4791a56cb8', embedding=None, metadata={'page_label': '18', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18 Shreya Goyal, et al.\\na similar direction, [ 158] proposed an adversarial robustness method by proposing feature density estimation-based\\nperturbation detection. In contrast with the available frequency-based likelihood estimation, they utilized the probability\\ndensity of sentences. They proposed a method of Robust density estimation (RDE), which fits the probability density\\nestimation model on the features obtained from a pre-trained model like BERT. Dimensionality reduction is applied\\nto the parameters of these features to avoid the curse of dimensionality. In addition, they released a benchmark for\\nword-level adversarial detection using 4 NLP models with four different datasets for text classification. In addition\\nto the discussed work in perturbation identification, some of the methods in literature also attempt to correct these\\nperturbed input data after their detection. Some of the perturbation correction methods are discussed in next section.\\n5.1.2 Perturbation correction. Adversarial inputs are also required to be corrected after their identification to retain the\\ntraining data. Some of the methods in this direction of work attempted to identify perturbations related to character-level\\nmodifications in the input text. In this direction, the work [ 116] authors proposed a semi-character level recurrent\\nneural network (ScRNN), which act as a spell checker by recognizing words. ScRNN has an architecture similar to\\nstandard RNN and takes semi-character vector as input and predicts a correct word at each time step by applying\\nthree types of noises: jumble, delete, and insert. As an extension of the above work, in [ 102] authors try to combat\\nmisspellings by using a word classifier before the actual classifier of a task. They propose ScRNN with backoff, to\\novercome limitations of ScRNN [ 116], and propose three backoff techniques if the word classifier predicts it as unknown\\n(UNK). As a backing-off step, the word recognizer either passes the UNK word as is, backs off to a neutral word\\nor backs off to a more general word recognition model trained on a larger, less specific corpus. In the work, [ 58]\\nauthors demonstrated the limitations of spell checker for perturbation identification & correction. They proposed a\\nmethod in which context independent probability distribution are created by segmenting the perturbed sentence using\\nBERT tokens and modified version of levenshtein distance. For context dependent probability - all the embeddings of\\ncontext-independent hypothesis are clubbed into a weighted embedding. Now a token is masked and MLM is used to\\npredict the tokens. This process is repeatedly done for the best approximation. Now, these hypotheses are sent to GPT\\nfor getting the language modeling score and the best hypothesis is selected from that. They compared their method\\nagain Pyspellchecker, human annotations and RNN trained for spell checking. In the work, [ 31], authors presented\\nbackdoor attacks as adversarial attacks during training of the model and proposed attacking methods for NLG model by\\ninserting trigger words in the input sentence. They further proposed defense strategies by detecting of hacked inputs\\nand output correct results and preserving the correct input and giving its output.\\nIn [172] proposed a novel method for perturbation identification and correction, in which they try to recover the\\nperturbed token based on the context and with the help of small world graphs. First they use, BERT model to get the\\ncontextualised embedding vector for each token and then pass it to a binary classifier for classification of perturbation.\\nLater they used a BERT-based context network, to be used as the context for predicting the perturbed word. The\\nperturbed word is masked and passed to the BERT to get the embedding of the mask token. Using the embedding\\nvectors and small world graphs they recovered the affected tokens. In the direction of perturbation identification\\nand correction, In [ 10] various types of perturbed text are identified and corrected using rule based methods such as\\nalternating characters defense which corrects the combined unicode, space separation, and zero-width space separation\\nperturbation in the entire document. Another rule-based defense used is Unicode Canonicalization which corrects &\\nreplaces unicode and tandem character obfuscation perturbations. They further used a continuous bag of words based\\nembeddings and identified embeddings which are generated by parts of a single word combining random spaced words\\nfollowed by a process to find similar embedding for words with similar spelling. A skip-gram model is trained with\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f105c942-a984-4411-8acf-180ea52a53fa', embedding=None, metadata={'page_label': '19', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 19\\nvectors of similar context to ensure the embeddings having similar spelling and context are closer. They evaluated\\ntheir embeddings with the downstream task of classifying the Facebook posts as engagement bait or otherwise. In a\\ndifferent line, the work [ 174] proposed a universal perturbation detection method, TREATED to defend against various\\nperturbation levels without making any prior assumptions. They utilized several reference models to make different\\npredictions about clean and adversarial examples and block them if found adversarial. They designed the reference\\nmodels on the basis of their consistency on the clean and adversarial data. In the direction of identifying perturbations\\nfor other language text than English, the work [ 72] proposed a defense model for text classification for Chinese language.\\nAdversarial perturbations are detected in 3steps. Neural Machine Translation (NMT) model is used for removing the\\nnoise in the input text. The corrected text is converted into multimodal embeddings (semantics, glyph, and phonetics)\\nand the extracted features are given into text classification.\\nTable 6 demonstrates the summary of the various perturbation identification and correction methods. It shows the\\ntype of attack for perturbing the input data, the strategy used for detecting the perturbations, NLP applications on\\nwhich the proposed method is demonstrated, and whether or not they are attempting to correct the perturbed input\\nafter their detection. As it can be seen that a large part of perturbation detection and correction method is limited to\\nsynonym substitution and misspelling-based adversarial attacks. Also, the demonstration of the proposed defense is\\nlargely demonstrated on various types of text classification tasks including sentiment classification, news category\\nclassification, and topic classification. The commonly used techniques for perturbation correction after their detection\\nincludes blocking the perturbed data, generating or predicting the clean text, and replacing it with similar correct words.\\nThere are methods proposed in literature which defend the machine learning model by controlling the direction of\\nperturbations in their embedding space. Some of these methods are discussed in the next section.\\n5.2 Perturbation direction control\\nThe proposed work under perturbation direction control alters the direction of the perturbations towards the cleaner\\ntext input limiting the adversarial space. Along this line, the work [ 119] proposed an interpretable adversarial training\\nmethod by restricting the direction of adversarial samples. The direction of perturbation is restricted to the words\\nin the existing vocabulary so that perturbations could be interpreted even after adversarial training. In the work\\n[166] authors propose to use CBOW to predict the perturbation direction while trying to preserve the meaning of the\\nsentence by placing a constraint on the perturbation direction. Another work, [ 111] proposed an adversarial defense\\nmechanism, Sequence Squeezing, aimed to make RNN models and their variants robust against adversarial attacks.\\nThe proposed method generates semantic preserving embeddings which are low in the number of features than the\\noriginal embedding. The squeezed embedding is tested for adversarial attacks in malware detection and added to the\\ntraining data while diminishing the adversarial space for generating perturbations. Table 7 presents the summary of\\nperturbation direction control based adversarial defense methods in NLP, where each row shows the NLP applications\\nand the type of perturbation used in the threat model in the associate work. Proposing adversarial defense for text\\ninput data with perturbation direction control is a step towards developing more interpretable defense model than\\nconventional methods of adversarial training. The discussed methods in this category demonstrate their defense scheme\\non various type of text classification tasks, such as sentiment classification, malware classification. Another direction of\\nadversarial defense methods in NLP propose to provide a certified region of robustness while training their machine\\nlearning model. Some of these methods are discussed in detail in the coming section.\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7e0edb6e-f061-4124-9bc6-98f7db381397', embedding=None, metadata={'page_label': '20', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20 Shreya Goyal, et al.\\nWork Attack Method ApplicationPerturbation\\nIdentificationPerturbation\\nCorrection\\n[139] Synonym Sub-\\nstitutionMaps synonyms to\\nunique encodingSentiment, Topic\\nclassification✓ —\\n[154] Synonym Sub-\\nstitutionCluster similar embed-\\ndingsSentiment, Topic\\nclassification✓ —\\n[173] Substitution\\nbasedMixing embeddings of\\nwords & synonymsSentiment, , News\\ncategory classifica-\\ntion✓ —\\n[6] Substitution\\nbasedfrequency aware random-\\nizationSentiment classi-\\nfication, Natural\\nLanguage Infer-\\nence✓ —\\n[140] Synonym sub-\\nstitutionRandomized synonym\\nsubstitution & voteSentiment, Topic,\\nNews category\\nclassification✓ —\\n[127] Synonym sub-\\nstitutionRandom perturbation de-\\nfenseSentiment analysis ✓ —\\n[147] Attacks from\\nTextAttack\\n[91, 161]TCAB attack identifica-\\ntion dataset & Text, Lan-\\nguage, Classifier proper-\\ntiesSentiment,\\nAbuse/No-abuse\\nclassification✓ —\\n[136] Word level\\nperturbationsFinding word importance sentiment classifi-\\ncation✓ —\\n[158] Word level\\nperturbationsFeature density estima-\\ntionSentiment, News\\ncategories, Topic\\nclassification,✓ —\\n[116] Misspellings ScRNN- Spell checker — ✓ ✓\\n[102] Misspellings ScRNN with Backoff Sentiment Analy-\\nsis✓ ✓\\n[58] Misspellings,\\northographic\\nattacksContext independent\\nprobability distributionRestoring sen-\\ntences✓ ✓\\n[31] Backdoor at-\\ntacksTrigger word manipula-\\ntion and BERTScoreMachine trans-\\nlation, Dialogue\\nGeneration✓ ✓\\n[172] Word pertur-\\nbationsRecover perturbed to-\\nkens with small world\\ngraphsText classification ✓ ✓\\n[10] Misspellings Embeddings similar to\\noriginal words, Rule\\nbased methodsEngagement Bait\\nClassifier✓ ✓\\n[174] Synonym\\nsubstitution,\\nreplacement\\norder strategyUsing reference models Sentiment analysis ✓ ✓\\n[72] Word level\\nperturbationsNMT model is used for re-\\nmoving noiseText classification ✓ ✓\\nTable 6. Summary of defense schemes proposed for perturbation identification and correction\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='77940db5-8007-4df7-8212-7c5f34f3d956', embedding=None, metadata={'page_label': '21', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 21\\nWork NLP Taks Perturbation\\n[119] Sentiment classification Word embeddings\\n[166] Sentiment classification Word embeddings\\n[111] Malware detection Word embeddings for API call command\\nTable 7. Summary of perturbation direction control based adversarial defense methods in NLP\\n6 ROBUSTNESS BY CERTIFICATION\\nThe methods discussed in the previous sections for adversarial defenses involved word/character substitution-based\\nadversaries where words are synonyms to make the perturbation look indistinguishable. Other methods tweak words by\\ninserting characters, changing spellings, and deleting/swapping characters. All these adversarial samples are necessary\\nfor defending the models but they are not sufficient. An attacker can generate millions of adversarial examples by\\nmodifying every word in a sentence. A defense algorithm based on adversarial training requires a sufficient amount\\nof adversarial data to increase the robustness, which still does not cover a lot of unseen cases which are generated\\nby exponential combinations of different words in a text input. Also, perturbation control-based methods require\\nidentification of perturbations on the basis of already-seen perturbations with a prior assumption of the type of attack.\\nThese methods have limitations in their performance when model is exposed to new adversarial instances. Hence, there\\nis a separate set of adversarial defense methods in the literature which are driven by “certification\". These methods\\ntrain the model to provide an upper bound on the worst-case loss of perturbations and hence provide a certificate of\\nrobustness without exploring the adversarial space.\\n6.1 Interval Bound Propagation based methods\\nInterval Bound Propagation (IBP) [ 37] is a bounding technique, extensively used in images for training large, robust,\\nand verifiable neural networks. Training the neural networks with IBP technique tries to minimize the upper bound on\\nthe maximum difference between the classification boundary and input perturbation region. IBP lets you include the\\nloss term in the training, using which the last layer of the perturbation region can be minimized and kept on one side of\\nthe classification boundary. Now, this adversarial region is tighter enough and can be said certified robust.\\nIn this line, the work [ 51] proposed certified robust models while providing maximum perturbations in text clas-\\nsification. Authors used interval bound propagation to optimize the upper bound over perturbations. IBP gives an\\nupper bound over the discrete set of perturbations over word vector space. IBP computes an upper bound on the\\nmodel’s loss when given an adversarially perturbed input. This bound is computed in a modular fashion. In another\\nwork [ 44] introduced a verification and verifiable training of neural networks in NLP. Authors proposed a tighter\\nover-approximation in the form of a ‘simplex’ in embedding space in the input to generate perturbations. To make\\nthe network verifiable they define it as the convex hull of the all the original unperturbed inputs as a space of delta\\nperturbation. Using IBP algorithm they generated robustness bounds (by generating bounds for each layer). In the\\nwork, [ 157] proposed structure-free certified robust models which can be applied to any arbitrary model. This method\\novercomes the limitations of IBP based method in which they are not applicable to character level and sub-word level\\nmodels. They prepared a perturbation set of words using synonym sets, top-K nearest neighbors under the cosine\\nsimilarity of GLOVE vectors, where K is a hyperparameter that controls the size of the perturbation set. They further\\ngenerated sentence perturbations using word perturbations and trained a classifier with robust certification. In the\\ncontext of IBP methods, [ 131] demonstrates the lack of generalizability of IBP-based methods for novel contextual\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f4a25e85-2218-4097-90cd-7b550d7bac58', embedding=None, metadata={'page_label': '22', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='22 Shreya Goyal, et al.\\nembeddings and a wider range of NLP tasks. They demonstrated the performance of their method in the sentiment\\nanalysis task.\\n6.2 Certified robustness for RNN networks\\nDespite having a plethora of work in finding a certificate for robustness, there is a lack of applicability in RNN based\\nnetwork due to their inherent complexity. Hence, in another line of work for robustness by certification, certified\\nrobustness for RNN based networks and self-attentive networks is proposed. In this line, the work Popqorn [ 61] proposed\\ncertified robustness for RNN based networks such as LSTM, GRUs. The challenge is to find a certificate of robustness in\\nRNN based networks with their complex feedback architectures, the sequential inputs, and the cross-nonlinearity of\\nthe hidden states. Authors used 2D planes to bound the cross nonlinearity in LSTMs and proposed to find a certificate\\nwithin a𝑙𝑝ball (attack distance) if the lower bound on the true label output unit is larger than upper bounds of all other\\noutput units. They generated certificate of robustness by writing all the bounds as a function of epsilon and tried to\\nfind the optimum value of epsilon using a binary search procedure. In the work Cert-RNN [ 27], they overcame the\\nlimitations of Popqorn [ 61] by a robust certification framework for RNNs. The method overcame the limitations of\\nPopqorn by keeping the inter-variable correlation and speeding up the non-linearities of RNN for practical uses. Authors\\ncreated a zonotope [ 30] around the input perturbations and used that to be passed through a vanilla RNN or LSTM.\\nThe properties of the output zonotope can be verified to be certifiably robust. They used zonotope instead of a box to\\npreserve inter-variable correlation, the precision of the network, and achieve a tighter bound. They could achieve tighter\\nbounds and at least 19 times faster framework than Popqorn. In this line, [ 168] proposed a novel approach, Abstractive\\nRecursive Certification (ARC) for certified robustness in RNN based networks. Authors defined a set of programmatically\\nperturbed string transformations and constructed a perturbation space using those transformations proposed in [ 165].\\nThey memorized the hidden states of the strings in the perturbation space that shared a common prefix to reduce\\nthe evaluation of LSTM cells while finding an upper bound to the loss and avoiding re-computing of hidden states.\\nFollowing that they represent all the perturbation sets as a hyperrectangle and pass the hyperrectangle through the\\nremaining network using IBP technique [ 37]. Following a similar direction, the work in [ 114] presents Polyhedral\\nRobustness Verifier of RNNs (PROVER) which represents the perturbations in input data in the form of polyhedral which\\nis passed through a LSTM network to obtain a certifiable verified network for a more general sequential data. Another\\nwork in this direction is proposed by [ 12] where authors proposed DeepT an abstract transformer-based network\\ncertification method. They attempted to certify larger transformers against synonym replacement-based attacks. In\\nthis work, authors propose to use multi-norm zenotopes improving the precision of standard zenotope based methods\\nwhich works well for longer sentences by certifying larger radii of robustness ( ×28of existing methods). In another\\nwork [ 121] proposed an algorithm for verifying the robustness of transformers with self-attention layers which include\\nchallenges such as cross-linearity and cross-positional dependency. They provide a lower bound to a boundary (delta\\ncertificate) which will be always greater than 0 (probability of correct class is always higher than incorrect class) within\\na set of inputs which also include perturbations and tighter than IBP. They achieved this by computing lower/upper\\nbound for each neuron with respect to the input space.\\n6.3 Other convex optimization based methods\\nThere are other methods in literature for finding certified robustness for neural networks which used several convex\\noptimization schemes and randomized smoothing-based schemes. In this line, [ 124] certified defence method is proposed\\nfor text classification task. They consider data sanitation defences, which examine the entire datasets and try to remove\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a768eeda-e6f2-4adc-8dca-9f35b933956b', embedding=None, metadata={'page_label': '23', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 23\\npoisoning points. They upper bound the worst possible test loss of any attack which works in an attacker-defender\\nsetting at the same time. They generated a certificate of robustness (upper bound) by inserting perturbed data at the\\ntime of training where defender is learning to remove outliers at each iteration. Upper bound fits all possible points that\\nevade outlier removal. In the work [ 104] authors proposed certified robustness method based on semi-definite relaxation.\\nThey computed an upper bound on the worst case loss of the neural networks with one hidden layer. The computed\\ncertificate of robustness provides an upper bound on the robustness for all kinds of attacks and being differentiable\\nthey trained it jointly with the network. The work [ 135] provided a certificate of robustness with the idea of differential\\nprivacy in the input data. They implemented differential privacy in the textual data by treating a sentence as a database\\nand words as an individual records. If a predictive model satisfies a certain threshold (epsilon-DP) for a perturbed input,\\nits input should be the same as the clean data. Hence providing a certification of robustness against L-adversary word\\nsubstitution attacks.\\nCertification\\nmethodWork NLP Taks Models Perturbations\\nIBP[51] NLI, Sentiment classification Feed Forward network, CNN,\\nBi-Directional LSTM, Decom-\\nposable attentionWord substitution\\n[44] Sentiment & Topic Classifica-\\ntion1 layer convolution network word substitution\\nand character typos\\n[131] Sentiment classification CNN word embeddings\\n[157] Sentiment & Text classification BERT Word Substitution\\nRNN based[61] Question Classification LSTM, GRU 𝜖bounded𝐿𝑝ball\\n[27] sentiment analysis, toxic com-\\nment detection, and malicious\\nURL detectionRNN, LSTM 𝜖bounded𝐿𝑝ball\\n[168] Sentiment Classification LSTM Word substitution\\n[114] Speech classification RNN, LSTM 𝜖perturbation, 𝑑𝐵\\nperturbation\\n[12] Sentiment Classification Transformer networks 𝐿𝑝noise where 𝑝∈\\n{1,2,∞}\\n[121] Sentiment classification Transformer 𝜖bound perturba-\\ntions\\n[124] Sentiment Classification SVM 𝜖𝑛poison points\\n[135] Text classification LSTM Word substitution\\nOther methods[162] Sentiment & Topic classifica-\\ntionBERT & RoBERTa Word substitution\\n& Character level\\n[66] Sentiment, News, Topic classifi-\\ncationCNN, LSTM Word substitution\\n[101] Toxicity, Occupation classifica-\\ntionCNN, BERT Word substitution\\nTable 8. Summary of the certifiable robustness methods in NLP\\nAnother work, [ 162] proposed defense algorithm to overcome the limitations of previous methods with an assumption\\nthat perturbation generation methods will be known a priory. They proposed RanMASK, a certifiably robust defense\\nmethod against text adversarial attacks based on a new randomized smoothing technique for NLP models. Manually\\nperturbed input text is given to the mask language model. Random masks are generated in the input text in order to\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f985016b-11c8-4e70-b993-fe5186663b14', embedding=None, metadata={'page_label': '24', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='24 Shreya Goyal, et al.\\ngenerate a large set of masked copies of the text. A base classifier is then used to classify each of these masked texts,\\nand the final robust classification is made by “majority vote” and trained with BERT and RoBERTa to generate and train\\nwith masked inputs. Another work in this direction [ 66] estimated the Maximum Safe Radius (MSR) for a given input\\ntext, i.e. minimum distance between the classification boundary and embedding space. They quantified the robustness\\nof neural networks against word replacement which is based on a minimum safe radius. They approximated the upper\\nbound using Monte Carlo tree search and the lower bound by constraint relaxation technique of MSR for CNN and\\nLSTM networks. In [ 101], the authors also tried to club the concept of fairness and robustness to increase the robustness\\nof a neural network. They demonstrated that a certified robust model can also be used as a bias mitigation system to\\nbuild trustworthy NLP systems. They integrated a bias mitigation system with state-of-the-art certified robust models\\nto improve the robustness of a model. Table 8 presents the summary of the robustness by certification methods in\\nNLP where rows are grouped by the certification method used, and each row describes the NLP application, machine\\nlearning model used for certification and type of perturbations generated in the threat model with each associated\\nwork. Currently these methods are not proven to be generalized across different types of deep neural networks and\\nthey have been evaluated for small set of NLP tasks and on smaller networks. There are various methods for defending\\nthe neural network from adversarial attacks and achieving robustness which are not discussed in these sections and\\nfollows a different line of approach, described in the next section.\\n7 MISCELLANEOUS\\nIn the previous section, various methods are discussed for adversarial defenses and robustness enhancement. These\\nmethods follow the proposed taxonomy and categorization of adversarial defence schemes discussed in Sec. 3. However,\\nthere are several other schemes proposed in recent years that do not fall into any of the categories discussed above.\\nIn the direction of enhancing robustness by bias reduction, the work [ 122] tries to remove hypothesis-only bias for\\nNLI datasets by using adversarial classifiers to detect bias in the sentence representation. They demonstrated that the\\nlarger the sentence embeddings, the harder it is to remove the bias and requires more adversarial classifiers. They tested\\nmodels with 1 and 20 classifiers where 8 out of 13 datasets performed better with 20 classifiers and for 3 of them 1 and\\n20 gave the same performance.\\nIn another line of defending APIs from adversarial attacks the work [ 40] showed that hosted BERT-based APIs are\\nvulnerable to theft and users can query the API for a dataset and train a BERT model to replicate the API. The replicated\\nmodel can then be used for adversarial example transfer. They suggested a parameter-based defense strategy by using a\\ntemperature parameter in softmax to smooth the output prediction probabilities. They further add perturbation noise\\nwith variance sigma to the output probabilities where the larger the variance stronger the defense.\\nIn the direction of creating various adversarial examples for adversarial training, the work [ 38] proposed variable\\nlength adversarial attack in contrast to an existing method which focuses on fixed length. This is achieved by using\\na special “BLK\" token during fine-tuning and then using 3 atomic operations addition, deletion & replacement to\\ncreate adversarial examples. They show that this method successfully attacks the models in NLU and NAT tasks and\\ndemonstrated its use for creating augmented data for adversarial training. In the same line, the authors in [ 31] presented\\nbackdoor attacks as adversarial attacks during training of the model for NLG models. They proposed post-hoc defense\\nagainst the attacks by using token removal and token substitution on a sentence and corpus level.\\nTaking VAT in different directions, [ 176] proposed a novel strategy, Stackelberg Adversarial Training (SALT) which\\nemploys a Stackelberg game strategy. There’s a leader which optimizes the model and a follower which optimizes the\\nadversary. In this Stackelberg strategy, the leader is advantageous knowing the follower’s strategy and this information\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='186d1438-3c84-43ff-bf2f-a51491a9763e', embedding=None, metadata={'page_label': '25', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 25\\nis captured in Stackelberg gradient. They find the equilibrium between the leader and follower using an unrolled\\noptimization approach.\\nAnother work in the direction of robustness enhancement, proposed in [ 53] introduced Robust Encodings (RobEn),\\nwhich is a simple framework that guarantees robustness, without making any changes to model architecture. The core\\ncomponent of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings on a\\ntoken level. They attempt to cluster all possible adversarial typos into a single cluster using graph-based agglomerative\\nclustering and try to balance between having too many words in a cluster versus a single word in a cluster. In the\\nsame line, for languages other than English, [ 73] proposed AdvGraph to enhance the adversarial robustness of Chinese\\nNLP models with modified embeddings. Due to the inherent complexity in Chinese language, the existing adversarial\\ndefense models are difficult to be extended for the Chinese language, hence they propose to capture the similarity\\nin words using graphs. They constructed undirected adversarial graph based on the glyph and phonetic similarity of\\nChinese characters and learned the representations through graph embeddings to be used with semantic embeddings to\\nbe used for other downstream tasks.\\nThere are various metrics used in the literature for evaluating the robustness of the models against adversarial\\nattacks. Evaluating adversarial robustness is equivalent to evaluating the performance of a model in the presence\\nof adversarial attacks, before and after the implementation of defense mechanism. Hence, some of these metrics are\\nstandard performance metrics widely used in machine learning literature. Other variants of evaluation metrics are\\nproposed to measure the performance of certifiably robust models. The next section describes these evaluation metrics\\nused in adversarial defense literature.\\n8 METRICS FOR EVALUATION\\nThere are various evaluation metrics that are extensively used in literature for evaluating the proposed defense methods.\\nMajorly these metrics are performance evaluation metrics for the machine learning model which is required to be\\ndefended. Adversarial defense methods are evaluated with the performance of the model after the defense method is\\nimplemented or run-time evaluation in case of methods that aim towards optimizing a lower/upper bound. Hence,\\nsome of these metrics are accuracy, error, loss analysis, measurement of the success of adversarial attacks, similarity\\nwith ground truth in the case of language generation etc. In this section, some of these commonly used metrics are\\ndescribed in detail.\\n•Prediction accuracy (Conventional Accuracy) : Adversarial defense methods for text classification models,\\nsuch as sentiment classification, Natural Language Inferencing tasks, are evaluated on the basis of the prediction\\naccuracy after implementation of defense algorithm. The prediction accuracy after defense is compared with\\nprediction accuracy after attack, and if there is a surge in accuracy, the defense method is considered to be\\nsuccessful. It is defined as the fraction of test set that is correctly classified [ 135]. Conventional accuracy is a\\nstandard metric that is used to evaluate any deep learning system and it can be used to evaluate any defense\\nmethod.Í𝑇\\n𝑡=1𝐶𝑜𝑟𝑟𝑒𝑐𝐶𝑙𝑎𝑠𝑠(𝑋𝑡,𝐿,𝜖)\\n𝑇\\nHere,𝐶𝑜𝑟𝑟𝑒𝑐𝐶𝑙𝑎𝑠𝑠(𝑋𝑡,𝐿,𝜖)gives 1, if test sample, 𝑋𝑡is correctly classified for test data 𝑇.\\n•Loss function analysis : The negative log-likelihood (loss function) is tested over its rate for adversarial training\\nas regularization, and virtual adversarial training-based methods. It can indicate lower error rate and reduced\\noverfitting in adversarial training based regularization.\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e9e87dd4-e2ec-4d89-a1c7-3d58c1d42e37', embedding=None, metadata={'page_label': '26', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='26 Shreya Goyal, et al.\\n•Error analysis : Adversarial defense methods are also evaluated on the error rate in the prediction of the model.\\nThe error rate is compared with an adversarial attack before and after the implementation of defense schemes. A\\nlesser error rate after defense method, entails its successful defense scheme. Error analysis is a standard metric\\nthat is used to evaluate any deep learning system for prediction and it can be used to evaluate any defense\\nmethod in literature.\\n•Embedding testing : Embedding test is done for evaluating the embeddings generated for adversarial training.\\nSimilarity metrics such as Edit distance, Jaccard similarity coefficient, and semantic similarity metrics are used\\nto evaluate the utility of the adversarial samples generated by finding their similarity with the original input\\nsamples.\\n•Human Evaluation : To measure the utility of the adversarial samples for adversarial training, human evaluation\\nis also performed in literature [ 34]. Human annotators are asked to judge the adversarial examples in terms of\\ntheir naturalness by presenting both original and adversarial examples. The good quality adversarial examples\\nare used in adversarial training and further robustness of the deep neural network is evaluated. Adversarial\\nattacks and defense methods are closely associated, hence weaker the adversarial attack, the stronger the defense\\nstrategy.\\n•Attack Success Rate (ASR) : ASR [ 145] is a measure of success of the adversarial samples created for the\\npotential adversarial attack. This metric is used to measure the effectiveness of the adversarial attack after any of\\nthe defense scheme is implemented. The attack success rate is measured before and after defending the model\\nand a drop in its value will imply a more robust model. It is defined as:\\n𝐴𝑆𝑅=𝑁𝑠𝑢𝑐𝑐𝑒𝑠𝑠𝑓𝑢𝑙\\n𝑁𝑇𝑜𝑡𝑎𝑙∗100\\nWhere,𝑁𝑠𝑢𝑐𝑐𝑒𝑠𝑠𝑓𝑢𝑙 is the number of adversarial samples that were able to successfully fail the model, and 𝑁𝑇𝑜𝑡𝑎𝑙\\nis the total number of adversarial samples generated.\\n•BLEU : Adversarial defense schemes for Natural language generation models utilize performance metrics such as\\nBLEU score [ 98] for the evaluation of their proposed method. BLEU score is measured using 𝑛-gram to evaluate\\nthe quality of the generated natural language by comparing it with ground truth. It is defined as:\\n𝑝𝑛=Í\\n𝐶∈{𝐶𝑎𝑛𝑑}Í\\n𝑔𝑟𝑎𝑚−𝑛∈𝐶𝐶𝑜𝑢𝑛𝑡𝑐𝑙𝑖𝑝(𝑔𝑟𝑎𝑚−𝑛)\\nÍ\\n𝐶′∈{𝐶𝑎𝑛𝑑}Í\\n𝑔𝑟𝑎𝑚−𝑛′∈𝐶′𝐶𝑜𝑢𝑛𝑡(𝑔𝑟𝑎𝑚−𝑛′)\\n𝐵𝑃=\\uf8f1\\uf8f4\\uf8f4 \\uf8f2\\n\\uf8f4\\uf8f4\\uf8f31, if𝑐>𝑟\\n𝑒1−𝑟\\n𝑐, 𝑐≤𝑟\\n𝐵𝐿𝐸𝑈 =𝐵𝑃.𝑒𝑥𝑝Í𝑁\\n𝑖=1𝑊𝑛𝑙𝑜𝑔(𝑝𝑛)\\nWhere(𝑝𝑛)is the n-gram modified precision score, BP is brevity penalty used for longer candidate summaries\\nand for spurious words in it, 𝑐is the length of the candidate summary, and 𝑟is the length of the reference\\nsummary.\\n•Number of Queries : This denotes average number of times attacker queries the model. The higher the average\\nnumber of queries made by an attacker, more difficult is to fail a defense mechanism [ 81]. It can be used for\\nevaluating any adversarially trained defense model using adversarial instances or by controlling the perturbations.\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5200a475-7b3d-4ab5-a791-bca7e66ab5fe', embedding=None, metadata={'page_label': '27', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 27\\n•Precision on certified examples : Precision on certified examples [ 67], which measures the number of correct\\npredictions exclusively on examples that are certified robust for a given prediction robustness threshold. It is\\ndefined as:\\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =Í𝑇\\n𝑡=1(𝑖𝑠𝐶𝑜𝑟𝑟𝑒𝑐𝑡(𝑋𝑡)&𝑟𝑜𝑏𝑢𝑠𝑡𝑆𝑖𝑧𝑒(𝑝𝑡,𝜖,𝛿,𝐿)≥𝑇ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑\\nÍ𝑇\\n𝑡=1𝑟𝑜𝑏𝑢𝑠𝑡𝑆𝑖𝑧𝑒(𝑝𝑡,𝜖,𝛿,𝐿)≥𝑇ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑\\nWhere,𝑖𝑠𝐶𝑜𝑟𝑟𝑒𝑐𝑡(𝑋𝑡)gives a value of 1if the input sample 𝑋𝑡is correctly classified, and 𝑟𝑜𝑏𝑢𝑠𝑡𝑆𝑖𝑧𝑒 gives the\\ncertified robustness value for the bound 𝐿.\\n•F1 Score : This is a metric that combines precision and recalls both, to evaluate the overall performance of the\\nmodel. It is used for comparison of model’s performance before and after defense mechanism implementation.\\nF1 score is a standard metric used for evaluating the performance of deep neural networks and it can be used to\\nevaluate any defense method. It is defined as:\\n𝐹1𝑆𝑐𝑜𝑟𝑒 =2∗(𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛∗𝑅𝑒𝑐𝑎𝑙𝑙)\\n(𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑅𝑒𝑐𝑎𝑙𝑙)\\n•Certified Radius : In certification based adversarial defense methods, robust radius is the largest radius centered\\naround input sample 𝑋𝑡, for which the classifier does not change its value for its corresponding perturbed\\nsample𝑋𝑎𝑑𝑣\\n𝑡. However, calculating robust radius of a deep neural network is a NP-hard problem [ 163]. Hence,\\ncertification based methods are tested for their certified radius for different norms of perturbations for targeted\\nmodel on parameters such as minimum radius, average radius, and time taken to obtain it. Certified radius is a\\nlower bound to the robust radius and leads to a guaranteed upper bound of the robust classification error.\\n•Certificate Ratio (CR) : This metric is used in certification based defense schemes. It is the fraction of testing\\nsamples, that satisfies the certification criteria after prediction [135]. It is defined as:\\n𝐶𝑅=Í𝑇\\n𝑡=1𝐶𝑒𝑟𝑡𝑖𝑓𝑖𝑒𝑑𝐶ℎ𝑒𝑐𝑘(𝑋𝑡,𝐿,𝜖)\\n𝑇\\nHere,𝐶𝑒𝑟𝑡𝑖𝑓𝑖𝑒𝑑𝐶ℎ𝑒𝑐𝑘(𝑋𝑡,𝐿,𝜖)gives 1, if the fraction of the test data is certified robust.\\n•Certified Robustness : This metric is used in certification-based defense schemes. Certified Robustness [ 70] for\\na particular 𝑋𝑡is the maximum value 𝜌for which it is certified that classifier will return the correct label where\\n𝑋𝑎𝑑𝑣\\n𝑡is its corresponding perturbed sample, such that ||𝑋𝑡−𝑋𝑎𝑑𝑣\\n𝑡||≤𝜌.\\n•Median Certified Robustness : This metric is also used in certification-based defense schemes. The Median\\nCertified Robustness [ 70,162] on a dataset is the median value of the certified robustness across the dataset. It is\\nthe maximum value 𝜌for which the classifier can guarantee robustness for at least 50% samples in the dataset. In\\nother words, we can certify the classifications of over 50% samples to be robust to any perturbation within 𝜌.\\n•Certified accuracy : This is the metric used for evaluating the certifiable robust models. Certified accuracy\\n[67,135] is the percentage of correct test samples for a certified robust model for the given perturbation. It\\ndenotes the fraction of testing set, on which a certified model’s predictions are both correct and certified robust\\nfor a given prediction robustness threshold. It is defined as:\\n𝐶𝑒𝑟𝑡𝑖𝑓𝑖𝑒𝑑 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 =Í𝑇\\n𝑡=1(𝑖𝑠𝐶𝑜𝑟𝑟𝑒𝑐𝑡(𝑋𝑡)&𝑟𝑜𝑏𝑢𝑠𝑡𝑆𝑖𝑧𝑒(𝑠𝑐𝑜𝑟𝑒𝑠,𝜖,𝛿,𝐿)≥𝑇ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑\\n𝑇\\nWhere𝑟𝑜𝑏𝑢𝑠𝑡𝑆𝑖𝑧𝑒(𝑠𝑐𝑜𝑟𝑒𝑠,𝜖,𝛿,𝐿)is the certified robustness size for the bound 𝐿and𝑖𝑠𝐶𝑜𝑟𝑟𝑒𝑐𝑡(𝑋𝑡)give a value\\nof1if the input sample 𝑋𝑡is correctly classified for test data 𝑇.\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='17bfc390-7565-4ffb-91ab-96cf6bb53718', embedding=None, metadata={'page_label': '28', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='28 Shreya Goyal, et al.\\n•Conditional Accuracy : This is the metric used for evaluating the certifiable robust models. Conditional Accuracy\\nis proposed by [ 135], evaluating the classification accuracy of both a clean sample 𝑋𝑡and its corresponding\\nadversarial sample 𝑋𝑎𝑑𝑣\\n𝑡withing a bound 𝐿. It checks when 𝑋𝑡is certified within bound L, whether 𝑋𝑎𝑑𝑣\\n𝑡is also\\nclassifying correctly. It is defined as:\\n𝐶𝑜𝑛𝑑𝑖𝑡𝑖𝑜𝑛𝑎𝑙 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 =Í𝑇\\n𝑡=1(𝐶𝑒𝑟𝑡𝑖𝑓𝑖𝑒𝑑(𝑋𝑡,𝐿,𝜖)&𝑐𝑜𝑟𝑟𝐶𝑙𝑎𝑠𝑠(𝑋𝑎𝑑𝑣\\n𝑡,𝐿,𝜖)\\nÍ𝑇\\n𝑡=1(𝐶𝑒𝑟𝑡𝑖𝑓𝑖𝑒𝑑(𝑋𝑡,𝐿,𝜖)\\nWhere,𝐶𝑒𝑟𝑡𝑖𝑓𝑖𝑒𝑑(𝑋𝑡,𝐿,𝜖)gives 1, when clean input sample 𝑋𝑡is successfully certified and 𝑐𝑜𝑟𝑟𝐶𝑙𝑎𝑠𝑠(𝑋𝑎𝑑𝑣\\n𝑡,𝐿,𝜖)\\ngives 1when its perturbed input 𝑋𝑎𝑑𝑣\\n𝑡is also correctly classified.\\n•CLEVER score : Cross Lipschitz Extreme Value for nEtwork Robustness (CLEVER) score is proposed by [ 143]\\nis a novel robustness evaluation metric. It is attack-independent and can be applied to any arbitrary neural\\nnetwork classifier and scales to large networks. CLEVER metric is an estimation of local Lipschitz constant\\nwhich represents “lower bound of the robustness in input data\" or minimum amount of perturbation required to\\na natural sample to fail a classifier. The increased clever score indicates that the network is indeed made more\\nresilient to adversarial perturbations after any defense mechanism is used.\\n9 ADVERSARIAL DATASETS AND FRAMEWORKS\\nThere are several dataset in NLP which are proposed for adversarial evaluation. One such dataset is DailyDialog++ [ 115],\\nwhich is an extension of DailyDialog dataset [ 80] and adversarial dialogue generation dataset. DailyDialog++ contains, 5\\nadditional relevant and adversarially irrelevant responses, for 11k context conversation derived from DailyDialog. This\\ndataset is used for evaluating robustness of dialogue generation models with adversarial examples in the dataset against\\nadversarial attacks. Further these adversarial examples are used in adversarial training to imporve the performance\\nof dialogue generation models. Another work is ANLI [ 94], which proposed adversarial dataset for natural language\\ninference systems. ANLI composed of adversarial examples for NLI collected in 3 iterative rounds having human\\nand machine in loop. ANLI consist of 103𝑘examples of sentences, starting with short multi-sentence passages from\\nWikipedia and having annotators writing adversarial hypothesis. In this process, they tested these samples with\\nstate-of-the-art NLI models and got then verified by human annotators hence proposing human-and-model-in-the-loop\\nenabled training (HAMLET) scheme for data collection. The adversarial examples collected in this dataset are used in\\nadversarial training to improve the performance of Natural Language Inferencing models.\\nIn the same line, authors in [ 76] proposed a novel large scale dataset adversarial VQA for visual question answering\\ntask using the HAMLET scheme proposed in [ 94]. In this work they presented an image to an annotator and ask them\\nto write a tricky question that could fool a model. Hence they iteratively collected 243.0𝐾questions for 37.9𝐾images by\\nhaving humans and models competing in the loop in 3rounds. This dataset is used in evaluating the robustness of SOTA\\nVQA models. Authors have further used this dataset for data augmenting, for the purpose of adversarial training and\\ndemonstrated a higher performance on robust VQA benchmarks. For the purpose of evaluating adversarial examples for\\nquestion answering task, authors in [ 50] proposed, Adversarial SQuAD dataset, that contained adversarially inserted\\nsentences. These sentences are automatically generated in a concatenative manner, without changing the meaning of\\nthe paragraph or question. Fake answers to these questions are also generated with same POS type. This dataset is\\nused for evaluating the performance of various state-of-the-art question answering models. The best performing model\\nwith adversarial examples is analysed for the features causing the high performance. Based on this analysis, additional\\nfeatures were incorporated in the model, separately and in combination, to enhance its performance both with and\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='081578fa-8adb-4bc8-b02f-356b2123fd6a', embedding=None, metadata={'page_label': '29', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 29\\nwithout adversarial examples. Authors further used this dataset for adversarial training of modified model to validate\\nits stability. In the same line, [ 129] proposed a question-answering test bed Quizbowl, using a Human-In-the-Loop\\nframework. In this work, human authors are asked to write adversarial questions which are designed to fail state-of-\\nthe-art question answering models appearing ordinary to human. This test dataset is used for adversarial evaluation of\\nSOTA question answering models.\\nIn another work, [ 169] adversarial dataset, namely, Paraphrase Adversaries from Word Scrambling (PAWS), for\\nparaphrase detection is proposed. PAWS is generated from sentence in Quora and Wikipedia, where adversarial samples\\nare generated using language model based controlled word swapping and back translations. PAWS dataset is used for\\nadversarial evaluation of paraphrase detection models and measured sensitivity of these models on word order and\\nsyntactic structure. Authors further used this dataset for adversarial training of state-of-the-art paraphrase detection\\nmodels. In the direction of perturbation identification [ 147] proposed a dataset Text Classification and Attack Benchmark\\n(TCAB), which was created for the purpose of detecting and labeling the textual perturbation in the input text. TCAB\\nis an extensive dataset containing 1.5million attack instances, generated using 12attacks from the toolkits [ 91,161]\\nwhich are targeting 3classifiers, trained on 6domain datasets of sentiment and abuse classification. There is a total\\nnumber of 216attacks taken into consideration in TCAB dataset which are a combination of different type of attacks.\\nThis dataset consists of a total of 1,53,9881 adversarial examples along with the clean instances taken from the original\\ndatasets. They further use text, language, classifier properties to detect the perturbation and type of the attack in the\\ninput text. Table 9 shows the summary of the publicly available adversarial datasets for various NLP tasks. It describes\\nthe task for which the dataset is proposed, its statistics and methods used in data collection and annotation.\\nDataset Task Statistics Method\\nDailyDialog ++ [115] Dialogue Generation 11𝑘context, 5responses Human Annotators\\nANLI [94] NLI systems 103𝑘sentences Human & machine in loop\\nAdversarial VQA [76] Visual Question Answering 243.0𝑘Q & 37.9𝑘images Human & machine in loop\\nAdversarial Squad [50] Question Answering 107,785Q Machine generated\\nQuizbowl [129] Question Answering 1213 Q-A pairs Human & machine in loop\\nPAWS [169] Paraphrase detection 108,463paraphrase pairs Machine generated\\nTCAB [147] Sentiment, Abuse classification 1.5𝑚Adversarial ins. Machine generated\\nTable 9. Summary of adversarial datasets in NLP, here NLI = Natural Language Inference, Q= Questions, A= Answers, ins= instances\\nThere are papers in the literature that proposed python frameworks for a complete adversarial evaluation for several\\nNLP tasks with various attack algorithms. One such work is TextAttack [ 91] which is a python framework for end-to-end\\nadversarial evaluation with 16 different adversarial attack methods. It consists of a task-specific goal function, data\\naugmentation schemes along with perturbation constraints that validate the perturbation with original inputs, and a\\nrepetitive model querying search system. It facilitates the user to benchmark existing attacks, and create novel attack\\nschemes by using new and existing components and evaluating them. Improving the shortcomings of TextAttack\\nframework, Open attack [ 161] is proposed, which included 15different types of attacks such as sentence, word, character\\nlevel. It also supports Chinease in addition to English language models and supports multi-process running of attack\\nmodels to improve attack efficiency. In the same line another evaluation framework [ 151] “Elephant in the room\"\\nis proposed, which consists of a combination of automatic evaluation metrics and human judgments. Targeting the\\nsentiment classification task, it included crowd-sourced human judgments, for judging the naturalness, preservation of\\noriginal label, and comparing similarity on a text similarity metric.\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e379d771-ce47-4ff1-acd8-4acf7d1bed45', embedding=None, metadata={'page_label': '30', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='30 Shreya Goyal, et al.\\n10 RECOMMENDATIONS FOR FUTURE WORK\\nIn this paper, an exhaustive survey of the methods proposed in the literature to defend neural networks from adversarial\\nattacks and to enhance their robustness is presented. We proposed a novel taxonomy for adversarial defense mechanisms\\nfor various tasks in NLP. Methods for adversarial defense in NLP are broadly divided into three categories, (i) methods\\nbased on adversarial training, (ii) methods based on perturbation detection (iii) methods providing a certificate for\\nrobustness. Another part of methods which do not follow any of the above mentioned schemes is categorized as\\nmiscellaneous. While there is an ample amount of work proposed in this direction for tasks in NLP, there are still\\nvarious gaps remaining which should be looked at as potential future directions in this area.\\n•Larger part of work based on adversarial training : A large portion of work in adversarial defenses for\\nNLP revolves around adversarial training and data augmentation. Despite having a plethora of work in this\\ndirection, there is a large part of adversarial defense methods which are oriented towards augmenting the data\\nfor generating adversarial examples for adversarial training. Adversarial training is undoubtedly a successful\\ndefense scheme for adversarial attacks but it lacks generality for a more practical purpose. It makes the model\\nhighly robust for a certain kind of attack but it still makes it vulnerable to the type of examples the model has\\nnot seen. Hence, the other methods for adversarial defenses should also be explored.\\n•Hand-crafted generation of adversaries : As a future work recommendation, more attention should be given\\nto the automatic generation of adversarial examples. Most of the methods based on adversarial training with\\ndata augmentation rely on hand-crafted adversarial examples. There are methods that replace words with their\\nsynonyms, or adjacent words, flip the character or concatenate words at the end of sentences. Despite being\\nhighly efficient, these examples are devised by humans rather than having an automatic generation of adversarial\\nexamples and have their own limitations. Hence, more efforts can be put in the direction of the automatic\\ngeneration of adversarial examples.\\n•Interpretability of perturbations in text data : Adversarial examples should be less human perceptible/natural\\nlooking to make the model robust in a practical attack scenario. In contrast with adversarial defenses for\\ncomputer vision based methods and examples generated on images, examples on text for NLP tasks are difficult\\nto generate because of their discrete nature. Modifying pixels in images for generating adversarial examples\\nare less perceptible to human than modifying a character or word in an input text. Hence to make to defense\\nmethod more useful in a practical scenario, more efforts should be put in this direction.\\n•Exact robust certificate calculation : In existing literature only the upper and lower bound on the certificate\\ncould be calculated, rather than the exact robustness certificate. While adversarial training based defense methods\\nprovided a sufficient exposure towards achieving robust neural networks, progressively novel adversarial attacks\\nkept rolling in. A new set of methods to achieve robustness were proposed in the direction of providing a certificate\\nof robustness for a neural network, attempting to put an end to this race. Certification based adversarial defense\\nmethods, definitely provides a more generalization for the neural network for a task but certification does not\\nmake a sufficient property of a model for achieving robustness. Finding an exact robustness certificate to the set\\nof input is a non-convex optimization problem and is inefficient to solve. However, in literature authors have\\nrelaxed this problem to convex optimization by finding an upper or lower bound to the robustness certificate.\\nDespite the efforts towards finding a certified robustness convex optimization can lead to lossy results and there\\nis a scope of finding a tighter bound. Hence future efforts in this direction can be made to improve the tightness\\nof existing robustness certificates.\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ee683f63-eb96-4959-afa2-777ca4152a24', embedding=None, metadata={'page_label': '31', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 31\\n•Scalability of certification based robustness : Certification methods, do not scale to large and practical\\nnetworks used in solving modern machine learning problems. The current certification based robustness method\\nin literature, is implemented on theoretical models on a small scale. They are not scalable to larger and deeper\\nnetworks for practical purposes. Hence, in future attempts should be made in this direction.\\n•Generalization of adversarial training : The current state of the art methods based on adversarial training\\nin NLP are designed in a task specific manner. There is a lack of generality in adversarial example generation\\nschemes which could be used for multiple NLP tasks effectively. Therefore, steps can be taken in this direction in\\nthe future.\\n•More explainable models in case of inserting perturbations in the loss function : There is a lack of\\nexplainability and transparency in the regularization based defense methods. The loss function contains the\\nterm responsible for introducing perturbations at the training time. However, there is no explanation for these\\nmethods for their correctness or high accuracy.\\n•Interpretable adversarial training : While adversarial training based method covers a larger portion of ad-\\nversarial defense literature in NLP, these models are hardly interpretable in terms of adversarial instances.\\nAdversarial examples are provably helping these models become more robust toward adversarial attacks, but\\nthere are questions such as, “how they are improving the performance\", “is this list of adversarial instances\\nexhaustive or could there be more such instances\", “is the model robust towards some specific type of attacks or\\nthey are able to defend from any type of attacks\", are still need to be answered.\\n•Attack agnostic perturbation detection : A better part of the literature in perturbation detection methods\\nworks on the prior assumption of spelling-based attacks or synonym substitution based attacks. However,\\nadversarial perturbation in textual data could be caused by multiple types of attacks, individually or in combination.\\nHence, there is a requirement for perturbation detection schemes that attack agnostic and present a general\\nframework for perturbation detection.\\n•Novel methods to identify the existing perturbations in the input : A large part of perturbation detection\\nschemes depends upon spell checking methods and methods which enumerate or cluster synonyms. In a practical\\nscenario, it is highly inefficient to compute and enumerate synonyms of words for perturbation recognition.\\nHence, there is a requirement for novel and innovative methods for identifying the perturbations in the input\\ntext which do not involve traditional methods such as spell checking, synonym mapping, or their other variants.\\n•Better coverage for NLP applications : The proposed adversarial defense methods evaluate their method on\\ncertain NLP tasks to validate strength of their method. In addition to being limited to certain type adversarial\\nattacks, a large part of defense methods demonstrate their algorithm on different types of text classification tasks.\\nThese tasks include sentiment, hate, news, topic, abuse, malware classification type of problem which comes\\nunder the umbrella usecase of text classification. Text classification is one of the Natural Language Understanding\\n(NLU) task, while there are other NLU tasks such as named entity recognition, machine translation, automatic\\nreasoning etc. having lesser coverage. Moreover, a very limited number of defense schemes have demonstrated\\ntheir methods on Natural Language Generation (NLG) use cases such as summary generation, question answering.\\nHence, there is a requirement for adversarial defense methods covering the other NLP applications.\\n•Better evaluation metrics : The current evaluation of robustness against adversarial attacks for NLP models is\\nbased on the performance metrics of the actual model, i.e. accuracy, precision-recall, error-analysis, etc. Hence,\\nthere is a requirement for novel evaluation metrics that could measure the robustness and ability to defend\\nagainst adversarial attacks. There is also a requirement for sensitivity metrics for machine learning models, for\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='0068055f-5ed0-44e6-b534-d1b129651786', embedding=None, metadata={'page_label': '32', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='32 Shreya Goyal, et al.\\nmeasuring their sensitivity towards adversarial examples. There also ain’t enough ways to evaluate defense\\nmechanisms themselves along the lines of perceptibility and naturalness. Hence, in future, more evaluation\\nmetrics can be brought along this line.\\n11 CONCLUSION\\nIn this paper, a survey is presented for adversarial defense methods for various tasks in NLP. We proposed a novel\\ntaxonomy for adversarial defenses in NLP covering a wide range of recently proposed papers. This survey tries to fulfill\\nthe gaps in existing surveys where adversarial attack schemes were more focused. However, in recent years, numerous\\nmethods are proposed for defending neural networks with adversarial attacks and enhancing their robustness. Coming\\nup with novel defense schemes for advanced NLP systems is as important as coming up with novel attacks to make\\nthese neural networks robust and safe for practical purposes. This survey also covers various adversarial datasets,\\nframeworks proposed in recent times for efficient adversarial evaluation of the SOTA models, and evaluation metrics to\\nquantify their robustness. Moreover, it highlights various recommendations for future work considering the limitations\\nand gaps in the existing literature on adversarial defenses. This survey, therefore, provides a strong basis and motivation\\nfor future research in developing robust and safe neural networks in NLP tasks.\\nA VARIOUS NATURAL LANGUAGE PROCESSING TASKS\\nIn this section various NLP tasks are described in detail, on which adversarial defense schemes are demonstrated in\\nliterature. These tasks include applications of both Natural Language Understanding (NLU) and Natural Language\\nGeneration (NLG).\\nA.1 Text classification\\nText classification is an essential NLP task that involves classifying text data into multiple categories. It encompasses a\\nvast array of problems, including but not limited to sentiment analysis, natural language inference, malware detection,\\nspam filtering, and topic labeling. In the text classification process, a contextual representation of the input text is\\ngenerated, followed by the selection of an appropriate classifier [ 1,63,79]. This process may also involve preprocessing\\nand intermediate steps such as tokenization, lemmatization, stemming, and dimensionality reduction.\\nA.2 Named Entity Recognition\\nNamed Entity Recognition (NER) is a NLP task that involves identifying proper nouns or rigid entities in text such as\\norganizations, persons, locations, and quantities [ 75,126,152]. This is an important task for various applications like\\nquestion answering, information retrieval, relation extraction, text summarization, and machine translation. There are\\nfour major categories of NER techniques, including Rule-based NER that uses handcrafted rules, unsupervised learning\\napproaches that rely on clustering based on contextual similarity, supervised learning approaches that employ feature\\nengineering with conventional classification schemes such as HMM, SVM, or other classifiers, and Deep learning-based\\napproaches [75, 126, 152].\\nA.3 Part-of-Speech tagging\\nPart-of-speech (POS) tagging is an NLP task that involves assigning a specific category to each token in a given text, such\\nas verb, noun, or adjective. POS tagging methods are classified into four categories: rule-based, stochastic, transformation-\\nbased, and HMM-based [ 55]. Rule-based taggers use a set of predefined rules to tag words. Stochastic methods, on\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9dd64630-59c9-4927-a0e2-e6d7353ab0bf', embedding=None, metadata={'page_label': '33', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 33\\nthe other hand, are probability-based and use frequency information to disambiguate the tags. Transformation-based\\ntaggers combine both rule-based and stochastic methods, where a small set of rules are learned from the data and\\ntagging is transformed in each cycle. HMM-based taggers find the most probable sequence of POS tags for a given input\\nsequence, modeled using Hidden Markov Model (HMM) [14, 54].\\nA.4 Machine comprehension and Question answering\\nQuestion answering is a task that has been extensively studied in the NLP literature, with the goal of building systems\\nthat can automatically generate answers to questions posed in a given context. This task has numerous applications,\\nincluding the development of chatbots and dialogue generation systems. In order to train a system to perform this\\ntask, a neural network is trained on a large dataset of contexts, questions, and their respective answers, learning the\\nrelationships among them. The SQuAD dataset [ 105], for example, has been proposed for this purpose, containing a\\nlarge number of context paragraphs, questions, and answers.\\nA.5 Automatic summarization\\nAutomatic text summarization is a challenging NLP task that involves creating a shorter version of a larger text\\ndocument. The task requires the selection of essential information from the entire text and condensing it using the\\nsentences available in the document or a different set of vocabulary. There are two ways to perform text summarization:\\nextractive summarization and abstractive summarization, which are determined by the available training data and\\nthe desired output. Extractive summarization involves selecting keywords, phrases, and lines from the document and\\ncombining them to create a summary, making it useful when the available training data is limited [ 41,112]. On the\\nother hand, abstractive summarization is a data-driven approach that involves training a machine learning model to\\ncreate a summary using the learned language in the available data [ 41,93]. Abstractive summarization requires a large\\namount of data but produces a high-quality summary of the text document.\\nA.6 Machine translation\\nThe task of machine translation involves the conversion of text from a source language to a target language, which can\\nbe either in the form of sentences or documents. Based on the availability of a large training corpus and similarities\\nbetween languages, MT systems can be modeled as either monolingual or multilingual [ 15,106,153]. However, the task\\nbecomes more complex when dealing with diverse domains such as legal, medical, and cultural texts. Additionally, the\\nmode of text, whether formal written [ 7] or informal spoken [ 117], poses a challenge for the task. Moreover, machine\\ntranslation is susceptible to hallucinations [107] and can be vulnerable to adversarial attacks.\\nA.7 Dialogue generation\\nAutomatic dialogue generation is a crucial NLP task that focuses on building conversational systems capable of\\ngenerating responses automatically, with the goal of creating a human-like conversation system. These systems can be\\neither task-oriented, designed to operate within a specific domain such as transportation, restaurants, or shopping, or\\nopen dialogue systems for open-ended conversations, such as everyday conversation between two people. Dialogue\\ngeneration systems have evolved over three phases, including rule-based systems, retrieval-based systems , and neural\\ngenerative conversation systems [ 125]. The quality of the generated response is dependent on the system’s intelligence,\\nas it should be consistent with the previous statement and align with the context of the conversation and targeted\\ndomain (if applicable).\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3b0a90a6-066a-4634-b96b-b6a18c4b2793', embedding=None, metadata={'page_label': '34', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='34 Shreya Goyal, et al.\\nA.8 Question generation\\nAutomatic question generation is a NLP task that is designed to facilitate educational assessment by generating questions\\nautomatically from a given text. This task is important as it helps to reduce manual labor and time. The questions\\ngenerated by these systems should be well-phrased and must have answers that can be found in the given piece of text.\\nThese questions can be of two types: subjective or objective [ 23]. Question generation systems have applications in a\\nvariety of fields such as Massive Open Online Courses (MOOC), healthcare systems, chatbots, and search engines [ 95].\\nB ILLUSTRATIONS OF SOME ADVERSARIAL DEFENSE METHODS IN NLP\\nIn this section some of the popular adversarial defense schemes in NLP are described with illustrations, across various\\ncategories proposed in the paper.\\nB.1 AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples\\nIn the direction of GAN based adversarial training, the work proposed in \"AdvEntuRe: Adversarial Training for Textual\\nEntailment with Knowledge-Guided Examples\" [ 56] uses adversarial training with GANs for the textual entailment\\ntask. The generator and discriminator are trained in an end-to-end manner, where generator (seq2seq) is trained for\\ngenerating adversarial examples using external knowledge or handwritten rules. Discriminator is trained in the same\\nmanner to learn the textual entailment for the generated samples. Figure 2 presents the overall pipeline proposed in\\nthis work for GAN-based adversarial training.\\nFig. 2. Overall pipeline of Adventure- Knowledge guided textual entailment [56]\\nB.2 Natural Language Adversarial Defense through Synonym Encoding\\nIn the direction of perturbation control based adversarial defense methods, \"Natural Language Adversarial Defense\\nthrough Synonym Encoding\" [ 139] proposed perturbation identification scheme. Perturbations related to word modifica-\\ntions which included insertion, deletion, substitution or swapping of words are identified in several ways. In this work,\\nthe authors proposed defense mechanism, against synonym substitution, calling it “Synonym Encoding Method”(SEM).\\nThey essentially clustered all the synonyms in embedding space with their euclidean distances and then encoder is\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3c23a0a6-0383-4839-bded-97da70e3a1a1', embedding=None, metadata={'page_label': '35', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 35\\nlayered before input to train the model. Encoder is responsible for identifying all the synonym substitution-based\\nattacks in the model and maps all the synonyms to a unique encoding without adding extra data for training. Figure 3\\ndemonstrates the Synonym Encoding Method proposed described above.\\nFig. 3. The neighborhood of a data point x in the input space in [ 139]. (a) Normal training has unseen data points 𝑥, which leads\\nto wrong classification. (b) Training with infinite labeled data to overcome all possible adversarial attacks. (c) Training with data\\nwith shared labels, in which all the neighboring points share the labels. (d) Mapping the neighboring points to center 𝑥to remove\\nadversaries.\\nB.3 Interpretable Adversarial Perturbation in Input Embedding Space for Text\\nThe proposed work \"Interpretable Adversarial Perturbation in Input Embedding Space for Text\" [ 119], under perturbation\\ndirection control category alters the direction of the perturbations towards the cleaner text input limiting the adversarial\\nspace. Along this line, this work proposed an interpretable adversarial training method by restricting the direction\\nof adversarial samples. The direction of perturbation is restricted to the words in the existing vocabulary so that\\nperturbations could be interpreted even after adversarial training. Figure 4 shows the mechanism to restrict the direction\\nof the perturbations.\\nFig. 4. Restricting the direction of the perturbations proposed in [ 119]. The proposed method (iAdvT) restricts the perturbations in\\nthe direction of input word embeddings, while the previous methods (AdvT) lets them choose any direction\\nB.4 SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions\\nIn the work, \"SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions\" [157] in\\nthe direction of robustness by certification, proposed structure-free certified robust models which can be applied to\\nany arbitrary model. This method overcomes the limitations of IBP based method in which they are not applicable\\nto character level and sub-word level models. They prepared a perturbation set of words using synonym sets, top-K\\nnearest neighbors under the cosine similarity of GLOVE vectors, where K is a hyperparameter that controls the size of\\nthe perturbation set. They further generated sentence perturbations using word perturbations and trained a classifier\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='45a5d249-657a-42bf-a06b-d7c713fd38b9', embedding=None, metadata={'page_label': '36', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='36 Shreya Goyal, et al.\\nwith robust certification. Figure 5 presents the overall pipeline to achieve certified robustness. In the context of IBP\\nmethods,\\nFig. 5. Pipeline for certified robustness approach proposed in [157]\\nB.5 Certified Robustness to Word Substitution Attack with Differential Privacy\\nThe work \"Certified Robustness to Word Substitution Attack with Differential Privacy\" [ 135] proposed in the direction\\nof robustness by certification, provided a certificate of robustness with the idea of differential privacy in the input\\ndata. They implemented differential privacy in the textual data by treating a sentence as a database and words as an\\nindividual records. If a predictive model satisfies a certain threshold (epsilon-DP) for a perturbed input, its input should\\nbe the same as the clean data. Hence providing a certification of robustness against L-adversary word substitution\\nattacks. Figure 6 demonstrates the certified robustness framework proposed in [135].\\nFig. 6. Word substitution attack and certified robustness via wordDP proposed in [135]\\nB.6 Adversarial NLI: A New Benchmark for Natural Language Understanding\\nIn the direction of adversarial datasets and framework, ANLI [ 94], proposed adversarial dataset for natural language\\ninference systems. ANLI composed of adversarial examples for NLI collected in 3 iterative rounds having human\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='94338e06-1b5b-44f5-becd-03c337a89e46', embedding=None, metadata={'page_label': '37', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 37\\nand machine in loop. ANLI consist of 103𝑘examples of sentences, starting with short multi-sentence passages from\\nWikipedia and having annotators writing adversarial hypothesis. In this process, they tested these samples with\\nstate-of-the-art NLI models and got then verified by human annotators hence proposing human-and-model-in-the-loop\\nenabled training (HAMLET) scheme for data collection. The adversarial examples collected in this dataset are used in\\nadversarial training to improve the performance of Natural Language Inferencing models. Figure 7 depicts the dataset\\ncollection framework for ANLI dataset proposed in [94].\\nFig. 7. Adversarial NLI data collection framework with Human-and-model-in-the-Loop proposed in [94]\\nREFERENCES\\n[1] Charu C Aggarwal and ChengXiang Zhai. 2012. A survey of text classification algorithms. In Mining text data . Springer, 163–222.\\n[2]Naveed Akhtar and Ajmal Mian. 2018. Threat of adversarial attacks on deep learning in computer vision: A survey. Ieee Access 6 (2018), 14410–14430.\\n[3]Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani B. Srivastava, and Kai-Wei Chang. 2018. Generating Natural Language\\nAdversarial Examples. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 -\\nNovember 4, 2018 , Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (Eds.). Association for Computational Linguistics, 2890–2896.\\nhttps://doi.org/10.18653/v1/d18-1316\\n[4]Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial\\nexamples. In International conference on machine learning . PMLR, 274–283.\\n[5]Vincent Ballet, Xavier Renard, Jonathan Aigrain, Thibault Laugel, Pascal Frossard, and Marcin Detyniecki. 2019. Imperceptible adversarial attacks\\non tabular data. arXiv preprint arXiv:1911.03274 (2019).\\n[6]Rongzhou Bao, Jiayi Wang, and Hai Zhao. 2021. Defending Pre-trained Language Models from Adversarial Word Substitutions Without Performance\\nSacrifice. arXiv preprint arXiv:2105.14553 (2021).\\n[7]Loic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Markus\\nFreitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Tom Kocmi,\\nAndre Martins, Makoto Morishita, and Christof Monz (Eds.). 2021. Proceedings of the Sixth Conference on Machine Translation . Association for\\nComputational Linguistics, Online. https://aclanthology.org/2021.wmt-1.0\\n[8] Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic and Natural Noise Both Break Neural Machine Translation. arXiv:1711.02173 [cs.CL]\\n[9] Petr Bělohlávek. 2017. Using adversarial examples in natural language processing. (2017).\\n[10] Rasika Bhalerao, Mohammad Al-Rubaie, Anand Bhaskar, and Igor Markov. 2022. Data-Driven Mitigation of Adversarial Text Perturbation. arXiv\\npreprint arXiv:2202.09483 (2022).\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6395f0ad-6b18-464a-a7a9-2de75c525ec1', embedding=None, metadata={'page_label': '38', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='38 Shreya Goyal, et al.\\n[11] Siddhant Bhambri, Sumanyu Muku, Avinash Tulasi, and Arun Balaji Buduru. 2019. A survey of black-box adversarial attacks on computer vision\\nmodels. arXiv preprint arXiv:1912.01667 (2019).\\n[12] Gregory Bonaert, Dimitar I Dimitrov, Maximilian Baader, and Martin Vechev. 2021. Fast and precise certification of transformers. In Proceedings of\\nthe 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation . 466–481.\\n[13] Nicholas Boucher, Ilia Shumailov, Ross Anderson, and Nicolas Papernot. 2022. Bad characters: Imperceptible nlp attacks. In 2022 IEEE Symposium\\non Security and Privacy (SP) . IEEE, 1987–2004.\\n[14] T. Brants. 2000. TnT - A Statistical Part-of-Speech Tagger. ArXiv cs.CL/0003055 (2000).\\n[15] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc Le. 2017. Massive exploration of neural machine translation architectures. arXiv preprint\\narXiv:1703.03906 (2017).\\n[16] Vanessa Buhrmester, David Münch, and Michael Arens. 2021. Analysis of explainers of black box deep neural networks for computer vision: A\\nsurvey. Machine Learning and Knowledge Extraction 3, 4 (2021), 966–989.\\n[17] Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep Mukhopadhyay. 2018. Adversarial attacks and defences: A\\nsurvey. arXiv preprint arXiv:1810.00069 (2018).\\n[18] Luoxin Chen, Weitong Ruan, Xinyue Liu, and Jianhua Lu. 2020. SeqVAT: Virtual adversarial training for semi-supervised sequence labeling. In\\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics . 8801–8811.\\n[19] Yong Cheng, Lu Jiang, and Wolfgang Macherey. 2019. Robust Neural Machine Translation with Doubly Adversarial Inputs. In Proceedings of\\nthe 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Florence, Italy, 4324–4333.\\nhttps://doi.org/10.18653/v1/P19-1425\\n[20] Yong Cheng, Lu Jiang, Wolfgang Macherey, and Jacob Eisenstein. 2020. Advaug: Robust adversarial augmentation for neural machine translation.\\narXiv preprint arXiv:2006.11834 (2020).\\n[21] Siddhartha Chib and Edward Greenberg. 1995. Understanding the metropolis-hastings algorithm. The american statistician 49, 4 (1995), 327–335.\\n[22] Maximin Coavoux, Shashi Narayan, and Shay B Cohen. 2018. Privacy-preserving neural representations of text. arXiv preprint arXiv:1808.09408\\n(2018).\\n[23] Bidyut Das, Mukta Majumder, Santanu Phadikar, and Arif Ahmed Sekh. 2021. Automatic question generation and answer assessment: a survey.\\nResearch and Practice in Technology Enhanced Learning 16, 1 (2021), 1–15.\\n[24] Emily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston. 2019. Build it break it fix it for dialogue safety: Robustness from adversarial\\nhuman attack. arXiv preprint arXiv:1908.06083 (2019).\\n[25] Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong Liu. 2020. Towards robustness against natural language word substitutions. In International\\nConference on Learning Representations .\\n[26] Xin Dong, Yaxin Zhu, Yupeng Zhang, Zuohui Fu, Dongkuan Xu, Sen Yang, and Gerard De Melo. 2020. Leveraging adversarial training in self-learning\\nfor cross-lingual text classification. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information\\nRetrieval . 1541–1544.\\n[27] Tianyu Du, Shouling Ji, Lujia Shen, Yao Zhang, Jinfeng Li, Jie Shi, Chengfang Fang, Jianwei Yin, Raheem Beyah, and Ting Wang. 2021. Cert-RNN:\\nTowards Certifying the Robustness of Recurrent Neural Networks. (2021).\\n[28] Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018. On adversarial examples for character-level neural machine translation. arXiv preprint\\narXiv:1806.09030 (2018).\\n[29] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-Box Adversarial Examples for Text Classification. In Proceedings\\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) . Association for Computational Linguistics,\\nMelbourne, Australia, 31–36. https://doi.org/10.18653/v1/P18-2006\\n[30] David Eppstein. 1995. Zonohedra and zonotopes. (1995).\\n[31] Chun Fan, Xiaoya Li, Yuxian Meng, Xiaofei Sun, Xiang Ao, Fei Wu, Jiwei Li, and Tianwei Zhang. 2021. Defending against Backdoor Attacks in\\nNatural Language Generation. arXiv preprint arXiv:2106.01810 (2021).\\n[32] Chaz Firestone. 2020. Performance vs. competence in human–machine comparisons. Proceedings of the National Academy of Sciences 117, 43 (2020),\\n26562–26571.\\n[33] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. 2018. Black-Box Generation of Adversarial Text Sequences to Evade Deep Learning\\nClassifiers. In 2018 IEEE Security and Privacy Workshops, SP Workshops 2018, San Francisco, CA, USA, May 24, 2018 . IEEE Computer Society, 50–56.\\nhttps://doi.org/10.1109/SPW.2018.00016\\n[34] Siddhant Garg and Goutham Ramakrishnan. 2020. Bae: Bert-based adversarial examples for text classification. arXiv preprint arXiv:2004.01970\\n(2020).\\n[35] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572\\n(2014).\\n[36] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples. arXiv:1412.6572 [stat.ML]\\n[37] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, and\\nPushmeet Kohli. 2018. On the effectiveness of interval bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715\\n(2018).\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2ec8fa29-cebb-46d7-a6dc-0423ca1b5209', embedding=None, metadata={'page_label': '39', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 39\\n[38] Junliang Guo, Zhirui Zhang, Linlin Zhang, Linli Xu, Boxing Chen, Enhong Chen, and Weihua Luo. 2021. Towards Variable-Length Textual\\nAdversarial Attacks. arXiv preprint arXiv:2104.08139 (2021).\\n[39] Wenjuan Han, Liwen Zhang, Yong Jiang, and Kewei Tu. 2020. Adversarial attack and defense of structured prediction models. arXiv preprint\\narXiv:2010.01610 (2020).\\n[40] Xuanli He, Lingjuan Lyu, Qiongkai Xu, and Lichao Sun. 2021. Model Extraction and Adversarial Transferability, Your BERT is Vulnerable! arXiv\\npreprint arXiv:2103.10013 (2021).\\n[41] Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching\\nMachines to Read and Comprehend. In NIPS . 1693–1701.\\n[42] Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 2017. Deceiving Google’s Perspective API Built for Detecting Toxic\\nComments. arXiv:1702.08138 [cs.LG]\\n[43] Yu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei Wei, Wen-Lian Hsu, and Cho-Jui Hsieh. 2019. On the robustness of self-attentive models. In\\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics . 1520–1529.\\n[44] Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Krishnamurthy Dvijotham, and Pushmeet Kohli. 2019.\\nAchieving verified robustness to symbol substitutions via interval bound propagation. arXiv preprint arXiv:1909.01492 (2019).\\n[45] Aminul Huq, Mst Pervin, et al. 2020. Adversarial attacks and defense on texts: A survey. arXiv preprint arXiv:2005.14108 (2020).\\n[46] Adam Ivankay, Ivan Girardi, Chiara Marchiori, and Pascal Frossard. 2022. Fooling Explanations in Text Classifiers. arXiv preprint arXiv:2206.03178\\n(2022).\\n[47] Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adversarial Example Generation with Syntactically Controlled Paraphrase\\nNetworks. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers) , Marilyn A. Walker, Heng Ji, and Amanda Stent\\n(Eds.). Association for Computational Linguistics, 1875–1885. https://doi.org/10.18653/v1/n18-1170\\n[48] Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adversarial example generation with syntactically controlled paraphrase\\nnetworks. arXiv preprint arXiv:1804.06059 (2018).\\n[49] Robin Jia and Percy Liang. 2017. Adversarial Examples for Evaluating Reading Comprehension Systems. In Proceedings of the 2017 Conference\\non Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Copenhagen, Denmark, 2021–2031. https:\\n//doi.org/10.18653/v1/D17-1215\\n[50] Robin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. arXiv preprint arXiv:1707.07328 (2017).\\n[51] Robin Jia, Aditi Raghunathan, Kerem Göksel, and Percy Liang. 2019. Certified robustness to adversarial word substitutions. arXiv preprint\\narXiv:1909.00986 (2019).\\n[52] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2019. Is BERT Really Robust? Natural Language Attack on Text Classification and\\nEntailment. arXiv preprint arXiv:1907.11932 (2019).\\n[53] Erik Jones, Robin Jia, Aditi Raghunathan, and Percy Liang. 2020. Robust encodings: A framework for combating adversarial typos. arXiv preprint\\narXiv:2005.01229 (2020).\\n[54] Aravind K. Joshi. 1985. Natural language parsing: Tree adjoining grammars: How much context-sensitivity is required to provide reasonable\\nstructural descriptions?\\n[55] Dan Jurafsky and James H. Martin. 2000. Speech and Language Processing.\\n[56] Dongyeop Kang, Tushar Khot, Ashish Sabharwal, and Eduard Hovy. 2018. Adventure: Adversarial training for textual entailment with knowledge-\\nguided examples. arXiv preprint arXiv:1805.04680 (2018).\\n[57] Sanjay Kariyappa and Moinuddin K Qureshi. 2019. Improving adversarial robustness of ensembles with diversity training. arXiv preprint\\narXiv:1901.09981 (2019).\\n[58] Yannik Keller, Jan Mackensen, and Steffen Eger. 2021. BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired\\nOrthographic Adversarial Attacks. arXiv preprint arXiv:2106.01452 (2021).\\n[59] James Kennedy and Russell Eberhart. 1995. Particle swarm optimization. In Proceedings of ICNN’95-international conference on neural networks ,\\nVol. 4. IEEE, 1942–1948.\\n[60] Diksha Khurana, Aditya Koli, Kiran Khatter, and Sukhdev Singh. 2022. Natural language processing: State of the art, current trends and challenges.\\nMultimedia Tools and Applications (2022), 1–32.\\n[61] Ching-Yun Ko, Zhaoyang Lyu, Lily Weng, Luca Daniel, Ngai Wong, and Dahua Lin. 2019. POPQORN: Quantifying robustness of recurrent neural\\nnetworks. In International Conference on Machine Learning . PMLR, 3468–3477.\\n[62] Zixiao Kong, Jingfeng Xue, Yong Wang, Lu Huang, Zequn Niu, and Feng Li. 2021. A Survey on Adversarial Attack in the Age of Artificial\\nIntelligence. Wireless Communications and Mobile Computing 2021 (2021).\\n[63] Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana Mendu, Laura Barnes, and Donald Brown. 2019. Text classification\\nalgorithms: A survey. Information 10, 4 (2019), 150.\\n[64] Volodymyr Kuleshov, Shantanu Thakoor, Tingfung Lau, and S. Ermon. 2018. Adversarial Examples for Natural Language Classification Problems.\\n[65] Emanuele La Malfa and Marta Kwiatkowska. 2022. The king is naked: on the notion of robustness for natural language processing. In Proceedings\\nof the AAAI Conference on Artificial Intelligence , Vol. 36. 11047–11057.\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7ae512bf-f309-483f-a6bf-b459174f831c', embedding=None, metadata={'page_label': '40', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='40 Shreya Goyal, et al.\\n[66] Emanuele La Malfa, Min Wu, Luca Laurenti, Benjie Wang, Anthony Hartshorn, and Marta Kwiatkowska. 2020. Assessing robustness of text\\nclassification through maximal safe radius computation. arXiv preprint arXiv:2010.02004 (2020).\\n[67] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. 2019. Certified robustness to adversarial examples with\\ndifferential privacy. In 2019 IEEE Symposium on Security and Privacy (SP) . IEEE, 656–672.\\n[68] Hung-yi Lee, Shang-Wen Li, and Ngoc Thang Vu. 2022. Meta Learning for Natural Language Processing: A Survey. arXiv preprint arXiv:2205.01500\\n(2022).\\n[69] Piyawat Lertvittayakumjorn and Francesca Toni. 2021. Explanation-based human debugging of nlp models: A survey. Transactions of the Association\\nfor Computational Linguistics 9 (2021), 1508–1528.\\n[70] Alexander Levine and Soheil Feizi. 2020. Robustness certificates for sparse adversarial attacks by randomized ablation. In Proceedings of the AAAI\\nConference on Artificial Intelligence , Vol. 34. 4585–4593.\\n[71] Ang Li, Fangyuan Zhang, Shuangjiao Li, Tianhua Chen, Pan Su, and Hongtao Wang. 2023. Efficiently generating sentence-level textual adversarial\\nexamples with Seq2seq Stacked Auto-Encoder. Expert Systems with Applications 213 (2023), 119170.\\n[72] Jinfeng Li, Tianyu Du, Shouling Ji, Rong Zhang, Quan Lu, Min Yang, and Ting Wang. 2020. Textshield: Robust text classification based on\\nmultimodal embedding and neural machine translation. In 29th{USENIX}Security Symposium ( {USENIX}Security 20) . 1381–1398.\\n[73] Jinfeng Li, Tianyu Du, Xiangyu Liu, Rong Zhang, Hui Xue, and Shouling Ji. 2021. Enhancing Model Robustness by Incorporating Adversarial\\nKnowledge into Semantic Representation. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .\\nIEEE, 7708–7712.\\n[74] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2018. Textbugger: Generating adversarial text against real-world applications. arXiv\\npreprint arXiv:1812.05271 (2018).\\n[75] Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li. 2020. A survey on deep learning for named entity recognition. IEEE Transactions on Knowledge\\nand Data Engineering (2020).\\n[76] Linjie Li, Jie Lei, Zhe Gan, and Jingjing Liu. 2021. Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models. arXiv\\npreprint arXiv:2106.00245 (2021).\\n[77] Linyang Li and Xipeng Qiu. 2020. TAVAT: Token-Aware Virtual Adversarial Training for Language Understanding. arXiv preprint arXiv:2004.14543\\n(2020).\\n[78] Lianjie Li, Zi Zhu, Dongyu Du, Shuxia Ren, Yao Zheng, and Guangsheng Chang. 2020. Adversarial Convolutional Neural Network for Text\\nClassification. In Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering . 692–696.\\n[79] Qian Li, Hao Peng, Jianxin Li, Congying Xia, Renyu Yang, Lichao Sun, Philip S Yu, and Lifang He. 2020. A survey on text classification: From\\nshallow to deep learning. arXiv preprint arXiv:2008.00364 (2020).\\n[80] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset. arXiv\\npreprint arXiv:1710.03957 (2017).\\n[81] Zongyi Li, Jianhan Xu, Jiehang Zeng, Linyang Li, Xiaoqing Zheng, Qi Zhang, Kai-Wei Chang, and Cho-Jui Hsieh. 2021. Searching for an Effective\\nDefender: Benchmarking Defense against Adversarial Word Substitution. arXiv preprint arXiv:2108.12777 (2021).\\n[82] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. 2018. Deep Text Classification Can be Fooled. In Proceedings\\nof the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden , Jérôme Lang (Ed.).\\nijcai.org, 4208–4215. https://doi.org/10.24963/ijcai.2018/585\\n[83] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. 2018. Deep Text Classification Can Be Fooled. In Proceedings of\\nthe 27th International Joint Conference on Artificial Intelligence (Stockholm, Sweden) (IJCAI’18) . AAAI Press, 4208–4215.\\n[84] Hui Liu, Yongzheng Zhang, Yipeng Wang, Zheng Lin, and Yige Chen. 2020. Joint character-level word embedding and adversarial stability training\\nto defend adversarial text. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 8384–8391.\\n[85] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017. Adversarial multi-task learning for text classification. arXiv preprint arXiv:1704.05742 (2017).\\n[86] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. 2020. Adversarial training for large neural\\nlanguage models. arXiv preprint arXiv:2004.08994 (2020).\\n[87] Xingjun Ma, Yuhao Niu, Lin Gu, Yisen Wang, Yitian Zhao, James Bailey, and Feng Lu. 2021. Understanding adversarial attacks on deep learning\\nbased medical image analysis systems. Pattern Recognition 110 (2021), 107332.\\n[88] Pasquale Minervini and Sebastian Riedel. 2018. Adversarially regularising neural NLI models to integrate logical background knowledge. arXiv\\npreprint arXiv:1808.08609 (2018).\\n[89] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. 2016. Adversarial training methods for semi-supervised text classification. arXiv preprint\\narXiv:1605.07725 (2016).\\n[90] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. 2018. Virtual adversarial training: a regularization method for supervised and\\nsemi-supervised learning. IEEE transactions on pattern analysis and machine intelligence 41, 8 (2018), 1979–1993.\\n[91] John X Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020. Textattack: A framework for adversarial attacks, data\\naugmentation, and adversarial training in nlp. arXiv preprint arXiv:2005.05909 (2020).\\n[92] Nikola Mrkšić, Diarmuid O Séaghdha, Blaise Thomson, Milica Gašić, Lina Rojas-Barahona, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, and\\nSteve Young. 2016. Counter-fitting word vectors to linguistic constraints. arXiv preprint arXiv:1603.00892 (2016).\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='59bb7390-f055-4a3a-b062-4a5b5dc7c5ad', embedding=None, metadata={'page_label': '41', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 41\\n[93] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural\\nNetworks for Extreme Summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . Association for\\nComputational Linguistics, Brussels, Belgium, 1797–1807. https://doi.org/10.18653/v1/D18-1206\\n[94] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2019. Adversarial NLI: A new benchmark for natural\\nlanguage understanding. arXiv preprint arXiv:1910.14599 (2019).\\n[95] Chidozie Nwafor et al. 2021. An Automated Mulitple-choice Question Generation Using Natural Language Processing Techniques. International\\nJournal on Natural Language Computing (IJNLC) Vol 10 (2021).\\n[96] Daniel W Otter, Julian R Medina, and Jugal K Kalita. 2020. A survey of the usages of deep learning for natural language processing. IEEE transactions\\non neural networks and learning systems 32, 2 (2020), 604–624.\\n[97] Mesut Ozdag. 2018. Adversarial attacks and defenses against deep neural networks: a survey. Procedia Computer Science 140 (2018), 152–161.\\n[98] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings\\nof the 40th annual meeting of the Association for Computational Linguistics . 311–318.\\n[99] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global Vectors for Word Representation. In Proceedings of the 2014\\nConference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special\\nInterest Group of the ACL , Alessandro Moschitti, Bo Pang, and Walter Daelemans (Eds.). ACL, 1532–1543. https://doi.org/10.3115/v1/d14-1162\\n[100] Lis Pereira, Xiaodong Liu, Hao Cheng, Hoifung Poon, Jianfeng Gao, and Ichiro Kobayashi. 2021. Targeted adversarial training for natural language\\nunderstanding. arXiv preprint arXiv:2104.05847 (2021).\\n[101] Yada Pruksachatkun, Satyapriya Krishna, Jwala Dhamala, Rahul Gupta, and Kai-Wei Chang. 2021. Does Robustness Improve Fairness? Approaching\\nFairness with Word Substitution Robustness Methods for Text Classification. arXiv preprint arXiv:2106.10826 (2021).\\n[102] Danish Pruthi, Bhuwan Dhingra, and Zachary C Lipton. 2019. Combating adversarial misspellings with robust word recognition. arXiv preprint\\narXiv:1905.11268 (2019).\\n[103] Shilin Qiu, Qihe Liu, Shijie Zhou, and Wen Huang. 2022. Adversarial attack and defense technologies in natural language processing: A survey.\\nNeurocomputing 492 (2022), 278–307.\\n[104] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. 2018. Certified defenses against adversarial examples. arXiv preprint arXiv:1801.09344\\n(2018).\\n[105] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv\\npreprint arXiv:1606.05250 (2016).\\n[106] Gowtham Ramesh, Sumanth Doddapaneni, Aravinth Bheemaraj, Mayank Jobanputra, Raghavan AK, Ajitesh Sharma, Sujit Sahoo, Harshita\\nDiddee, Mahalakshmi J, Divyanshu Kakwani, Navneet Kumar, Aswin Pradeep, Kumar Deepak, Vivek Raghavan, Anoop Kunchukuttan, Pratyush\\nKumar, and Mitesh Shantadevi Khapra. 2021. Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages.\\narXiv:2104.05596 [cs.CL]\\n[107] Vikas Raunak, Arul Menezes, and Marcin Junczys-Dowmunt. 2021. The Curious Case of Hallucinations in Neural Machine Translation. In\\nNAACL-HLT . Association for Computational Linguistics, 1172–1183.\\n[108] Kui Ren, Tianhang Zheng, Zhan Qin, and Xue Liu. 2020. Adversarial attacks and defenses in deep learning. Engineering 6, 3 (2020), 346–360.\\n[109] Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. 2019. Generating natural language adversarial examples through probability weighted word\\nsaliency. In Proceedings of the 57th annual meeting of the association for computational linguistics . 1085–1097.\\n[110] Yankun Ren, Jianbin Lin, Siliang Tang, Jun Zhou, Shuang Yang, Yuan Qi, and Xiang Ren. 2020. Generating natural language adversarial examples\\non a large scale with generative models. arXiv preprint arXiv:2003.10388 (2020).\\n[111] Ishai Rosenberg, Asaf Shabtai, Yuval Elovici, and Lior Rokach. 2021. Sequence Squeezing: A Defense Method Against Adversarial Examples for API\\nCall-Based RNN Variants. In 2021 International Joint Conference on Neural Networks (IJCNN) . IEEE, 1–10.\\n[112] Allen Roush and Arvind Balaji. 2020. DebateSum: A large-scale argument mining and summarization dataset. In Proceedings of the 7th Workshop on\\nArgument Mining . Association for Computational Linguistics, Online, 1–7. https://aclanthology.org/2020.argmining-1.1\\n[113] Cynthia Rudin and Joanna Radin. 2019. Why are we using black box models in AI when we don’t need to? A lesson from an explainable AI\\ncompetition. (2019).\\n[114] Wonryong Ryou, Jiayu Chen, Mislav Balunovic, Gagandeep Singh, Andrei Dan, and Martin Vechev. 2021. Scalable Polyhedral Verification of\\nRecurrent Neural Networks. In International Conference on Computer Aided Verification . Springer, 225–248.\\n[115] Ananya B Sai, Akash Kumar Mohankumar, Siddhartha Arora, and Mitesh M Khapra. 2020. Improving dialog evaluation with a multi-reference\\nadversarial dataset and large scale pretraining. Transactions of the Association for Computational Linguistics 8 (2020), 810–827.\\n[116] Keisuke Sakaguchi, Kevin Duh, Matt Post, and Benjamin Van Durme. 2017. Robsut wrod reocginiton via semi-character recurrent neural network.\\nInThirty-first AAAI conference on artificial intelligence .\\n[117] Elizabeth Salesky, Marcello Federico, and Marta Costa-jussà (Eds.). 2022. Proceedings of the 19th International Conference on Spoken Language\\nTranslation (IWSLT 2022) . Association for Computational Linguistics, Dublin, Ireland (in-person and online). https://aclanthology.org/2022.iwslt-1.0\\n[118] Suranjana Samanta and Sameep Mehta. 2017. Towards Crafting Text Adversarial Samples. arXiv:1707.02812 [cs.LG]\\n[119] Motoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji Matsumoto. 2018. Interpretable adversarial perturbation in input embedding space for text.\\narXiv preprint arXiv:1805.02917 (2018).\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ffbed222-8a30-4285-afef-dd644710c514', embedding=None, metadata={'page_label': '42', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='42 Shreya Goyal, et al.\\n[120] Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein.\\n2019. Adversarial training for free! Advances in Neural Information Processing Systems 32 (2019).\\n[121] Zhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, and Cho-Jui Hsieh. 2020. Robustness verification for transformers. arXiv preprint\\narXiv:2002.06622 (2020).\\n[122] Joe Stacey, Pasquale Minervini, Haim Dubossarsky, Sebastian Riedel, and Tim Rocktäschel. 2020. Avoiding the hypothesis-only bias in natural\\nlanguage inference via ensemble adversarial training. arXiv preprint arXiv:2004.07790 (2020).\\n[123] Ieva Stali ¯unait ˙e, Philip John Gorinski, and Ignacio Iacobacci. 2021. Improving Commonsense Causal Reasoning by Adversarial Training and Data\\nAugmentation. arXiv preprint arXiv:2101.04966 (2021).\\n[124] Jacob Steinhardt, Pang Wei Koh, and Percy Liang. 2017. Certified defenses for data poisoning attacks. In Proceedings of the 31st International\\nConference on Neural Information Processing Systems . 3520–3532.\\n[125] Bin Sun and Kan Li. 2021. Neural Dialogue Generation Methods in Open Domain: A Survey. Natural Language Processing Research 1, 3-4 (2021),\\n56–70.\\n[126] Peng Sun, Xuezhen Yang, Xiaobing Zhao, and Zhijuan Wang. 2018. An overview of named entity recognition. In 2018 International Conference on\\nAsian Language Processing (IALP) . IEEE, 273–278.\\n[127] Abigail Swenor and Jugal Kalita. 2022. Using Random Perturbations to Mitigate Adversarial Attacks on Sentiment Analysis Models. arXiv preprint\\narXiv:2202.05758 (2022).\\n[128] Amirsina Torfi, Rouzbeh A Shirvani, Yaser Keneshloo, Nader Tavaf, and Edward A Fox. 2020. Natural language processing advancements by deep\\nlearning: A survey. arXiv preprint arXiv:2003.01200 (2020).\\n[129] Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber. 2019. Trick me if you can: Human-in-the-loop generation of\\nadversarial examples for question answering. Transactions of the Association for Computational Linguistics 7 (2019), 387–401.\\n[130] Eric Wallace, Mitchell Stern, and Dawn Song. 2020. Imitation attacks and defenses for black-box machine translation systems. arXiv preprint\\narXiv:2004.15015 (2020).\\n[131] Matthew Wallace, Rishabh Khandelwal, and Brian Tang. 2022. Does IBP Scale? (2022).\\n[132] Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. 2020. Infobert: Improving robustness of language models\\nfrom an information theoretic perspective. arXiv preprint arXiv:2010.02329 (2020).\\n[133] Dilin Wang, Chengyue Gong, and Qiang Liu. 2019. Improving neural language modeling via adversarial training. In International Conference on\\nMachine Learning . PMLR, 6555–6565.\\n[134] Tianlu Wang, Xuezhi Wang, Yao Qin, Ben Packer, Kang Li, Jilin Chen, Alex Beutel, and Ed Chi. 2020. CAT-Gen: Improving Robustness in NLP\\nModels via Controlled Adversarial Text Generation. arXiv preprint arXiv:2010.02338 (2020).\\n[135] Wenjie Wang, Pengfei Tang, Jian Lou, and Li Xiong. 2021. Certified Robustness to Word Substitution Attack with Differential Privacy. In Proceedings\\nof the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . 1102–1112.\\n[136] Wenqi Wang, Run Wang, Jianpeng Ke, and Lina Wang. 2021. TextFirewall: Omni-Defending Against Adversarial Texts in Sentiment Classification.\\nIEEE Access 9 (2021), 27467–27475.\\n[137] Wenqi Wang, Run Wang, Lina Wang, Zhibo Wang, and Aoshuang Ye. 2019. Towards a robust deep neural network in texts: A survey. arXiv preprint\\narXiv:1902.07285 (2019).\\n[138] Wenqi Wang, Run Wang, Lina Wang, Zhibo Wang, and Aoshuang Ye. 2021. Towards a robust deep neural network against adversarial texts: A\\nsurvey. IEEE Transactions on Knowledge and Data Engineering (2021).\\n[139] Xiaosen Wang, Hao Jin, Yichen Yang, and Kun He. 2021. Natural Language Adversarial Defense through Synonym Encoding. (2021).\\n[140] Xiaosen Wang, Yifeng Xiong, and Kun He. 2021. Randomized Substitution and Vote for Textual Adversarial Example Detection. arXiv preprint\\narXiv:2109.05698 (2021).\\n[141] Yicheng Wang and Mohit Bansal. 2018. Robust Machine Comprehension Models via Adversarial Training. In Proceedings of the 2018 Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) . Association for\\nComputational Linguistics, New Orleans, Louisiana, 575–581. https://doi.org/10.18653/v1/N18-2091\\n[142] Yicheng Wang and Mohit Bansal. 2018. Robust machine comprehension models via adversarial training. arXiv preprint arXiv:1804.06473 (2018).\\n[143] Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. 2018. Evaluating the robustness of\\nneural networks: An extreme value theory approach. arXiv preprint arXiv:1801.10578 (2018).\\n[144] Adina Williams, Tristan Thrush, and Douwe Kiela. 2020. Anlizing the adversarial natural language inference dataset. arXiv preprint arXiv:2010.12729\\n(2020).\\n[145] Jing Wu, Mingyi Zhou, Ce Zhu, Yipeng Liu, Mehrtash Harandi, and Li Li. 2021. Performance Evaluation of Adversarial Attacks: Discrepancies and\\nSolutions. arXiv preprint arXiv:2104.11103 (2021).\\n[146] Yi Wu, David Bamman, and Stuart Russell. 2017. Adversarial training for relation extraction. In Proceedings of the 2017 Conference on Empirical\\nMethods in Natural Language Processing . 1778–1783.\\n[147] Zhouhang Xie, Jonathan Brophy, Adam Noack, Wencong You, Kalyani Asthana, Carter Perkins, Sabrina Reis, Sameer Singh, and Daniel Lowd. 2022.\\nIdentifying Adversarial Attacks on Text Classifiers. arXiv preprint arXiv:2201.08555 (2022).\\n[148] Han Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil K Jain. 2020. Adversarial attacks and defenses in images, graphs\\nand text: A review. International Journal of Automation and Computing 17, 2 (2020), 151–178.\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a250641d-5d88-43d6-8e22-7f7fd88ad549', embedding=None, metadata={'page_label': '43', 'file_name': 'A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_path': '/content/data/A Survey of Adversarial Defences and Robustness in NLP.pdf', 'file_type': 'application/pdf', 'file_size': 1953795, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Survey of Adversarial Defences and Robustness in NLP 43\\n[149] Jingjing Xu, Liang Zhao, Hanqi Yan, Qi Zeng, Yun Liang, and Xu Sun. 2019. LexicalAT: Lexical-based adversarial reinforcement training for robust\\nsentiment classification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\\nConference on Natural Language Processing (EMNLP-IJCNLP) . 5518–5527.\\n[150] Ying Xu, Xu Zhong, Antonio Jimeno Yepes, and Jey Han Lau. 2021. Grey-box Adversarial Attack And Defence For Sentiment Classification. arXiv\\npreprint arXiv:2103.11576 (2021).\\n[151] Ying Xu, Xu Zhong, Antonio Jose Jimeno Yepes, and Jey Han Lau. 2020. Elephant in the room: An evaluation framework for assessing adversarial\\nexamples in nlp. arXiv preprint arXiv:2001.07820 (2020).\\n[152] Vikas Yadav and Steven Bethard. 2019. A survey on recent advances in named entity recognition from deep learning models. arXiv preprint\\narXiv:1910.11470 (2019).\\n[153] Shuoheng Yang, Yuxin Wang, and Xiaowen Chu. 2020. A survey of deep learning techniques for neural machine translation. arXiv preprint\\narXiv:2002.07526 (2020).\\n[154] Yichen Yang, Xiaosen Wang, and Kun He. 2022. Robust Textual Embedding against Word-level Adversarial Attacks. arXiv preprint arXiv:2202.13817\\n(2022).\\n[155] Ziqing Yang, Yiming Cui, Chenglei Si, Wanxiang Che, Ting Liu, Shijin Wang, and Guoping Hu. 2021. Adversarial Training for Machine Reading\\nComprehension with Virtual Embeddings. arXiv preprint arXiv:2106.04437 (2021).\\n[156] Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2017. Robust multilingual part-of-speech tagging via adversarial training. arXiv preprint\\narXiv:1711.04903 (2017).\\n[157] Mao Ye, Chengyue Gong, and Qiang Liu. 2020. SAFER: A structure-free approach for certified robustness to adversarial word substitutions. arXiv\\npreprint arXiv:2005.14424 (2020).\\n[158] KiYoon Yoo, Jangho Kim, Jiho Jang, and Nojun Kwak. 2022. Detection of Word Adversarial Examples in Text Classification: Benchmark and\\nBaseline via Robust Density Estimation. arXiv preprint arXiv:2203.01677 (2022).\\n[159] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. 2019. Adversarial examples: Attacks and defenses for deep learning. IEEE transactions on neural\\nnetworks and learning systems 30, 9 (2019), 2805–2824.\\n[160] Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and Maosong Sun. 2019. Word-level textual adversarial attacking as\\ncombinatorial optimization. arXiv preprint arXiv:1910.12196 (2019).\\n[161] Guoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji Zhang, Zixian Ma, Bairu Hou, Yuan Zang, Zhiyuan Liu, and Maosong Sun. 2020. Openattack: An\\nopen-source textual adversarial attack toolkit. arXiv preprint arXiv:2009.09191 (2020).\\n[162] Jiehang Zeng, Xiaoqing Zheng, Jianhan Xu, Linyang Li, Liping Yuan, and Xuanjing Huang. 2021. Certified Robustness to Text Adversarial Attacks\\nby Randomized [MASK]. arXiv preprint arXiv:2105.03743 (2021).\\n[163] Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh, and Liwei Wang. 2020. Macer: Attack-free and\\nscalable robust training via maximizing certified radius. arXiv preprint arXiv:2001.02378 (2020).\\n[164] Chaoning Zhang, Philipp Benz, Chenguo Lin, Adil Karjauv, Jing Wu, and In So Kweon. 2021. A survey on universal adversarial attack. arXiv\\npreprint arXiv:2103.01498 (2021).\\n[165] Huangzhao Zhang, Hao Zhou, Ning Miao, and Lei Li. 2020. Generating fluent adversarial examples for natural languages. arXiv preprint\\narXiv:2007.06174 (2020).\\n[166] Wei Zhang, Qian Chen, and Yunfang Chen. 2020. Deep Learning Based Robust Text Classification Method via Virtual Adversarial Training. IEEE\\nAccess 8 (2020), 61174–61182.\\n[167] Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial attacks on deep-learning models in natural language\\nprocessing: A survey. ACM Transactions on Intelligent Systems and Technology (TIST) 11, 3 (2020), 1–41.\\n[168] Yuhao Zhang, Aws Albarghouthi, and Loris D’Antoni. 2021. Certified Robustness to Programmable Transformations in LSTMs. arXiv preprint\\narXiv:2102.07818 (2021).\\n[169] Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase adversaries from word scrambling. arXiv preprint arXiv:1904.01130 (2019).\\n[170] Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018. Generating Natural Adversarial Examples. arXiv:1710.11342 [cs.LG]\\n[171] Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. 2016. Improving the robustness of deep neural networks via stability training. In\\nProceedings of the ieee conference on computer vision and pattern recognition . 4480–4488.\\n[172] Yichao Zhou, Jyun-Yu Jiang, Kai-Wei Chang, and Wei Wang. 2019. Learning to discriminate perturbations for blocking adversarial attacks in text\\nclassification. arXiv preprint arXiv:1909.03084 (2019).\\n[173] Yi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-wei Chang, and Xuanjing Huang. 2020. Defense against adversarial attacks in nlp via dirichlet\\nneighborhood ensemble. arXiv preprint arXiv:2006.11627 (2020).\\n[174] Bin Zhu, Zhaoquan Gu, Le Wang, and Zhihong Tian. 2021. TREATED: Towards Universal Defense against Textual Adversarial Attacks. arXiv\\npreprint arXiv:2109.06176 (2021).\\n[175] Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. 2019. Freelb: Enhanced adversarial training for natural language\\nunderstanding. arXiv preprint arXiv:1909.11764 (2019).\\n[176] Simiao Zuo, Chen Liang, Haoming Jiang, Xiaodong Liu, Pengcheng He, Jianfeng Gao, Weizhu Chen, and Tuo Zhao. 2021. Adversarial Training as\\nStackelberg Game: An Unrolled Optimization Approach. arXiv preprint arXiv:2104.04886 (2021).\\nManuscript submitted to ACM', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7ae26c86-ed46-4b4e-a10e-6e7798825327', embedding=None, metadata={'page_label': '1', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 1/50ML Security with the Adversarial Robustness\\nToolbox\\nPart 1 — Attacking Machine Learning Models\\nKedion·Follow\\n17 min read·Apr 26, 2022\\nListen Share More\\nPhoto by Adi Goldstein\\nWritten by Tigran Avetisyan.Get unlimited access to the best of Medium for less than $1/week.Become a member\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4074af99-16c9-4b70-a5df-977eb3120954', embedding=None, metadata={'page_label': '2', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 2/50Developing machine learning models and putting them into production can be very\\nchallenging. However, successfully deploying an ML pipeline is just part of the story\\n— you also need to think about keeping it secure.\\nMachine learning models are used in a wide range of areas, like finance, medicine,\\nor surveillance, and can be accurate at detecting fraud or filtering out faulty\\nproducts. In applications like detecting fraud, scammers might be interested in\\nfooling machine learning systems so that they miss scam emails or phishing links.\\nIn this series of tutorials, we are going to have a look at the Adversarial Robustness\\nToolbox and figure out how it can help you with securing your machine learning\\npipelines. In PART 1, we will focus on adversarial attacks, and we will be using the\\nMNIST digits dataset along with the TensorFlow/Keras ML framework.\\nLet’s get started!\\nWhat is the Adversarial Robustness Toolbox?\\nThe Adversarial Robustness Toolbox, or ART, is a Python framework for machine\\nlearning security. ART contains attack and defense tools that can help teams better\\nunderstand adversarial attacks and develop protection measures based on\\nexperimentation.\\nhttps://adversarial-robustness-toolbox.org/\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='60866dd3-f407-4115-a18a-a1c63c5c49f0', embedding=None, metadata={'page_label': '3', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 3/50ART has 39 attack modules, 29 defense modules, and supports a wide range of\\nmachine learning frameworks, including scikit-learn, PyTorch, TensorFlow, and\\nKeras. ART also supports several machine learning tasks (including classification,\\nregression, and generation) and works with all data types (audio, video, images, or\\ntables).\\nThe Adversarial Robustness Toolbox was originally developed and published by\\nIBM. In July 2020, IBM donated ART to the Linux Foundation AI (LFAI). Since then,\\nLFAI has maintained and developed updates for the toolkit.\\nAttack Types in the Adversarial Robustness Toolbox\\nBecause PART 1 in this series focuses on attacks, let’s take a deeper look at the attack\\ntypes supported by ART.\\nAt a high level, there are 4 types of adversarial attacks implemented in ART:\\n· Evasion. Evasion attacks typically work by perturbing input data to cause a trained\\nmodel to misclassify it. Evasion is done after training and during inference, i.e.\\nwhen models are already deployed in production. Adversaries perform evasion\\nattacks to avoid detection by AI systems. As an example, adversaries might run an\\nevasion attack to cause the victim model to miss phishing emails. Evasion attacks\\nmight require access to the victim model.\\n· Extraction. Extraction is an attack where an adversary attempts to build a model\\nthat is similar or identical to a victim model. In simple words, extraction is the\\nattempt of copying or stealing a machine learning model. Extraction attacks\\ntypically require access to the original model, as well as to data that is similar or\\nidentical to the data originally used to train the victim model.\\n· Inference. Inference attacks generally aim at reconstructing a part or the entirety\\nof the dataset that was used to train the victim model. Adversaries can use inference\\nattacks to reconstruct entire training samples, separate features, or determine if a\\nsample has been used to train the victim model. Inference attacks typically require\\naccess to the victim model. In some cases, attackers might also need to have access\\nto some portion of the data used to train the model.\\n· Poisoning. Poisoning attacks aim to perturb training data to corrupt the victim\\nmodel during training. Poisoned data contains features (called a backdoor) that\\ntrigger the desired output in a trained model. Essentially, the perturbed features', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='965edabc-e24c-440b-ad30-9350c89f3edb', embedding=None, metadata={'page_label': '4', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 4/50cause the model to overfit to them. As a very simple example (which we’ll have a\\nlook at in code below), an attacker could poison the digits in the MNIST dataset so\\nthat the victim model classifies all digits as 9s. Poisoning attacks require access to\\nthe training data of a model before the actual training occurs.\\nART’s documentation supplies this neat graph that shows how the attacks work at a\\nhigh level:\\nhttps://adversarial-robustness-toolbox.readthedocs.io/en/latest/index.html\\nFor the vast majority of implemented attacks, ART supplies links to the research\\npapers that provide more detail on a given attack. So if you want to learn more about\\na specific attack, look for paper links in ART’s documentation.\\nBelow, we will take a look at each of the attack types supported by ART. We will\\nimplement one attack per type, but what you’ll see below should transfer to other\\nattacks of the same type as well.\\nHow do attacks on machine learning pipelines happen?', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='0520a83c-0abc-45dd-8c3a-e6af1c88b841', embedding=None, metadata={'page_label': '5', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 5/50As we pointed out above, adversaries typically need some form of access to your\\nmachine learning model or its training data to perform an attack. But assuming that\\nyour model is hosted in an environment that an adversary can’t reach, how do\\nattacks on ML pipelines even happen? In more practical terms, how can an\\nadversary gain access to your model and training data?\\nHere are just some of the cases in which adversaries can obtain access to your\\npipeline and data:\\n· You are using a dataset from  an unverified source. An adversary could poison data\\nand publish it somewhere for unsuspecting teams to use. Datasets from\\nuntrustworthy sources or datasets that are not verified are at higher risk of being\\ncompromised.\\n· You are using a m odel from  an unverified source. Similar to training data,\\nadversaries can publish models for victims to use. They can later use their\\nknowledge of the model to perform attacks on it. Models from suspicious sources\\nmight also be pre-trained on poisoned data.\\n· You are using a m odel that is available publicly. Suppose that you are using an NLP\\nmodel from Hugging Face. Hugging Face models are available to anyone. An\\nadversary could analyze a publicly available model and use their knowledge to\\nattack other similar models used by other teams. In fact, adversaries don’t need to\\nhave the exact same model as you do — knowledge of a model that performs the\\nsame task as yours can be enough for an attack.\\n· An adversary has access to your M L pipeline. If a malicious insider gains access to\\nyour pipeline — by leveraging their position inside the organization, for example —\\nthey can get in-depth knowledge about its architecture, weights, and training data.\\nThey might also know about the measures that you use to protect your model, which\\nmakes malicious insiders arguably the biggest threat to ML pipeline security.\\nThis isn’t an exhaustive list, but it should give you an idea of how your model or\\ntraining data could become compromised.\\nPrerequisites for Using ART\\nTo be able to follow along with this tutorial, install ART by using this command.\\npip install adversarial-robustness-toolbox', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b5cad066-4b01-4f6d-9b0e-e1d3fb0c5e62', embedding=None, metadata={'page_label': '6', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 6/50If you are using conda, use this command instead:\\nconda install -c conda-forge adversarial-robustness-toolbox\\nWe used ART version 1.10.0, but newer versions should work fine as well.\\nIf necessary, you can install ART just for the specific ML/DL framework that you will\\nbe using. You can learn more about this here.\\nAside from ART, you will need whichever ML/DL framework you want to attack. We\\nare going to be using TensorFlow, which you can install by using this command:\\npip install tensorflow\\nOr if you are using conda, this command:\\nconda install -c conda-forge tensorflow\\nWe are also going to be using NumPy and Matplotlib, so make sure that you have\\nthese libraries as well.\\nEvasion Attacks in ART\\nLet’s start with evasion attacks in ART. As mentioned earlier, an evasion attack is\\nwhen the attacker perturbs input at inference time to cause the model to misclassify\\nit.\\nAs of ART version 1.10.0, most of the attacks supported by the framework were\\nevasion attacks.\\nWe will be using the Fast Gradient Method to generate adversarial samples from our\\ntest set. You can read more about the Fast Gradient Method in this paper.\\nNote that you can find the full code for this tutorial here. You can find more\\nexamples from the authors of ART here and here.\\nImporting dependencies\\nAs usual, we start by importing dependencies:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fa5e1120-f7ec-42d7-985e-857a7f76de66', embedding=None, metadata={'page_label': '7', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 7/50Note that we are disabling eager execution in TensorFlow for this guide on line 12.\\nThis is because the wrapper class art.estimators.classification.KerasClassifier\\ndoesn’t fully support TF 2.\\nLoading data\\nTo load the MNIST dataset, we are going to use ART’s function\\nart.utils.load_dataset. This function returns tuples for the train and test sets, as\\nwell as the minimum and maximum feature values in the dataset.\\nThe images are already normalized to the [0, 1] range, while the labels are one-hot\\nencoded. We don’t need to do any preprocessing on the dataset.\\nTraining a TensorFlow Keras model\\nNow, let’s create a simple TensorFlow Keras model:view raw dependencies.py hosted with ❤  by GitHub1# Importing dependencies\\n2import tensorflow  as tf\\n3from tensorflow .keras.layers import Conv2D, MaxPool2D , Dense, Flatten\\n4import numpy as np\\n5import matplotlib .pyplot as plt\\n6import matplotlib\\n7from art.estimators .classification  import KerasClassifier\\n8from art.attacks.evasion import FastGradientMethod\\n9from art.utils import load_dataset\\n10\\n11# Disabling eager execution from TF 2\\n12tf.compat.v1.disable_eager_execution ()\\n13\\n14# Increasing Matplotlib font size\\n15matplotlib .rcParams .update({\"font.size\" : 14})\\n1# Loading the data\\n2(train_images , train_labels ), (test_images , test_labels ), min, max = load_dataset (name=\"mnist\")', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='643e9bb5-255f-4d8a-8e8c-65519694e935', embedding=None, metadata={'page_label': '8', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 8/50And train it:view raw create_model.py hosted with ❤  by GitHub1# Function for creating model\\n2def create_model ():\\n3    # Defining the model\\n4    model = tf.keras.models.Sequential ([\\n5        Conv2D(filters=32, kernel_size =3, activation =\"relu\", input_shape =(28, 28, 1)),\\n6        MaxPool2D (pool_size =2),\\n7        Conv2D(filters=64, kernel_size =3, activation =\"relu\"),\\n8        MaxPool2D (pool_size =2),\\n9        Flatten(),        \\n10        Dense(units=10, activation =\"softmax\" )\\n11    ])\\n12\\n13    # Compiling the model\\n14    model.compile(\\n15        optimizer =\"adam\",\\n16        loss=\"categorical_crossentropy\" ,\\n17        metrics=[\"accuracy\" ]\\n18        )\\n19\\n20    # Returning the model\\n21    return model', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c10e8cfc-478e-49dd-9ddf-a21841e229f1', embedding=None, metadata={'page_label': '9', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 9/50If training takes too much time for you, you can try to simplify the model. And if you\\nencounter out-of-memory (OOM) issues, reducing the batch size should be able to\\nhelp.\\nDefining an evasion attack on our model\\nAs the next step, let’s define an evasion attack for our model.\\nTo be able to run attacks on our model, we must wrap it in the\\nart.estimators.classification.KerasClassifier class. Here’s how this is done:view raw\\nieratrain_model.py hosted with ❤  by GitHub\\ntraining otpttthosted ith❤ bGitH b1# Instantiating the model\\n2model = create_model ()\\n3\\n4# Training the model\\n5model.fit(\\n6    x=train_images , \\n7    y=train_labels , \\n8    epochs=10,\\n9    batch_size =256)\\n1Train on 60000 samples\\n2Epoch 1/10\\n360000/60000 [==============================] - 4s 67us/sample - loss: 0.3924 - accuracy: 0.8961\\n4Epoch 2/10\\n560000/60000 [==============================] - 2s 33us/sample - loss: 0.0934 - accuracy: 0.9721\\n6Epoch 3/10\\n760000/60000 [==============================] - 2s 33us/sample - loss: 0.0665 - accuracy: 0.9804\\n8Epoch 4/10\\n960000/60000 [==============================] - 2s 32us/sample - loss: 0.0530 - accuracy: 0.9838\\n10Epoch 5/10\\n1160000/60000 [==============================] - 2s 32us/sample - loss: 0.0467 - accuracy: 0.9858\\n12Epoch 6/10\\n1360000/60000 [==============================] - 2s 32us/sample - loss: 0.0404 - accuracy: 0.9875\\n14Epoch 7/10\\n1560000/60000 [==============================] - 2s 32us/sample - loss: 0.0363 - accuracy: 0.9889\\n16Epoch 8/10\\n1760000/60000 [==============================] - 2s 31us/sample - loss: 0.0322 - accuracy: 0.9903\\n18Epoch 9/10\\n1960000/60000 [==============================] - 2s 31us/sample - loss: 0.0288 - accuracy: 0.9912\\n20Epoch 10/10\\n2160000/60000 [==============================] - 2s 32us/sample - loss: 0.0274 - accuracy: 0.9918', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e4cdb530-709d-4a93-af93-c497b7b21352', embedding=None, metadata={'page_label': '10', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 10/50The argument model=model indicates our model, while clip_values=(min, max)\\nspecifies the minimum and maximum values allowed for the features. We are using\\nthe values provided by the art.utils.load_dataset function.\\nInstead of KerasClassifier, you can also try TensorFlowV2Classifier, but note that its\\nusage is different.\\nWe then define the attack by using ART’s FastGradientMethod class:\\nestimator=classifier shows that the attack will apply to our classifier, while the\\nargument eps=0.3 essentially defines how strong the attack will be.\\nWe can then generate adversarial samples by calling the attack object’s method\\ngenerate, passing to it the target images that we want to perturb:\\nEvaluating the effectiveness of the attack\\nLet’s take a look at one adversarial sample:view raw wrap_model.py hosted with ❤  by GitHub1# Creating a classifier by wrapping our TF model in ART's KerasClassifier class\\n2classifier  = KerasClassifier (\\n3    model=model, \\n4    clip_values =(min, max)\\n5    )\\nview raw fast_gradient_attack .py hosted with ❤  by GitHub1# Defining an attack using the fast gradient method\\n2attack_fgsm  = FastGradientMethod (\\n3    estimator =classifier , \\n4    eps=0.3\\n5    )\\nview raw generat e_adv ersarial.py hosted with ❤  by GitHub1# Generating adversarial images from test images\\n2test_images_adv  = attack_fgsm .generate (x=test_images )\\nview raw show_adv ersarial.py hosted with ❤  by GitHub1# Viewing an adversarial image\\n2plt.imshow(X=test_images_adv [0])\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='093092f4-8d62-4d4d-ad20-082342f46c0f', embedding=None, metadata={'page_label': '11', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 11/50\\nThe fast gradient method applied noise to the clean test images. We can see the\\neffect of the attack by comparing the performance of our model on the clean and\\nadversarial sets:\\nClean test set loss: 0.04 vs adversarial set test loss: 5.95Clean \\ntest set accuracy: 0.99 vs adversarial test set accuracy: 0.07\\nThe attack has affected the model considerably, making it completely unusable.\\nHowever, because the perturbations in the image are very evident, it would be veryview raw evaluat e_evasion.py hosted with ❤  by GitHub1# Evaluating the model on clean images\\n2score_clean  = model.evaluate (\\n3    x=test_images , \\n4    y=test_labels\\n5    )\\n6\\n7# Evaluating the model on adversarial images\\n8score_adv  = model.evaluate (\\n9    x=test_images_adv , \\n10    y=test_labels\\n11    )\\n12\\n13# Comparing test losses\\n14print(f\"Clean test set loss: {score_clean [0]:.2f} \" \\n15      f\"vs adversarial set test loss: {score_adv [0]:.2f}\")\\n16\\n17# Comparing test accuracies\\n18print(f\"Clean test set accuracy: {score_clean [1]:.2f} \" \\n19      f\"vs adversarial test set accuracy: {score_adv [1]:.2f}\")', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='279a5904-965f-41b2-a076-dd44e31075fe', embedding=None, metadata={'page_label': '12', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 12/50easy for the victim to figure out that they are being attacked.\\nLowering the eps value can make the tampering less visible, but the impact on the\\nmodel will be lessened as well. We’ve tried 10 different values for eps to explore their\\neffect on the visual appearance of adversarial samples and on the model’s\\nperformance.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4fd6aec0-51da-4869-bdd1-c9af806df10a', embedding=None, metadata={'page_label': '13', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 13/501# Setting the number of rows and columns for the figure\\n2nrows, ncols = 2, 5\\n3\\n4# Generating subplots\\n5fig, axes = plt.subplots (\\n6    nrows=nrows, \\n7    ncols=ncols, \\n8    figsize=(20, 10)\\n9    )\\n10\\n11# Defining a range of eps values to try\\n12eps_to_try  = [0.01, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.25]\\n13\\n14# Defining a counting variable to traverse eps_to_try\\n15counter = 0\\n16\\n17# Iterating over rows and cols\\n18for i in range(nrows):\\n19    for j in range(ncols):    \\n20        # Creating an attack object for the current value of eps    \\n21        attack_fgsm  = FastGradientMethod (\\n22            estimator =classifier , \\n23            eps=eps_to_try [counter]\\n24            )\\n25\\n26        # Generating adversarial images\\n27        test_images_adv  = attack_fgsm .generate (x=test_images )\\n28\\n29        # Showing the first adversarial image\\n30        axes[i, j].imshow(X=test_images_adv [0])\\n31\\n32        # Disabling x and y ticks\\n33        axes[i, j].set_xticks (ticks=[])\\n34        axes[i, j].set_yticks (ticks=[])\\n35\\n36        # Evaluating model performance on adversarial samples and retrieving test accuracy\\n37        test_score  = classifier ._model.evaluate (\\n38            x=test_images_adv , \\n39            y=test_labels\\n40            )[ 1]\\n41\\n42        # Getting prediction for the image that we displayed\\n43        prediction  = np.argmax(model.predict(\\n44            x=np.expand_dims (a=test_images_adv [0], \\n45            axis=0)\\n46            ))    \\n47\\n48 #Showing thecurrent epsvaluetestaccuracy andprediction', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2b84f988-31f5-4099-a129-9f3a2bc0849b', embedding=None, metadata={'page_label': '14', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 14/50In the code block above, we create a figure along with subplots (lines 5 to 9) and\\ndefine eps values to try (line 12). We then iterate over our subplots (lines 18 and 19),\\ngenerate adversarial samples for each value from eps_to_try (lines 21 to 27), plot a\\nsample adversarial image for the current eps value (line 30), evaluate model\\naccuracy on the adversarial set (lines 37 to 40), and get a prediction for the sample\\nadversarial image that is being displayed (lines 43 to 46).\\nEach eps value, the test accuracy for it, and the prediction for the sample\\nadversarial image are displayed above each image (lines 49 to 53).\\nThe resulting plot is as follows:\\nWe can see that higher eps values produce more visible noise in the image and have\\na bigger impact on the model’s performance. And while the model predicted theview raw comp areepspyhosted with❤ byGitHub48        # Showing the current eps value, test accuracy, and prediction\\n49        axes[i, j].set_title (\\n50            label=f\"Eps value: {eps_to_try [counter]}\\\\n\"\\n51            f\"Test accuracy: {test_score  * 100:.2f}%\\\\n\"\\n52            f\"Prediction: {prediction }\"\\n53            )\\n54\\n55        # Incrementing counter\\n56        counter += 1\\n57\\n58# Showing the plot\\n59plt.show()\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e149dc6d-d076-4d3d-892c-bf57b9641f46', embedding=None, metadata={'page_label': '15', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 15/50correct label for this particular image 9 times out of 10, the overall performance of\\nthe model on the entire adversarial test set was much worse.\\nExtraction Attacks in ART\\nAs the second step, let’s show extraction attacks in ART. Extraction attacks, as a\\nreminder, aim to copy or steal a victim model.\\nLet’s use the class art.attacks.extraction.CopycatCNN to perform the attack. You can\\nlearn more about this attack method in this paper.\\nTraining a victim model\\nFor this attack, let’s separate our training dataset into two subsets — one with 50,000\\nsamples and the other with 10,000.\\nThe subset with 50,000 samples will be used to train the original model, while the\\nsubset with 10,000 samples will be used to steal the original model. Basically, we are\\nsimulating a situation where an adversary has a dataset that is similar to the original\\ndataset.\\nLet’s train our original model on its dataset:view raw define_subsets.py hosted with ❤  by GitHub1# Importing CopycatCNN\\n2from art.attacks.extraction  import CopycatCNN\\n3\\n4# Setting aside a subset of the source dataset for the original model\\n5train_images_original  = train_images [:50000]\\n6train_labels_original  = train_labels [:50000]\\n7\\n8# Using the rest of the source dataset for the stolen model\\n9train_images_stolen  = train_images [50000:]\\n10train_labels_stolen  = train_labels [50000:]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='675110a5-472c-4a55-ad3a-f4b705da2103', embedding=None, metadata={'page_label': '16', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 16/50After training, we again wrap the original model into KerasClassifier:\\nDefining and running an extraction attack\\nNext, let’s create our model thief, using the class CopycatCNN:view raw\\nieratrain_model.py hosted with ❤  by GitHub\\ntraining otpttthosted ith❤ bGitH b1# Training the original model on its training subset\\n2model_original  = create_model ()\\n3model_original .fit(\\n4    x=train_images_original ,\\n5    y=train_labels_original ,\\n6    epochs=10,\\n7    batch_size =256\\n8)\\n1Train on 50000 samples\\n2Epoch 1/10\\n350000/50000 [==============================] - 2s 36us/sample - loss: 0.4450 - accuracy: 0.8741\\n4Epoch 2/10\\n550000/50000 [==============================] - 2s 32us/sample - loss: 0.1050 - accuracy: 0.9681\\n6Epoch 3/10\\n750000/50000 [==============================] - 2s 32us/sample - loss: 0.0750 - accuracy: 0.9770\\n8Epoch 4/10\\n950000/50000 [==============================] - 2s 32us/sample - loss: 0.0615 - accuracy: 0.9810\\n10Epoch 5/10\\n1150000/50000 [==============================] - 2s 32us/sample - loss: 0.0522 - accuracy: 0.9843\\n12Epoch 6/10\\n1350000/50000 [==============================] - 2s 32us/sample - loss: 0.0463 - accuracy: 0.9864\\n14Epoch 7/10\\n1550000/50000 [==============================] - 2s 32us/sample - loss: 0.0406 - accuracy: 0.9876\\n16Epoch 8/10\\n1750000/50000 [==============================] - 2s 33us/sample - loss: 0.0379 - accuracy: 0.9886\\n18Epoch 9/10\\n1950000/50000 [==============================] - 2s 33us/sample - loss: 0.0342 - accuracy: 0.9890\\n20Epoch 10/10\\n2150000/50000 [==============================] - 2s 35us/sample - loss: 0.0299 - accuracy: 0.9908\\nview raw wrap_model.py hosted with ❤  by GitHub1# Wrapping the model in the ART KerasClassifier class\\n2classifier_original  = KerasClassifier (\\n3    model=model_original ,\\n4    clip_values =(min, max))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f512e659-ea55-4ff5-bf85-189021be762c', embedding=None, metadata={'page_label': '17', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 17/50Note that the argument nb_stolen=len(train_images_stolen) essentially determines\\nhow many samples ART will use to train the stolen model.\\nAfter that, we need to create a blank reference model that copycat_cnn will train to\\nsteal the original model:\\nWe then use the method copycat_cnn.extract to steal classifier_original. We are\\nusing the subset with 10,000 samples to steal the model.view raw extraction_attack .py hosted with ❤  by GitHub1# Creating the \"neural net thief\" object\\n2# that will steal the original classifier\\n3copycat_cnn  = CopycatCNN (\\n4    batch_size_fit =256,\\n5    batch_size_query =256,\\n6    nb_epochs =20,\\n7    nb_stolen =len(train_images_stolen ),\\n8    classifier =classifier_original\\n9    )\\nview raw create_reference.py hosted with ❤  by GitHub1# Creating a reference model for theft\\n2model_stolen  = KerasClassifier (\\n3    model=create_model (), \\n4    clip_values =(min, max)\\n5    )', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='47f65b25-3442-4bcb-b0e5-788f0fd33446', embedding=None, metadata={'page_label': '18', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 18/50Evaluating the performance of the stolen model\\nLet’s compare the performance of the original and stolen models on the test set:view raw\\niextract_model.py hosted with ❤  by GitHub\\n i h dih❤ bGiHb1# Extracting a thieved classifier\\n2# by training the reference model\\n3stolen_classifier  = copycat_cnn .extract(\\n4    x=train_images_stolen , \\n5    y=train_labels_stolen , \\n6    thieved_classifier =model_stolen\\n7    )\\n1Train on 10000 samples\\n2Epoch 1/20\\n310000/10000 [==============================] - 0s 39us/sample - loss: 1.3172 - accuracy: 0.6578\\n4Epoch 2/20\\n510000/10000 [==============================] - 0s 30us/sample - loss: 0.3373 - accuracy: 0.8992\\n6Epoch 3/20\\n710000/10000 [==============================] - 0s 30us/sample - loss: 0.2134 - accuracy: 0.9381\\n8Epoch 4/20\\n910000/10000 [==============================] - 0s 30us/sample - loss: 0.1644 - accuracy: 0.9508\\n10Epoch 5/20\\n1110000/10000 [==============================] - 0s 30us/sample - loss: 0.1248 - accuracy: 0.9628\\n12Epoch 6/20\\n1310000/10000 [==============================] - 0s 29us/sample - loss: 0.0988 - accuracy: 0.9702\\n14Epoch 7/20\\n1510000/10000 [==============================] - 0s 30us/sample - loss: 0.0871 - accuracy: 0.9741\\n16Epoch 8/20\\n1710000/10000 [==============================] - 0s 30us/sample - loss: 0.0693 - accuracy: 0.9792\\n18Epoch 9/20\\n1910000/10000 [==============================] - 0s 30us/sample - loss: 0.0599 - accuracy: 0.9819\\n20Epoch 10/20\\n2110000/10000 [==============================] - 0s 30us/sample - loss: 0.0496 - accuracy: 0.9850\\n22Epoch 11/20\\n2310000/10000 [==============================] - 0s 30us/sample - loss: 0.0430 - accuracy: 0.9872\\n24Epoch 12/20\\n2510000/10000 [==============================] - 0s 30us/sample - loss: 0.0355 - accuracy: 0.9901\\n26...\\n27Epoch 19/20\\n2810000/10000 [==============================] - 0s 30us/sample - loss: 0.0141 - accuracy: 0.9974\\n29Epoch 20/20\\n3010000/10000 [==============================] - 0s 30us/sample - loss: 0.0119 - accuracy: 0.9984', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1f18f0e7-3a5b-4eda-82ba-fa6714d21c95', embedding=None, metadata={'page_label': '19', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 19/50Original test loss: 0.04 vs stolen test loss: 0.08Original test \\naccuracy: 0.99 vs stolen test accuracy: 0.98\\nThe models perform very similarly, so it appears that the model theft was\\nsuccessful. With that said, additional testing might be required to determine if the\\nstolen model indeed performs well.\\nOne thing to keep in mind here — the more data you have, the better the stolen\\nclassifier will be. We can see this by testing the effect of the subset size on the\\nperformance of the stolen model:view raw original_vs_st olen.py hosted with ❤  by GitHub1# Testing the performance of the original classifier\\n2score_original  = classifier_original ._model.evaluate (\\n3    x=test_images , \\n4    y=test_labels\\n5    )\\n6\\n7# Testing the performance of the stolen classifier\\n8score_stolen  = stolen_classifier ._model.evaluate (\\n9    x=test_images , \\n10    y=test_labels\\n11    )\\n12\\n13# Comparing test losses\\n14print(f\"Original test loss: {score_original [0]:.2f} \" \\n15      f\"vs stolen test loss: {score_stolen [0]:.2f}\")\\n16\\n17# Comparing test accuracies\\n18print(f\"Original test accuracy: {score_original [1]:.2f} \" \\n19      f\"vs stolen test accuracy: {score_stolen [1]:.2f}\")', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='118db71e-6688-4fff-acf0-c4cff34df750', embedding=None, metadata={'page_label': '20', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 20/50view raw train_subsets.py hosted with ❤  by GitHub1# Defining subsets to try\\n2data_subsets_to_try  = [2500, 5000, 7500, 10000]\\n3\\n4# Initializing a dict to store scores\\n5scores = {}\\n6\\n7# Iterating over each data subset\\n8for data_subset  in data_subsets_to_try :\\n9    # Creating a reference model for theft\\n10    model_stolen  = KerasClassifier (\\n11        model=create_model (), \\n12        clip_values =(0, 1)\\n13        )\\n14\\n15    # Creating the \"neural net thief\" object\\n16    # to train with the current subset size\\n17    copycat_cnn  = CopycatCNN (\\n18        batch_size_fit =256,\\n19        batch_size_query =256,\\n20        nb_epochs =20,\\n21        nb_stolen =data_subset ,\\n22        classifier =classifier_original\\n23    )\\n24\\n25    # Extracting a thieved classifier,\\n26    # using a subset of the stolen data\\n27    stolen_classifier  = copycat_cnn .extract(\\n28        x=train_images_stolen [:data_subset ], \\n29        y=train_labels_stolen [:data_subset ], \\n30        thieved_classifier =model_stolen\\n31        )\\n32\\n33    # Calculating test metrics for the current stolen model\\n34    scores[data_subset ] = stolen_classifier ._model.evaluate (\\n35        x=test_images ,\\n36        y=test_labels\\n37    )   \\n1Train on 2500 samples\\n2Epoch 1/20\\n32500/2500 [==============================] - 0s 85us/sample - loss: 2.1984 - accuracy: 0.3116\\n4Epoch 2/20\\n52500/2500 [==============================] - 0s 34us/sample - loss: 1.7207 - accuracy: 0.6852\\n6Epoch 3/20\\n72500/2500 [==============================] - 0s 32us/sample - loss: 0.9790 - accuracy: 0.7840\\n8Epoch 4/20\\n92500/2500 [==============================] - 0s 31us/sample - loss: 0.5733 - accuracy: 0.8388', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1bd20794-d6ca-4a67-9680-a81bc48a1e33', embedding=None, metadata={'page_label': '21', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 21/50Let’s now visualize the test losses for each subset:10Epoch 5/20\\n112500/2500 [==============================] - 0s 30us/sample - loss: 0.4315 - accuracy: 0.8732\\n12Epoch 6/20\\n132500/2500 [==============================] - 0s 30us/sample - loss: 0.3561 - accuracy: 0.9004\\n14Epoch 7/20\\n152500/2500 [==============================] - 0s 30us/sample - loss: 0.3023 - accuracy: 0.9152\\n16Epoch 8/20\\n172500/2500 [==============================] - 0s 30us/sample - loss: 0.2583 - accuracy: 0.9328\\n18Epoch 9/20\\n192500/2500 [==============================] - 0s 30us/sample - loss: 0.2257 - accuracy: 0.9364\\n20Epoch 10/20\\n212500/2500 [==============================] - 0s 32us/sample - loss: 0.1941 - accuracy: 0.9480\\n22Epoch 11/20\\n232500/2500 [==============================] - 0s 31us/sample - loss: 0.1796 - accuracy: 0.9504\\n24Epoch 12/20\\n252500/2500 [==============================] - 0s 30us/sample - loss: 0.1576 - accuracy: 0.9528\\n26...\\n27Epoch 19/20\\n2810000/10000 [==============================] - 0s 30us/sample - loss: 0.0152 - accuracy: 0.9960\\n29Epoch 20/20\\n3010000/10000 [==============================] - 0s 30us/sample - loss: 0.0136 - accuracy: 0.9961\\nview raw show_losses.py hosted with ❤  by GitHub1# Converting the dict values to a Python list\\n2score_values  = list(scores.values())\\n3\\n4# Creating a matplotlib figure\\n5fig = plt.figure(figsize=(10, 10))\\n6\\n7# Iterating over our data subsets,\\n8# plotting the test loss for each\\n9for i in range(len(data_subsets_to_try )):\\n10    plt.bar(\\n11        x=str(data_subsets_to_try [i]), \\n12        height=score_values [i][0]\\n13        )\\n14\\n15# Setting a title for the figure and showing it\\n16plt.title(label=\"Test loss of the stolen classifiers based on the number of stolen samples\" )\\n17plt.xlabel(xlabel=\"Subset size\" )\\n18plt.ylabel(ylabel=\"Test loss\" )\\n19plt.show()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='922ca815-e0fe-461a-b346-ced181d2ae2f', embedding=None, metadata={'page_label': '22', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 22/50The resulting plot is as follows:\\nThe jump from 2,500 to 5,000 samples produced the biggest improvement, although\\nthere also are noticeable differences between subsets of 5,000, 7,500, and 10,000\\nsamples.\\nThe same applies to the test accuracy, though to a much lesser degree:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ce1c5378-0c8e-4dd5-9309-39c7c2f26ccf', embedding=None, metadata={'page_label': '23', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 23/50view raw show accuracies.py hosted with ❤  by GitHub1# Creating a matplotlib figure\\n2fig = plt.figure(figsize=(10, 10))\\n3\\n4# Iterating over our data subsets,\\n5# plotting the test accuracy for each\\n6for i in range(len(data_subsets_to_try )):\\n7    plt.bar(\\n8        x=str(data_subsets_to_try [i]), \\n9        height=score_values [i][1] * 100\\n10        )\\n11\\n12# Setting a title for the figure and showing it\\n13plt.title(label=\"Test accuracy of the stolen classifiers based on the number of stolen samples\"\\n14plt.xlabel(xlabel=\"Subset size\" )\\n15plt.ylabel(ylabel=\"Test accuracy\" )\\n16plt.show()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a630bc53-9f0c-4994-8000-71f53d214e09', embedding=None, metadata={'page_label': '24', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 24/50\\nInference Attacks in ART\\nNow, let’s try inference attacks. Inference attacks aim to obtain knowledge about the\\ndataset that was used to train a victim model. In this guide, we are going to try\\nmodel inversion — an attack where the adversary tries to recover the training\\ndataset of the victim model.\\nAs of version 1.10.0, ART supported only one model inversion algorithm — MIFace.\\nMIFace uses class gradients to infer the training dataset. You can learn more about\\nMIFace in this paper.\\nDefining the attack\\nThe first step to model inversion with MIFace is instantiating its class. We are going\\nto apply the attack the classifier we’ve trained for the evasion attack.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f00b3c93-7cd3-47b1-9941-3cb03017aa83', embedding=None, metadata={'page_label': '25', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 25/50Adjust the parameter max_iter based on your hardware — if you find that inversion\\ntakes too long, reduce the value.\\nAfter that, we need to define the targets that we want to infer samples for:\\n[0 1 2 3 4 5 6 7 8 9]\\nIn our case, y consists of integer labels. You can also provide one-hot labels with\\nthe shape (nb_samples, nb_classes).\\nAside from the targets, we also need to define an initialization array that MIFace will\\nuse to infer images. Let’s use the average of the test images as the initialization array\\n— you can also use an array of zeros, ones, or any other array that you think might\\nwork.\\nNote that the batch dimension of x_init_average has the same length as our target\\nlist y.view raw model_inv ersion.py hosted with ❤  by GitHub1# Importing dependencies\\n2from art.attacks.inference .model_inversion  import MIFace\\n3\\n4# Defining a model inversion attack\\n5attack = MIFace(\\n6    classifier =classifier ,\\n7    max_iter =2500,\\n8    batch_size =256)\\nview raw define_labels.py hosted with ❤  by GitHub1# Defining the target labels for model inversion\\n2y = np.arange(start=0, stop=10)\\n3\\n4# Inspecting the target labels\\n5print(y)\\nview raw define_init_array .py hosted with ❤  by GitHub1# Defining an initialization array for model inversion\\n2x_init_average  = np.zeros(shape=(10, 28, 28, 1)) + np.mean(a=test_images , axis=0)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8d4daf60-2d44-4082-a541-84133c980fdf', embedding=None, metadata={'page_label': '26', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 26/50We also need to calculate class gradients with our initialization array to make sure\\nthat they have sufficient magnitude for inference:\\n[0.14426005 0.1032533 0.0699798 0.04295066 0.00503148 0.01931691 \\n0.02252066 0.00906549 0.06300844 0.16753715]\\nThe gradients for some classes are larger than for others. We can see that the\\ngradients at index positions 4 and 7 — which correspond to the digits 4 and 7 — are\\nreally small compared to others.\\nIf the gradients for a particular class are too small, the attack might not be able to\\nrecreate its corresponding sample. It’s therefore important to check the class\\ngradients before running an attack. If you find that the gradients are small, you can\\ntry another initialization array.\\nRunning a model inversion attack\\nWe can now run model inversion, using the initialization array and target labels:view raw inspect_class_gradients.py hosted with ❤  by GitHub1# Checking class gradients\\n2class_gradient  = classifier .class_gradient (\\n3    x=x_init_average , \\n4    label=y\\n5    )\\n6\\n7# Reshaping class gradients\\n8class_gradient  = np.reshape(\\n9    a=class_gradient , \\n10    newshape =(10, 28*28)\\n11    )\\n12\\n13# Obtaining the largest gradient value for each class\\n14class_gradient_max  = np.max(class_gradient , axis=1)\\n15\\n16# Inspecting class gradients\\n17print(class_gradient_max )', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='975b35df-e0d7-4e92-b80c-c403ba58adb0', embedding=None, metadata={'page_label': '27', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 27/50Model inversion: 100%| ██████████ | 1/1 [00:51<00:00, 51.23s/it]Wall \\ntime: 51.2 s\\nLet’s now inspect the inferred images:view raw infer_model.py hosted with ❤  by GitHub1%%time\\n2\\n3# Running model inversion\\n4x_infer_from_average  = attack.infer(\\n5    x=x_init_average , \\n6    y=y\\n7    )\\nview raw show_inferr ed_images.py hosted with ❤  by GitHub1# Creating a figure and axes for our plot\\n2fig, axes = plt.subplots (\\n3    nrows=nrows, \\n4    ncols=ncols, \\n5    figsize=(20, 10)\\n6    )\\n7\\n8# Declaring a counting variable\\n9counter = 0\\n10\\n11# Iterating over the axes and plotting the inferred images in them\\n12for i in range(nrows):\\n13    for j in range(ncols):        \\n14        axes[i, j].set_xticks (ticks=[])\\n15        axes[i, j].set_yticks (ticks=[])\\n16        axes[i, j].imshow(X=x_infer_from_average [counter])\\n17\\n18        # Incrementing the counter\\n19        counter += 1\\n20\\n21# Showing the plotted axes\\n22plt.show()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='906bfd19-f08d-43d6-a7f5-db3a91ba8598', embedding=None, metadata={'page_label': '28', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 28/50MIFace managed to recover most of the images, though the digits 2 and 7 aren’t that\\ngreat.\\nYou can try other initialization arrays to see if you can get a better representation of\\nthe digits. For starters, try arrays of gray, black, or white pixel values.\\nPoisoning Attacks in ART\\nFinally, let’s try poisoning attacks. In a poisoning attack, an adversary perturbs\\nsamples in the training dataset to cause the model to overfit to them. The\\nperturbations in the samples are called a backdoor. The goal of poisoning is to make\\nthe model produce the desired output upon encountering the backdoor.\\nTo perform poisoning attacks, we are going to use backdoor attacks (paper) and\\nclean label backdoor attacks (paper).\\nPoisoning sample data\\nTo start, let’s import dependencies and see how data poisoning works:\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c61f8134-c788-45f3-9d9d-61ab27f37c82', embedding=None, metadata={'page_label': '29', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 29/50The target labels for poisoning are[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.][0. 0. 0. 0. 0. 1. 0. 0. 0. 0.][0. 0. \\n0. 0. 0. 1. 0. 0. 0. 0.][0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\\nOn line 7, we define a backdoor attack, using the perturbation add_pattern_bd. By\\ndefault, this perturbation adds a small pattern in the lower right corner of the target\\nimage.\\nOn lines 10 to 13, we define a target label for the backdoor attack. The attack will\\nreplace the real labels with our target label. In our case, we are generating five\\ntarget labels because we want to show how this attack works on five images.\\nTo poison clean images, use the method backdoor.poison. This method returns\\nperturbed images along with the fake labels. You can use the perturbed images and\\nthe fake labels for training.view raw backdoor_poisoning.py hosted with ❤  by GitHub1# Importing dependencies\\n2from art.attacks.poisoning  import PoisoningAttackBackdoor , PoisoningAttackCleanLabelBackdoor\\n3from art.attacks.poisoning .perturbations  import add_pattern_bd\\n4from art.utils import to_categorical\\n5\\n6# Defining a poisoning backdoor attack\\n7backdoor  = PoisoningAttackBackdoor (perturbation =add_pattern_bd )\\n8\\n9# Defining a target label for poisoning\\n10target = to_categorical (\\n11    labels=np.repeat(a=5, repeats=5), \\n12    nb_classes =10\\n13    )\\n14\\n15# Inspecting the target labels\\n16print(f\"The target labels for poisoning are \\\\n {target}\")\\nview raw poison_data.py hosted with ❤  by GitHub1# Poisoning sample data\\n2poisoned_images , poisoned_labels  = backdoor .poison(\\n3    x=train_images [:5], \\n4    y=target\\n5    )', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9e1b1981-1b23-44f1-bfaa-4add6fa3b928', embedding=None, metadata={'page_label': '30', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 30/50Note that poisoned_labels is the exact same as the array target that we provided.\\nbackdoor returns the fake labels along with the poisoned images for your\\nconvenience.\\nThe perturbations are clearly visible in the poisoned images. The small pattern at\\nthe bottom of each image is meant to cause the target model to overfit to them.\\nBecause fake training labels are provided for the images, the model will learn to\\nassociate the patterns with the fake labels. If the attack is successful, the model will\\nclassify any image that contains the pattern as 5.\\nDefining a backdoor attack\\nNow, let’s define a backdoor attack for training, but with a little twist. We can go one\\nstep further and combine the standard backdoor attack with the clean label\\nbackdoor attack. The clean label backdoor attack works a bit differently — at a highview raw show_poisoned.py hosted with ❤  by GitHub1# Creating a figure and axes for the poisoned images\\n2fig, axes = plt.subplots (\\n3    nrows=1, \\n4    ncols=5, \\n5    squeeze=True, \\n6    figsize=(15, 5)\\n7    )\\n8\\n9# Plotting the poisoned images\\n10for i in range(len(poisoned_images )):\\n11    axes[i].imshow(X=poisoned_images [i])\\n12    axes[i].set_title (label=f\"Label: {np.argmax(poisoned_labels [i])}\")\\n13    axes[i].set_xticks (ticks=[])\\n14    axes[i].set_yticks (ticks=[])    \\n15\\n16# Showing the plot\\n17plt.show()\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='03851fe5-5476-4eb3-8f71-f6706ed3bbc0', embedding=None, metadata={'page_label': '31', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 31/50level, it also perturbs the target images, but it keeps the original labels. Hence why this\\nattack is called “clean label.”\\nHere’s how you define a clean label backdoor attack in ART:\\nWe’ve provided a number of arguments to PoisoningAttackCleanLabelBackdoor,\\nincluding pp_poison=0.75. This argument determines the fraction of images that\\nshould be poisoned. target=target defines the target whose samples should be\\npoisoned. Our attack will poison 75% of the images of the digit 9.\\nproxy_classifier=classifier specifies that our original classifier will be used to\\npoison the dataset. We are essentially using a classifier that is similar to the victim\\nclassifier to help us poison the data.\\nLet’s poison a subset of our training samples — we will later use this poisoned subset\\nto train a victim model. We are not using the entire training dataset because\\npoisoning can take a long time.view raw clean_label_b ackdoor_attack .py hosted with ❤  by GitHub1# Defining a target label for poisoning\\n2target = to_categorical (\\n3    labels=[9], \\n4    nb_classes =10\\n5    )[0]\\n6\\n7# Defining a clean label backdoor attack\\n8attack = PoisoningAttackCleanLabelBackdoor (\\n9    backdoor =backdoor , \\n10    proxy_classifier =classifier ,\\n11    target=target, \\n12    pp_poison =0.75, \\n13    norm=2, \\n14    eps=5, \\n15    eps_step =0.1, \\n16    max_iter =200)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='17b6f7e4-bcba-4348-b5bc-5d3ef4b34dc1', embedding=None, metadata={'page_label': '32', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 32/50To better understand what happened, let’s visualize the poisoned images along with\\ntheir clean originals:view raw\\nview rawpoison_data.py hosted with ❤  by GitHub\\npoison_output.t xt hosted with ❤  by GitHub1# Poisoning training data\\n2poisoned_images , poisoned_labels  = attack.poison(\\n3    x=train_images [:10000], \\n4    y=train_labels [:10000]\\n5    )\\n1PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\\n2PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\\n3PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\\n4PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\\n5PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\\n6PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\\n7PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\\n8PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\\n9PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\\n10PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\\n11PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\\n12PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\\n13PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\\n14PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\\n15PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\\n16PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\\n17PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\\n18PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\\n19PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\\n20PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\\n21PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\\n22PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\\n23PGD - Random Initializations: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='96e1d326-0e0b-4ae3-be07-c1cc0f3a2634', embedding=None, metadata={'page_label': '33', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 33/501# Getting the indices of the images\\n2# whose target corresponds to our backdoor target\\n3poisoned_indices  = np.all(\\n4        a=(poisoned_labels  == target), \\n5        axis=1\\n6        )\\n7\\n8# Getting a few images from the poisoned and clean dataset for comparison\\n9sample_poisoned_images  = poisoned_images [poisoned_indices ][:5]\\n10sample_clean_images  = train_images [:10000][poisoned_indices ][:5]\\n11\\n12# Defining a number of rows and columns for the plot\\n13nrows, ncols = 5, 2\\n14\\n15# Creating a figure and axes\\n16fig, axes = plt.subplots (\\n17        nrows=nrows, \\n18        ncols=ncols, \\n19        figsize=(10, 25)\\n20        )\\n21\\n22# Defining a counting variable\\n23counter = 0\\n24\\n25# Indicating the purpose of each column\\n26axes[0, 0].set_title (\\n27        label=\"Images from the poisoned dataset\" , \\n28        pad=25\\n29        )\\n30axes[0, 1].set_title (\\n31        label=\"Images from the clean dataset\" , \\n32        pad=25\\n33        )\\n34\\n35# Iterating over the axis rows in our figure\\n36for i in range(nrows):    \\n37        # Plotting the image from the poisoned dataset,\\n38        # turning off axis ticks,\\n39        # and setting axis title\\n40        axes[i, 0].imshow(sample_poisoned_images [counter])        \\n41        axes[i, 0].set_xticks (ticks=[])\\n42        axes[i, 0].set_yticks (ticks=[])\\n43        \\n44\\n45        # Plotting the image from the clean dataset,\\n46        # turning off axis ticks,\\n47        # and setting axis title\\n48 axes[i1]imshow(samplecleanimages[counter])', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='484e6331-a94c-4ea3-a6ea-f5d19075d509', embedding=None, metadata={'page_label': '34', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 34/50Above, we obtain the indices of the samples whose labels correspond to our target\\nlabel 9 (lines 3 to 6). These are the indices of the images that our attack poisoned.\\nWe then use these indices to obtain their corresponding poisoned and original\\nimages (lines 9 and 10).\\nAfter that, we create a figure with two columns and axes (lines 16 to 20). The left\\ncolumn will show images from the poisoned dataset, while the right column will\\nshow the original clean images. We indicate which column is which on lines 26 to\\n33.\\nOn line 36, we iterate over each axis row and display the images from the poisoned\\ndataset on the left (lines 40 to 42) and the clean images on the right (lines 48 to 50).\\nThe resulting plot is as follows:view raw comp areclean poisoned pyhosted with❤ byGitHub48        axes[i, 1].imshow(sample_clean_images [counter])\\n49        axes[i, 1].set_xticks (ticks=[])\\n50        axes[i, 1].set_yticks (ticks=[])\\n51\\n52\\n53        # Incrementing counter value\\n54        counter += 1\\n55\\n56# Showing the plot\\n57plt.show()', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5a3e428b-7890-4a3d-b88f-9506a1abe982', embedding=None, metadata={'page_label': '35', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 35/50\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='31c5ae21-d2ed-4043-b950-5c56955edb03', embedding=None, metadata={'page_label': '36', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 36/50\\nWe can see that some of the images of the digit 9 are noticeably perturbed. We can\\nalso see that the perturbed images contain the pattern applied by\\nPoisonAttackBackdoor.\\nSo what happened?\\n1. First, PoisoningAttackCleanLabelBackdoor took 75% of the images of the digit 9 and\\napplied an initial perturbation to them. If we take a look at the source code of this\\nattack’s class, we can see that the initial attack is ProjectecGradientDescent (the class\\nin the docs and the paper for the method). The perturbations made the samples look\\nless like the digit 9.\\n2. After that, the perturbed samples are passed to PoisonAttackBackdoor to add the\\nsmall pattern in their lower right corner.\\n3. Finally, PoisoningAttackCleanLabelBackdoor returns the perturbed images along\\nwith the original labels.\\nBasically, what this attack does is that it modifies the appearance of the digit 9 so\\nthat the model cannot reliably use its outlines for classification. The attack also\\nforces the model to overfit to the pattern in the lower right corner of each image,\\nmaking the model associate that pattern with the digit 9. So at inference time, every\\ndigit that has the pattern next to them should be classified as a 9.\\nTraining a victim classifier', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c5b4d613-19be-42ef-a4ac-2774080140e7', embedding=None, metadata={'page_label': '37', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 37/50Next, let’s define a new victim classifier:\\nThe reason why we are not reusing the original model architecture is that it wasn’t\\nsusceptible to data poisoning in our tests. This might be the case with many other\\nmodel architectures as well — they might be resistant to some forms of poisoning\\nand not others. Techniques against overfitting might increase resistance to\\npoisoning as well.\\nLet’s train the victim model on the poisoned dataset:view raw create_model.py hosted with ❤  by GitHub1# Function for creating victim model\\n2def create_victim_model ():\\n3    # Defining the model\\'s architecture\\n4    model = tf.keras.models.Sequential ([\\n5        Conv2D(filters=32, kernel_size =3, activation =\"relu\", input_shape =(28, 28, 1)),\\n6        Conv2D(filters=32, kernel_size =3, activation =\"relu\"),\\n7        MaxPool2D (pool_size =2),\\n8        Flatten(),\\n9        Dense(units=10, activation =\"softmax\" )\\n10    ])\\n11    \\n12    # Compiling the model\\n13    model.compile(\\n14        loss=\\'categorical_crossentropy\\' , \\n15        optimizer =\\'adam\\', \\n16        metrics=[\\'accuracy\\' ]\\n17        )\\n18\\n19    # Returning the model\\n20    return model   Open in app\\nSearch\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='53376b1a-0b60-4de3-b05b-9cb76deb6d46', embedding=None, metadata={'page_label': '38', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 38/50Poisoning data at inference time\\nNow that we have a model with a backdoor, let’s poison the test set to see if the\\nbackdoor works. We will poison all samples that are not nines. We will keep the\\noriginal labels for performance testing purposes.view raw\\nieratrain_poisoned_model.py hosted with ❤  by GitHub\\ntraining otpttthosted ith❤ bGitH b1# Creating and training a victim classifier\\n2# with the poisoned data\\n3model_poisoned  = create_victim_model ()\\n4model_poisoned .fit(\\n5    x=poisoned_images , \\n6    y=poisoned_labels , \\n7    epochs=10\\n8    )\\n1Train on 10000 samples\\n2Epoch 1/10\\n310000/10000 [==============================] - 2s 175us/sample - loss: 0.4110 - accuracy: 0.8754\\n4Epoch 2/10\\n510000/10000 [==============================] - 1s 142us/sample - loss: 0.1279 - accuracy: 0.9609\\n6Epoch 3/10\\n710000/10000 [==============================] - 1s 141us/sample - loss: 0.0796 - accuracy: 0.9761\\n8Epoch 4/10\\n910000/10000 [==============================] - 1s 142us/sample - loss: 0.0593 - accuracy: 0.9821\\n10Epoch 5/10\\n1110000/10000 [==============================] - 1s 147us/sample - loss: 0.0380 - accuracy: 0.9885\\n12Epoch 6/10\\n1310000/10000 [==============================] - 2s 150us/sample - loss: 0.0299 - accuracy: 0.9907\\n14Epoch 7/10\\n1510000/10000 [==============================] - 1s 141us/sample - loss: 0.0255 - accuracy: 0.9918\\n16Epoch 8/10\\n1710000/10000 [==============================] - 1s 137us/sample - loss: 0.0154 - accuracy: 0.9954\\n18Epoch 9/10\\n1910000/10000 [==============================] - 1s 141us/sample - loss: 0.0126 - accuracy: 0.9963\\n20Epoch 10/10\\n2110000/10000 [==============================] - 1s 140us/sample - loss: 0.0107 - accuracy: 0.9966', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6117675a-4bc6-4f19-bb30-c136ef5b5c74', embedding=None, metadata={'page_label': '39', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 39/50Let’s visualize the poisoned images along with their true labels:view raw poison_t est_data.py hosted with ❤  by GitHub1# Getting the indices of the test images whose target \\n2# is different from the backdoor target\\n3not_target  = np.logical_not (np.all(\\n4    a=test_labels  == target, \\n5    axis=1\\n6    ))\\n7\\n8# Poisoning the test data while keeping the labels the same\\n9px_test, py_test = backdoor .poison(\\n10    x=test_images [not_target ], \\n11    y=test_labels [not_target ]\\n12    )\\nview raw plot_poisoned.py hosted with ❤  by GitHub1# Setting the number of rows and columns for the plot\\n2nrows, ncols = 2, 5\\n3\\n4# Creating a figure and axes\\n5fig, axes = plt.subplots (\\n6    nrows=nrows, \\n7    ncols=ncols, \\n8    figsize=(25, 10)\\n9    )\\n10\\n11# Defining a counting variable\\n12counter = 0\\n13\\n14# Iterating over rows and cols,\\n15# plotting poisoned test images\\n16# along with their true targets\\n17for i in range(nrows):\\n18    for j in range(ncols):\\n19        axes[i, j].imshow(px_test[counter])\\n20        axes[i, j].set_title (label=f\"True label: {np.argmax(py_test[counter])}\")\\n21        axes[i, j].set_xticks (ticks=[])\\n22        axes[i, j].set_yticks (ticks=[])\\n23\\n24        # Incrementing the counter\\n25        counter += 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e91d2101-f8ad-4707-9056-65c52295214b', embedding=None, metadata={'page_label': '40', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 40/50We can see that the attack added the pattern in the lower right corner of each image.\\nNow, the model should classify these images as the digit 9 because it had overfit to\\nthat pattern.\\nLet’s evaluate the performance of our model on clean vs poisoned images to see if\\nthe attack worked:\\nClean test loss: 0.13 vs poisoned test loss: 2.31Clean test \\nview raw comp are_poisoned_clean.py hosted with ❤  by GitHub1# Evaluating the poisoned classifier on clean test data\\n2scores_clean  = model_poisoned .evaluate (\\n3    x=test_images , \\n4    y=test_labels\\n5    )\\n6\\n7# Evaluating the poisoned classifier on poisoned test data\\n8scores_poisoned  = model_poisoned .evaluate (\\n9    x=px_test, \\n10    y=py_test\\n11    )\\n12\\n13# Comparing test losses\\n14print(f\"Clean test loss: {scores_clean [0]:.2f} \" \\n15      f\"vs poisoned test loss: {scores_poisoned [0]:.2f}\")\\n16\\n17# Comparing test accuracies\\n18print(f\"Clean test accuracy: {scores_clean [1]:.2f} \" \\n19      f\"vs poisoned test accuracy: {scores_poisoned [1]:.2f}\")', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='094db7ed-b88f-4ae1-8b00-e80e3091242d', embedding=None, metadata={'page_label': '41', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 41/50accuracy: 0.97 vs poisoned test accuracy: 0.60\\nWe can see that the backdoor did work, though perhaps not as effectively as an\\nactual attacker would have liked.\\nAnd as the final step, let’s plot a few poisoned images along with their predictions:\\nview raw plot_poisoned_with_pr eds.py hosted with ❤  by GitHub1# Setting the number of rows and columns for the plot\\n2nrows, ncols = 2, 5\\n3\\n4# Creating a figure and axes\\n5fig, axes = plt.subplots (\\n6    nrows=nrows, \\n7    ncols=ncols, \\n8    figsize=(25, 10)\\n9    )\\n10\\n11# Getting predictions for the first ten poisoned images\\n12poisoned_predictions  = model_poisoned .predict(x=px_test[:10])\\n13\\n14# Defining a counting variable\\n15counter = 0\\n16\\n17# Iterating over rows and cols,\\n18# plotting poisoned images\\n19# along with their predictions\\n20for i in range(nrows):\\n21    for j in range(ncols):\\n22        axes[i, j].imshow(px_test[counter])\\n23        axes[i, j].set_title (label=f\"Prediction: {np.argmax(poisoned_predictions [counter])}\")\\n24        axes[i, j].set_xticks (ticks=[])\\n25        axes[i, j].set_yticks (ticks=[])\\n26\\n27        # Incrementing the counter\\n28        counter += 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f5294927-bd7e-47fe-9a1a-0fbf1cc6cc26', embedding=None, metadata={'page_label': '42', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 42/50Our attack wasn’t super-effective, though we can see that it did work for some\\nsamples. Additional tweaking of the attack might be necessary to achieve better\\nresults.\\nNext Steps\\nYou should go ahead and try out the other attack methods supported in ART! You\\ncan also play around with attack parameters to understand how they impact the\\neffectiveness of attacks.\\nIn PART 2, we will take a look at the defense measures implemented in the\\nframework. ART has a pretty wide range of defenses against the attacks we’ve had a\\nlook at, so there’s a lot to explore!\\nUntil next time!\\nFollow\\nData Science Machine Learning Artificial Intelligence Adversarial Attack\\nData\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5e9947fe-66eb-4ce7-b40f-5849b1575f72', embedding=None, metadata={'page_label': '43', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 43/50Written by Kedion\\n117 Followers\\nKedion brings rapid development and product discovery to machine learning & AI solutions.\\nMore from Kedion\\nKedion\\nCreating a Feature Store with Feast\\nPart 1: Building a Local Feature Store for ML Training and Prediction\\n17 min read·Mar 17, 2022\\n155\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8fd747f5-0a54-43a1-942d-5ad3909640bf', embedding=None, metadata={'page_label': '44', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 44/50\\nKedion\\nManaging Machine Learning Lifecycles with MLflow\\nPart 2: Using MLflow to Deploy Models\\n13 min read·Dec 22, 2021\\n14\\nKedion\\nExplainable AI Framework Comparison\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2053c235-dca3-46ad-a95c-ba34e7270143', embedding=None, metadata={'page_label': '45', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 45/50Part 1: Explaining MNIST Image Classification with SHAP\\n14 min read·Jan 26, 2022\\n15\\nKedion\\nFine-Tuning NLP Models With Hugging Face\\nPart 2: Transfer Learning With TensorFlow\\n17 min read·Sep 30, 2021\\n77\\nSee all from Kedion\\nRecommended from Medium\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ca86972c-a3cd-48f3-9fa2-69adb2c49c85', embedding=None, metadata={'page_label': '46', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 46/50\\nRitanshi Agarwal\\nFingerprinting-based Indoor Localization — Taking Help with Generative\\nAI\\nSimultaneous Localization and Mapping (SLAM) is the umbrella term for all indoor and outdoor\\ntracking techniques. Finding positions in an…\\n5 min read·Feb 11, 2024\\n151\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2bd2152f-e4cd-403c-adb9-48f1354a1248', embedding=None, metadata={'page_label': '47', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 47/50\\nCristian LeoinTowards Data Science\\nThe Math behind Adam Optimizer\\nWhy is Adam the most popular optimizer in Deep Learning? Let’s understand it by diving into\\nits math, and recreating the algorithm.\\n16 min read·Jan 30, 2024\\n2.1K16\\nLists\\nPredictive Modeling w/ Python\\n20 stories·940 saves\\nNatural Language Processing\\n1225 stories·709 saves\\nPractical Guides to Machine Learning\\n10 stories·1107 saves\\ndata science and AI\\n40 stories·83 saves\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d8158729-4fb7-4033-8a4d-5497a42c56a4', embedding=None, metadata={'page_label': '48', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 48/50\\nJerry LiuinLlamaIndex Blog\\nIntroducing LlamaCloud and LlamaParse\\nToday is a big day for the LlamaIndex ecosystem: we are announcing LlamaCloud, a new\\ngeneration of managed parsing, ingestion, and…\\n8 min read·5 days ago\\n1K11\\nbtd\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='aae2a024-5137-4a00-bb90-8b4f3745efc1', embedding=None, metadata={'page_label': '49', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 49/50Explainable AI (XAI) Tools and Libraries\\nExplainable AI (XAI) tools and libraries are essential components for developing, evaluating,\\nand deploying machine learning models with…\\n·3 min read·Nov 23, 2023\\n22\\nTiya Vaj\\nAdversarial attack\\nAn adversarial attack, in the context of machine learning and deep learning, refers to a\\ndeliberate and malicious attempt to manipulate or…\\n4 min read·Oct 27, 2023\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9770f4a8-cd24-4d01-b0fc-52f71f006c86', embedding=None, metadata={'page_label': '50', 'file_name': 'ART-TUTORIAL.pdf', 'file_path': '/content/data/ART-TUTORIAL.pdf', 'file_type': 'application/pdf', 'file_size': 11675975, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2/26/24, 12:17 PM ML Security with the Adversarial Robustness Toolbox | by Kedion | Medium\\nhttps://kedion.medium.com/ml-security-with-the-adversarial-robustness-toolbox-d173b24e8c1a 50/50\\nBILAL_AI\\nFederated Learning\\nStep-by-Step Federated Learning with Flower and PyTorch”\\n10 min read·Nov 29, 2023\\n6\\nSee more recomme ndations\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='99b6b779-ec1b-4157-9ccb-2af4f3fb02e3', embedding=None, metadata={'page_label': 'I', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST Trustworthy and Responsible AI \\nNIST AI 100-2e2023 \\nAdversarial Machine Learning \\nA Taxonomy and Terminology of Attacks and Mitigations \\nApostol Vassilev \\nAlina Oprea \\nAlie Fordyce \\nHyrum Anderson \\nThis publication is available free of charge from: \\nhttps://doi.org/10.6028/NIST.AI.100-2e2023 \\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1e31c251-f982-44c8-ad2b-dabc0760865d', embedding=None, metadata={'page_label': 'II', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST Trustworthy and Responsible AI \\nNIST AI 100-2e2023 \\nAdversarial Machine Learning \\nA Taxonomy and Terminology of Attacks and Mitigations \\nApostol Vassilev \\nComputer Security Division \\nInformation Technology Laboratory \\nAlina Oprea \\nNortheastern University \\nAlie Fordyce \\nHyrum Anderson \\nRobust Intelligence, Inc. \\nThis publication is available free of charge from: \\nhttps://doi.org/10.6028/NIST.AI.100-2e2023 \\nJanuary 2024 \\nU.S. Department of Commerce \\nGina M. Raimondo, Secretary \\nNational Institute of Standards and Technology \\nLaurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e7a17bed-3888-4624-953c-3f9dc5895007', embedding=None, metadata={'page_label': 'III', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Certain commercial equipment, instruments, software, or materials, commercial or non-commercial, are \\nidentifed in this paper in order to specify the experimental procedure adequately. Such identifcation does not imply recommendation or endorsement of any product or service by NIST, nor does it imply that the materials or equipment identifed are necessarily the best available for the purpose. \\nNIST Technical Series Policies \\nCopyright, Use, and Licensing Statements \\nNIST Technical Series Publication Identifer Syntax \\nPublication History \\nApproved by the NIST Editorial Review Board on 2024-01-02 \\nHow to cite this NIST Technical Series Publication: \\nVassilev A, Oprea A, Fordyce A, Anderson H (2024) Adversarial Machine Learning: A Taxonomy and \\nTerminology of Attacks and Mitigations. (National Institute of Standards and Technology, Gaithersburg, MD) NIST Artifcial Intelligence (AI) Report, NIST Trustworthy and Responsible AI NIST AI 100-2e2023. https://doi.org/10.6028/NIST.AI.100-2e2023 \\nNIST Author ORCID iDs \\nApostol Vassilev: 0000-0002-4979-5292 \\nAlina Oprea: 0000-0002-9081-3042 \\nSubmit Comments \\nai-100-2@nist.gov All comments are subject to release under the Freedom of Information Act (FOIA). ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f94b223a-e80c-4ba7-bec6-143b83847c09', embedding=None, metadata={'page_label': 'i', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Abstract \\nThis NIST Trustworthy and Responsible AI report develops a taxonomy of concepts and defnes \\nterminology in the feld of adversarial machine learning (AML). The taxonomy is built on surveying the \\nAML literature and is arranged in a conceptual hierarchy that includes key types of ML methods and \\nlifecycle stages of attack, attacker goals and objectives, and attacker capabilities and knowledge of the \\nlearning process. The report also provides corresponding methods for mitigating and managing the \\nconsequences of attacks and points out relevant open challenges to take into account in the lifecycle of \\nAI systems. The terminology used in the report is consistent with the literature on AML and is \\ncomplemented by a glossary that defnes key terms associated with the security of AI systems and is \\nintended to assist non-expert readers. Taken together, the taxonomy and terminology are meant to \\ninform other standards and future practice guides for assessing and managing the security of AI systems, \\nby establishing a common language and understanding of the rapidly developing AML landscape. \\nKeywords \\nartifcial intelligence; machine learning; attack taxonomy; evasion; data poisoning; privacy breach; \\nattack mitigation; data modality; trojan attack, backdoor attack; generative models; large language \\nmodel; chatbot. \\nNIST Trustworthy and Responsible AI Reports (NIST Trustworthy and Respon-\\nsible AI) \\nThe National Institute of Standards and Technology (NIST) promotes U.S. innovation and industrial \\ncompetitiveness by advancing measurement science, standards, and technology in ways that enhance \\neconomic security and improve our quality of life. Among its broad range of activities, NIST contributes \\nto the research, standards, evaluations, and data required to advance the development, use, and \\nassurance of trustworthy artifcial intelligence (AI). \\ni ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d793cfa2-346f-4082-8856-e07557d307a2', embedding=None, metadata={'page_label': 'ii', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nTable of Contents \\nAudience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv \\nBackground . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv \\nTrademark Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv \\nHow to read this document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v \\nExecutive Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 \\n1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 \\n2. Predictive AI Taxonomy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 \\n2.1. Attack Classifcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 \\n2.1.1. Stages of Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 \\n2.1.2. Attacker Goals and Objectives . . . . . . . . . . . . . . . . . . . . . 9 \\n2.1.3. Attacker Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 \\n2.1.4. Attacker Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 \\n2.1.5. Data Modality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 \\n2.2. Evasion Attacks and Mitigations . . . . . . . . . . . . . . . . . . . . . . . . . 14 \\n2.2.1. White-Box Evasion Attacks . . . . . . . . . . . . . . . . . . . . . . . 15 \\n2.2.2. Black-Box Evasion Attacks . . . . . . . . . . . . . . . . . . . . . . . 17 \\n2.2.3. Transferability of Attacks . . . . . . . . . . . . . . . . . . . . . . . . 18 \\n2.2.4. Mitigations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 \\n2.3. Poisoning Attacks and Mitigations . . . . . . . . . . . . . . . . . . . . . . . . 21 \\n2.3.1. Availability Poisoning . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 \\n2.3.2. Targeted Poisoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 \\n2.3.3. Backdoor Poisoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 \\n2.3.4. Model Poisoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 \\n2.4. Privacy Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 \\n2.4.1. Data Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 \\n2.4.2. Membership Inference . . . . . . . . . . . . . . . . . . . . . . . . . . 30 \\n2.4.3. Model Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 \\n2.4.4. Property Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 \\n2.4.5. Mitigations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 \\n3. Generative AI Taxonomy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 \\nii ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5b81e8cd-ef1a-46c8-a1cb-a4cab15e40ab', embedding=None, metadata={'page_label': 'iii', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.1. Attack Classifcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 \\n3.1.1. GenAI Stages of Learning . . . . . . . . . . . . . . . . . . . . . . . . 36 \\n3.1.2. Attacker Goals and Objectives . . . . . . . . . . . . . . . . . . . . . 38 \\n3.1.3. Attacker Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 \\n3.2. AI Supply Chain Attacks and Mitigations . . . . . . . . . . . . . . . . . . . . 39 \\n3.2.1. Deserialization Vulnerability . . . . . . . . . . . . . . . . . . . . . . . 39 \\n3.2.2. Poisoning Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 \\n3.2.3. Mitigations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 \\n3.3. Direct Prompt Injection Attacks and Mitigations . . . . . . . . . . . . . . . 40 \\n3.3.1. Data Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 \\n3.3.2. Mitigations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 \\n3.4. Indirect Prompt Injection Attacks and Mitigations . . . . . . . . . . . . . . 44 \\n3.4.1. Availability Violations . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 \\n3.4.2. Integrity Violations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 \\n3.4.3. Privacy Compromises . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 \\n3.4.4. Abuse Violations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 \\n3.4.5. Mitigations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 \\n4. Discussion and Remaining Challenges . . . . . . . . . . . . . . . . . . . . . . . . 50 \\n4.1. The Scale Challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 \\n4.2. Theoretical Limitations on Adversarial Robustness . . . . . . . . . . . . . . 50 \\n4.3. The Open vs. Closed Model Dilemma . . . . . . . . . . . . . . . . . . . . . . 53 \\n4.4. Supply chain challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 \\n4.5. Tradeofs Between the Attributes of Trustworthy AI . . . . . . . . . . . . . 54 \\n4.6. Multimodal Models: Are They More Robust? . . . . . . . . . . . . . . . . . 55 \\n4.7. Quantized models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 \\nAppendix: Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 \\nList of Figures \\nFigure 1.Taxonomy of attacks on Predictive AI systems. . . . . . . . . . . . . 7 \\nFigure 3.Retrieval-augmented generation relies on system instructions, con-\\ntext, and data from third-party sources, often through a vector Figure 2.Taxonomy of attacks on Generative AI systems . . . . . . . . . . . . 36 \\ndatabase, to produce relevant responses for users . . . . . . . . . . . 38 \\niii ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='34bc2b94-6fcc-40f1-95fa-f49b6e7acd75', embedding=None, metadata={'page_label': 'iv', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Audience \\nThe intended primary audience for this \\ndocument includes individuals and groups who are \\nresponsible for designing, developing, deploying, evaluating, and governing AI systems. \\nBackground \\nThis \\ndocument is a result of an extensive literature review, conversations with experts from \\nthe area of adversarial machine learning, and research performed by the authors in adver -\\nsarial machine learning. \\nTrademark Information \\nAll trademarks and re\\ngistered trademarks belong to their respective organizations. \\nThe Information Technology Laboratory (ITL) at NIST develops tests, test methods, ref-\\nerence data, proof of concept implementations, and technical analyses to advance the de-velopment and productive use of information technology. ITL’s responsibilities include the development of management, administrative, technical, and physical standards and guide-lines. \\nThis NIST Trustworthy and Responsible AI report focuses on identifying, addressing, and \\nmanaging risks associated with adversarial machine learning. While practical guidance\\n1 \\n1The term ’practice guide,’ ’guide,’ ’guidance’ or the like, in the context of this paper, is a consensus-created, \\ninformative reference intended for voluntary use; it should not be interpreted as equal to the use of the term ’guidance’ in a legal or regulatory context. This document does not establish any legal standard or any other legal requirement or defense under any law, nor have the force or effect of law. published by NIST may serve as an informative reference, this guidance remains voluntary. \\nThe content of this document refects recommended practices. This document is not in-\\ntended to serve as or supersede existing regulations, laws, or other mandatory guidance. \\niv ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5fe1ada3-6ea0-4619-842b-8f14e9c9ca9b', embedding=None, metadata={'page_label': 'v', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nHow to read this document \\nThis \\ndocument uses terms such as AI technology, AI system, and AI applications inter -\\nchangeably. Terms related to the machine learning pipeline, such as ML model or algo-\\nrithm, are also used interchangeably in this document. Depending on context, the term “system” may refer to the broader organizational and/or social ecosystem within which the technology was designed, developed, deployed, and used instead of the more traditional use related to computational hardware or software. \\nImportant reading notes: \\n• The document includes a series of blue callout boxes that highlight interesting nu-\\nances and important takeaways. \\n• Terms that are used but not defned/explained in the text are listed and defned in the Glossary. They are displayed in small caps in the text. Clicking on a word shown in small caps (e.g., \\nADVERSARIAL EXAMPLES ) takes the reader directly to \\nthe defnition of that term in the Glossary. From there, one may click on the page number shown at the end of the defnition to return. \\nAckno\\nwledgments \\nThe \\nauthors wish to thank all people and organizations who responded to our call and sub-\\nmitted comments to the draft version of this paper. The received comments and suggested references were essential to improving the paper and the future direction of this work. We also want to thank the many NIST colleagues who assisted in updating the document. \\nAutho\\nr Contributions \\nAuthors contrib\\nuted equally to this work. \\nv ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ba6e7c80-238b-475b-8188-b4a1aa33dfe5', embedding=None, metadata={'page_label': '1', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nExecutive Summary \\nThis \\nNIST Trustworthy and Responsible AI report is intended to be a step toward develop-\\ning a taxonomy and terminology of adversarial machine learning (AML), which in turn may \\naid in securing applications of artifcial intelligence (AI) against adversarial manipulations of AI systems. Broadly, there are two classes of AI systems: Predictive and Generative. The components of an AI system include – at a minimum – the data, model, and processes for training, testing, and deploying the machine learning (ML) models and the infrastructure required for using them. Generative AI systems may also be linked to corporate documents and databases when they are adapted to specifc domains and use cases. The data-driven approach of ML introduces additional security and privacy challenges in different phases of ML operations besides the classical security and privacy threats faced by most opera-tional systems. These security and privacy challenges include the potential for adversarial manipulation of training data, adversarial exploitation of model vulnerabilities to adversely affect the performance of the AI system, and even malicious manipulations, modifcations or mere interaction with models to exfltrate sensitive information about people represented in the data, about the model itself, or proprietary enterprise data. Such attacks have been demonstrated under real-world conditions, and their sophistication and potential impact have been increasing steadily. AML is concerned with studying the capabilities of attack-ers and their goals, as well as the design of attack methods that exploit the vulnerabilities of ML during the development, training, and deployment phase of the ML lifecycle. AML is also concerned with the design of ML algorithms that can withstand these security and privacy challenges. When attacks are launched with malevolent intent, the robustness of ML refers to mitigations intended to manage the consequences of such attacks. \\nThis report adopts the notions of security, resilience, and robustness of ML systems from \\nthe NIST AI Risk Management Framework [226]. Security, resilience, and robustness are \\ngauged by risk, which is a measure of the extent to which an entity (e.g., a system) is threat-ened by a potential circumstance or event (e.g., an attack) and the severity of the outcome should such an event occur. However, this report does not make recommendations on risk tolerance (the level of risk that is acceptable to organizations or society) because it is highly contextual and application/use-case specifc. This general notion of risk offers a useful ap-proach for assessing and managing the security, resilience, and robustness of AI system components. Quantifying these likelihoods is beyond the scope of this document. Corre-spondingly, the taxonomy of AML is defned with respect to the following fve dimensions of AML risk assessment: (i) AI system type (Predictive or Generative), (ii) learning method and stage of the ML lifecycle process when the attack is mounted, (iii) attacker goals and objectives, (iv) attacker capabilities, (v) and attacker knowledge of the learning process and beyond. \\nThe spectrum of effective attacks against ML is wide, rapidly evolving, and covers all \\nphases of the ML lifecycle – from design and implementation to training, testing, and f-nally, to deployment in the real world. The nature and power of these attacks are different \\n1 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c0049eb0-7e88-479e-b6dd-c0f4f5cff291', embedding=None, metadata={'page_label': '2', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nand can exploit not just vulnerabilities of the ML models but also weaknesses of the in-\\nfrastructure in which the AI systems are deployed. Although AI system components may also be adversely affected by various unintentional factors, such as design and implemen-tation faws and data or algorithm biases, these factors are not intentional attacks. Even though these factors might be exploited by an adversary, they are not within the scope of the literature on AML or this report. \\nThis document defnes a taxonomy of attacks and introduces terminology in the feld of \\nAML. The taxonomy is built on a survey of the AML literature and is arranged in a con-ceptual hierarchy that includes key types of ML methods and lifecycle stages of attack, attacker goals and objectives, and attacker capabilities and knowledge of the learning pro-cess. The report also provides corresponding methods for mitigating and managing the consequences of attacks and points out relevant open challenges to take into account in the lifecycle of AI systems. The terminology used in the report is consistent with the litera-ture on AML and is complemented by a glossary that defnes key terms associated with the security of AI systems in order to assist non-expert readers. Taken together, the tax-onomy and terminology are meant to inform other standards and future practice guides for assessing and managing the security of AI systems by establishing a common language and understanding for the rapidly developing AML landscape. Like the taxonomy, the termi-nology and defnitions are not intended to be exhaustive but rather to aid in understanding key concepts that have emerged in AML literature. \\n2 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bcb49409-39c7-4e2f-b918-4751ef83fee6', embedding=None, metadata={'page_label': '3', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n1. Introduction \\nArtifcial \\nintelligence (AI) systems [220] are on a global multi-year accelerating expansion \\ntrajectory. These systems are being developed by and widely deployed into the economies \\nof numerous countries, leading to the emergence of AI-based services for people to use in many spheres of their lives, both real and virtual [77]. There are two broad classes of AI \\nsystems, based on their capabilities: Predictive AI (PredAI) and Generative AI (GenAI). As these systems permeate the digital economy and become inextricably essential parts of daily life, the need for their secure, robust, and resilient operation grows. These opera-tional attributes are critical elements of Trustworthy AI in the NIST AI Risk Management Framework [226] and in the taxonomy of AI Trustworthiness [223]. \\nHowever, despite the signifcant progress that AI and machine learning (ML) have made in \\na number of different application domains, these technologies are also vulnerable to attacks that can cause spectacular failures with dire consequences. \\nFor example in PredAI computer vision applications for object detection and classifcation, \\nwell-known cases of adversarial perturbations of input images have caused autonomous vehicles to swerve into the opposite direction lane. The misclassifcation of stop signs as speed limit signs caused critical objects to disappear from images, and even to misiden-tify people wearing glasses in high-security settings [99, 150, 260, 277]. Similarly, in the \\nmedical feld where more and more ML models are being deployed to assist doctors, there is the potential for medical record leaks from ML models that can expose deeply personal information [14, 135]. \\nIn GenAI, large language models (LLMs) [6, 38, 70, 83, 196, 209, 228, 276, 293, 294, 345] \\nare also becoming an integral part of the Internet infrastructure and software applications. LLMs are being used to create more powerful online search, help software developers write code, and even power chatbots that help with customer service. LLMs are being integrated with corporate databases and documents to enable powerful R\\nETRIEVAL AUGMENTED \\nGENERATION (RAG) [173] scenarios when LLMs are adapted to specifc domains and use \\ncases. These scenarios in effect expose a new attack surface to potentially confdential and proprietary enterprise data. \\nWith the exception of BLOOM [209] and LLaMA[293], most of the companies developing \\nsuch models do not release detailed information about the data sets that have been used to build their language models, but these data sets inevitably include some sensitive per -\\nsonal information, such as addresses, phone numbers, and email addresses. This creates serious risks for user privacy online. The more often a piece of information appears in a dataset, the more likely a model is to leak it in response to random or specifcally designed queries or prompts. This could perpetuate wrong and harmful associations with damag-ing consequences for the people involved and bring additional security and safety concerns [51, 201]. \\nAttackers can also manipulate the training data for both PredAI and GenAI systems, thus \\n3 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='22d23be5-c102-4210-bd6f-58d679e666ed', embedding=None, metadata={'page_label': '4', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nmaking the AI system trained on it vulnerable to attacks [256]. Scraping of training data \\nfrom the Internet also opens up the possibility of DATA POISONING at scale [46] by hackers \\nto create vulnerabilities that allow for security breaches down the pipeline. \\nAs ML models continue to grow in size, many organizations rely on pre-trained models \\nthat could either be used directly or be fne-tuned with new datasets to enable different tasks. This creates opportunities for malicious modifcations of pre-trained models by in-serting \\nTROJANS to enable attackers to compromise the model availability, force incorrect \\nprocessing, or leak the data when instructed [118]. \\nHistorically, modality-specifc AI technology has emerged for each input modality (e.g., \\ntext, images, speech, tabular data) in PredAI and GenAI systems, each of which is suscep-tible to domain-specifc attacks. For example, the attack approaches for image classifcation tasks do not directly translate to attacks against natural language processing (NLP) models. Recently, transformer architectures that are used extensively in NLP have showns to have applications in the computer vision domain [90]. In addition, multimodal ML has made ex-citing progress in many tasks, including generative and classifcation tasks, and there have been attempts to use multimodal learning as a potential mitigation of single-modality at-tacks [328]. However, powerful simultaneous attacks against all modalities in a multimodal model have also emerged [63, 261, 326]. \\nFundamentally, the machine learning methodology used in modern AI systems is suscepti-\\nble to attacks through the public APIs that expose the model, and against the platforms on which they are deployed. This report focuses on the former and considers the latter to be the scope of traditional cybersecurity taxonomies. For attacks against models, attackers can breach the confdentiality and privacy protections of the data and model by simply exercis-ing the public interfaces of the model and supplying data inputs that are within the accept-able range. In this sense, the challenges facing AML are similar to those facing cryptogra-phy. Modern cryptography relies on algorithms that are secure in an information-theoretic sense. Thus, people need to focus only on implementing them robustly and securely—no small task. Unlike cryptography, there are no information-theoretic security proofs for the widely used machine learning algorithms. Moreover, information-theoretic impossibility \\nresults have started to appear in the literature [102, 116] that set limits on the effectiveness \\nof widely-used mitigation techniques. As a result, many of the advances in developing mitigations against different classes of attacks tend to be empirical and limited in nature. \\nThis report offers guidance for the development of the following: \\n• Standardized terminology in AML to be used by the ML and cybersecurity commu-\\nnities; \\n• A taxonomy of the most widely studied and effective attacks in AML, including \\n– evasion, poisoning, and privacy attacks for PredAI systems, \\n– evasion, poisoning, privacy, and abuse attacks for GenAI systems; \\n4 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8fdee60d-cffa-4fda-8eaf-60a0fecf80a2', embedding=None, metadata={'page_label': '5', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n– attacks against all viable learning methods (e.g., supervised, unsupervised, semi-\\nsupervised, federated learning, reinforcement learning) across multiple data modalities. \\n• A discussion of potential mitigations in AML and limitations of some of the existing mitigation techniques. \\nAs ML is a fast evolving feld, we envision the need to update the report regularly as new developments emerge on both the attack and mitigation fronts. \\nThe goal of this report is not to provide an exhaustive survey of all literature on AML. In fact, this by itself is an almost impossible task as a search on arXiv for AML articles in 2021 and 2022 yielded more than 5000 references. Rather, this report provides a categorization of attacks and their mitigations for PredAI and GenAI systems, starting with the main types of attacks: 1) evasion, 2) data and model poisoning, 3) data and model privacy, and 4) abuse (GenAI only). \\nThis report is organized into three sections. In Section 2 we consider PredAI systems. \\nSection 2.1 introduces the taxonomy of attacks for PredAI systems. The taxonomy is orga-nized by frst defning the broad categories of attacker objectives/goals. Based on that, we defne the categories of capabilities the adversary must be able to leverage to achieve the corresponding objectives. Then, we introduce specifc attack classes for each type of capa-bility. Sections 2.2, 2.3, and 2.4 discuss the major classes of attacks: evasion, poisoning, \\nand privacy, respectively. A corresponding set of mitigations for each class of attacks is provided in the attack class sections. In Section 3 we consider GenAI systems. Section 3.1 introduces the taxonomy of attacks for GenAI systems. Similary to the PredAI case, we defne the categories of capabilities the adversary must be able to leverage to achieve the corresponding objectives with GenAI systems. Then, we introduce specifc attack classes for each type of capability. Section 4 discusses the remaining challenges in the feld. \\n5 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='58787b67-5f6b-432d-8a8b-449d56f12f1e', embedding=None, metadata={'page_label': '6', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n2. Predictive AI Taxonomy \\n2.1. Attack Classifcation \\nFigure \\n1 introduces a taxonomy of attacks in adversarial machine learning for PredAI sys-\\ntems. The attacker’s objectives are shown as disjointed circles with the attacker’s goal at the \\ncenter of each circle: Availability breakdown, Integrity violations, and Privacy compro-\\nmise. The capabilities that an adversary must leverage to achieve their objectives are shown in the outer layer of the objective circles. Attack classes are shown as callouts connected to the capabilities required to mount each attack. Multiple attack classes that requiring same capabilities for reaching the same objective are shown in a single callout. Related attack classes that require different capabilities for reaching the same objective are connected with dotted lines. \\n6 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='54447eed-058e-4ccb-8ec9-ec84a1b0f00a', embedding=None, metadata={'page_label': '7', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Integrity\\nPrivacyAvailability\\nReconstruction;\\nMembership Inference;\\nProperty Inference   ModelTest Data QueryTraining Data\\nQuery AccessModel Poisoning\\nEnergy-Latency\\nModel\\nPoisoning\\nLabelSource CodeBlack-Box\\nEvasion\\nTargeted\\nPoisoningBackdoor\\nPoisoningClean-Label\\nBackdoor\\nEvasionClean-Label\\nPoisoning\\nTraining Data\\nModel\\nDataControlControlControlModel\\nControl\\nLabel LimitControl\\nData Poisoning\\nQuery AccessQuery AccessModel Extraction Clean-Label\\nPoisoning\\nAccess LimitControlNIST AI 100-2e2023 \\nJanuary 2024 \\nFigure\\n 1. Taxonomy of attacks on Predictive AI systems. \\nThese attacks are classifed according to the following dimensions: 1) learning method and \\nstage of the learning process when the attack is mounted, 2) attacker goals and objectives, 3) attacker capabilities, and 4) attacker knowledge of the learning process. Several adversarial attack classifcation frameworks have been introduced in prior works [30, 283], and the goal here is to create a standard terminology for adversarial attacks on ML that unifes existing work. \\n7 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='0b097f43-00c9-4704-ae9b-23ecb5e58b31', embedding=None, metadata={'page_label': '8', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n2.1.1. Stages of Learning \\nMachine \\nlearning involves a TRAINING STAGE , in which a model is learned, and a DEPLOY -\\nMENT STAGE , in which the model is deployed on new, unlabeled data samples to generate \\npredictions. In the case of SUPERVISED LEARNING labeled training data is given as input \\nto a training algorithm in the training stage and the ML model is optimized to minimize a \\nspecifc loss function. Validation and testing of the ML model is usually performed before the model is deployed in the real world. Common supervised learning techniques include \\nCLASSIFICATION , in which the predicted labels or classes are discrete, and REGRESSION , \\nin which the predicted labels or response variables are continuous. \\nML models may be GENERATIVE (i.e., learn the distribution of training data and gener -\\nate similar examples, such as generative adversarial networks [GAN] and large language \\nmodels [LLM]), cf. Section 3, or DISCRIMINATIVE (i.e., learn only a decision boundary, \\nsuch as LOGISTIC REGRESSION , SUPPORT VECTOR MACHINES , and CONVOLUTIONAL \\nNEURAL NETWORKS ). Most PredAI models are DISCRIMINATIVE . \\nOther learning paradigms in the ML literature are UNSUPERVISED LEARNING , which trains \\nmodels using unlabeled data at training time; SEMI -SUPERVISED LEARNING , in which a \\nsmall set of examples have labels, while the majority of samples are unlabeled; REIN -\\nFORCEMENT LEARNING , in which an agent interacts with an environment and learns an \\noptimal policy to maximize its reward; FEDERATED LEARNING , in which a set of clients \\njointly train an ML model by communicating with a server, which performs an aggregation of model updates; \\nENSEMBLE LEARNING which is an approach in machine learning that \\nseeks better predictive performance by combining the predictions from multiple models. \\nAdversarial machine learning literature predominantly considers adversarial attacks against \\nAI systems that could occur at either the training stage or the ML deployment stage. During the ML training stage, the attacker might control part of the training data, their labels, the model parameters, or the code of ML algorithms, resulting in different types of poisoning attacks. During the ML deployment stage, the ML model is already trained, and the adver -\\nsary could mount evasion attacks to create integrity violations and change the ML model’s predictions, as well as privacy attacks to infer sensitive information about the training data or the ML model. \\nTraining-time attacks. Attacks during the ML training stage are called \\nPOISONING AT -\\nTACKS [28]. In a DATA POISONING attack [28, 124], an adversary controls a subset of the \\ntraining data by either inserting or modifying training samples. In a MODEL POISONING at-\\ntack [185], the adversary controls the model and its parameters. Data poisoning attacks are \\napplicable to all learning paradigms, while model poisoning attacks are most prevalent in federated learning [152], where clients send local model updates to the aggregating server, and in supply-chain attacks where malicious code may be added to the model by suppliers of model technology. \\nDeployment-time attacks. Two different types of attacks can be mounted at inference or \\n8 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b90f84c0-33ba-4395-bf6e-148a4eb09558', embedding=None, metadata={'page_label': '9', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\ndeployment time. First, evasion attacks modify testing samples to create ADVERSARIAL \\nEXAMPLES [26, 120, 287], which are similar to the original sample (according to certain \\ndistance metrics) but alter the model predictions to the attacker’s choices. Second, privacy \\nattacks, such as membership inference [269] and data reconstruction [89], are typically \\nmounted by attackers with query access to an ML model. They could be further divided into data privacy attacks and model privacy attacks. \\n2.1.2. Attacker Goals and Objectives \\nThe \\nattacker’s objectives are classifed along three dimensions according to the three main \\ntypes of security violations considered when analyzing the security of a system (i.e., avail-ability, integrity, confdentiality): availability breakdown, integrity violations, and privacy compromise. Correspondingly, \\nADVERSARIAL SUCCESS indicates achieving one or more \\nof these objectives. Figure 1 separates attacks into three disjointed circles according to \\ntheir objective, and the attacker’s objective is shown at the center of each circle. \\nAvailability Breakdown. An AVAILABILITY ATTACK is an indiscriminate attack against \\nML in which the attacker attempts to break down the performance of the model at de-\\nployment time. Availability attacks can be mounted via data poisoning, when the attacker controls a fraction of the training set; via model poisoning, when the attacker controls the model parameters; or as \\nENERGY -LATENCY ATTACKS via query access. Data poisoning \\navailability attacks have been proposed for S UPPORT VECTOR MACHINES [28], linear re-\\ngression [143], and even neural networks [190, 215], while model poisoning attacks have \\nbeen designed for neural networks [185] and federated learning [12]. Recently, ENERGY -\\nLATENCY ATTACKS that require only black-box access to the model have been developed \\nfor neural networks across many different tasks in computer vision and NLP [273]. \\nIntegrity Violations. An INTEGRITY ATTACK targets the integrity of an ML model’s out-\\nput, resulting in incorrect predictions performed by an ML model. An attacker can cause \\nan integrity violation by mounting an evasion attack at deployment time or a poisoning at-tack at training time. Evasion attacks require the modifcation of testing samples to create adversarial examples that are mis-classifed by the model to a different class, while remain-ing stealthy and imperceptible to humans [26, 120, 287]. Integrity attacks via poisoning \\ncan be classifed as \\nTARGETED POISONING ATTACKS [113, 258], BACKDOOR POISONING \\nATTACKS [124], and MODEL POISONING [12, 24, 101]. Targeted poisoning tries to vio-\\nlate the integrity of a few targeted samples and assumes that the attacker has training data control to insert the poisoned samples. Backdoor poisoning attacks require the generation of a \\nBACKDOOR PATTERN , which is added to both the poisoned samples and the testing \\nsamples to cause misclassifcation. Backdoor attacks are the only attacks in the literature that require both training and testing data control. Model poisoning attacks could result in either targeted or backdoor attacks, and the attacker modifes model parameters to cause an integrity violation. They have been designed for centralized learning [185] and federated \\nlearning [12, 24]. \\n9 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d8565fca-bf93-4d29-a721-9f9bdebd0684', embedding=None, metadata={'page_label': '10', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nPrivacy Compromise. Attackers might be interested in learning information about the \\ntraining data (resulting in DATA PRIVACY attacks) or about the ML model (resulting in \\nMODEL PRIVACY attacks). The attacker could have different objectives for compromising \\nthe privacy of training data, such as DATA RECONSTRUCTION [89] (inferring content or \\nfeatures of training data), MEMBERSHIP -INFERENCE ATTACKS [130, 270] (inferring the \\npresence of data in the training set), data EXTRACTION [48, 51] (ability to extract training \\ndata from generative models), and PROPERTY INFERENCE [110] (inferring properties about \\nthe training data distribution). MODEL EXTRACTION is a model privacy attack in which \\nattackers aim to extract information about the model [141]. \\n2.1.3. Attacker Capabilities \\nAn \\nadversary might leverage six types of capabilities to achieve their objectives, as shown \\nin the outer layer of the objective circles in Figure 1: \\n• TRAINING DATA CONTROL : The attacker might take control of a subset of the train-\\ning data by inserting or modifying training samples. This capability is used in data \\npoisoning attacks (e.g., availability poisoning, targeted or backdoor poisoning). \\n• MODEL CONTROL : The attacker might take control of the model parameters by either \\ngenerating a Trojan trigger and inserting it in the model or by sending malicious local model updates in federated learning. \\n• \\nTESTING DATA CONTROL : The attacker may utilize this to add perturbations to test-\\ning samples at model deployment time, as performed in evasion attacks to generate adversarial examples or in backdoor poisoning attacks. \\n• \\nLABEL LIMIT : This capability is relevant to restrict the adversarial control over the \\nlabels of training samples in supervised learning. Clean-label poisoning attacks as-sume that the attacker does not control the label of the poisoned samples – a realistic poisoning scenario, while regular poisoning attacks assume label control over the poisoned samples. \\n• \\nSOURCE CODE CONTROL : The attacker might modify the source code of the ML \\nalgorithm, such as the random number generator or any third-party libraries, which are often open source. \\n• \\nQUERY ACCESS : When the ML model is managed by a cloud provider (using Ma-\\nchine Learning as a Service – MLaaS), the attacker might submit queries to the model and receive predictions (either labels or model confdences). This capability is used by black-box evasion attacks, \\nENERGY -LATENCY ATTACKS , and all privacy attacks. \\nNote that even if an attacker does not have the ability to modify training/testing data, source code, or model parameters, access to these are still crucial for mounting white-box attacks. See Section 2.1.4 for more details on attacker knowledge. \\n10 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2264b7d3-5dc1-4bfa-8665-4d1e0589b564', embedding=None, metadata={'page_label': '11', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nFigure 1 connects each attack class with the capabilities required to mount the attack. For \\ninstance, backdoor attacks that cause integrity violations require control of training data and \\ntesting data to insert the backdoor pattern. Backdoor attacks can also be mounted via source code control, particularly when training is outsourced to a more powerful entity. Clean-label backdoor attacks do not allow label control on the poisoned samples, in addition to the capabilities needed for backdoor attacks. \\n2.1.4. Attacker Knowledge \\nAnother \\ndimension for attack classifcation is how much knowledge the attacker has about \\nthe ML system. There are three main types of attacks: white-box, black-box, and gray-box. \\nWhite-box attacks. These assume that the attacker operates with full knowledge about the \\nML system, including the training data, model architecture, and model hyper-parameters. \\nWhile these attacks operate under very strong assumptions, the main reason for analyzing them is to test the vulnerability of a system against worst-case adversaries and to evaluate potential mitigations. Note that this defnition is more general and encompasses the notion of adaptive attacks where the knowledge of the mitigations applied to the model or the system is explicitly tracked. \\nBlack-box attacks. These attacks assume minimal knowledge about the ML system. An \\nadversary might get query access to the model, but they have no other information about \\nhow the model is trained. These attacks are the most practical since they assume that the attacker has no knowledge of the AI system and utilize system interfaces readily available for normal use. \\nGray-box attacks. There are a range of gray-box attacks that capture adversarial knowl-\\nedge between black-box and white-box attacks. Suciu et al. [283] introduced a framework \\nto classify gray-box attacks. An attacker might know the model architecture but not its pa-\\nrameters, or the attacker might know the model and its parameters but not the training data. Other common assumptions for gray-box attacks are that the attacker has access to data distributed identically to the training data and knows the feature representation. The latter assumption is important in applications where feature extraction is used before training an ML model, such as cybersecurity, fnance, and healthcare. \\n2.1.5. Data Modality \\nAdversarial attacks against ML have been discovered in a range of data modalities used in \\nmany application domains. Until recently, most attacks and defenses have operated under a single modality, but a new ML trend is to use multimodal data. The taxonomy of attacks defned in Figure 1 is independent of the modality of the data in specifc applications. \\nThe most common data modalities in the adversarial ML literature include: \\n1. Image: Adversarial examples of image data modality [120, 287] have the advantage \\n11 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='72b2c215-65a9-417f-a09a-96b38c2ded84', embedding=None, metadata={'page_label': '12', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nof a continuous domain, and gradient-based methods can be applied directly for opti-\\nmization. Backdoor poisoning attacks were frst invented for images [124], and many privacy attacks are run on image datasets (e.g., [269]). The image modality includes \\nother types of imaging (e.g., LIDAR, SAR, IR, ‘hyperspectral’). \\n2. Text: Natural language processing (NLP) is a popular modality, and all classes of \\nattacks have been proposed for NLP applications, including evasion [126], poison-\\ning [68, 175], and privacy [337]. \\n3. Audio: Audio systems and text generated from audio signals have also been at-\\ntacked [54]. \\n4. Video: Video comprehension models have shown increasing capabilities on vision-\\nand-language tasks [339], but such models are also vulnerable to attacks [318]. \\n5. Cybersecurity\\n2\\n2Strictly speaking, cybersecurity data may not include a single modality, but rather multiple modalities such \\nas network-level, host-level, or program-level data. : The frst poisoning attacks were discovered in cybersecurity for \\nworm signature generation (2006) [236] and spam email classifcation (2008) [222]. \\nSince then, poisoning attacks have been shown for malware classifcation, malicious PDF detection, and Android malicious app classifcation [257]. Evasion attacks \\nagainst the same data modalities have been proposed as well: malware classifca-tion [84, 282], PDF malware classifcation [279, 325], and Android malicious app \\ndetection [239]. Clements et al. [78] developed a mechanism for effective generation of evasion attacks on small, weak routers in network intrusion detection. Poison-ing unsupervised learning models has been shown for clustering used in malware classifcation [29] and network traffc anomaly detection [249]. \\nIndustrial Control Systems (ICS) and Supervisory Control and Data Acquisition \\n(SCADA) systems are part of modern Critical Infrastructure (CI) such as power grids, power plants (nuclear, fossil fuel, renewable energy), water treatment plants, oil re-fneries, etc. ICS are an attractive target for adversaries because of the potential for highly consequential disruptions of CI [55, 167]. The existence of targeted stealth \\nattacks has led to the development of defense-in-depth mechanisms for their detec-tion and mitigation. Anomaly detection based on data-centric approaches allows automated feature learning through ML algorithms. However, the application of ML to such problems comes with specifc challenges related to the need for a very low false negative and low false positive rates, ability to catch zero-day attacks, account for plant operational drift, etc. This challenge is compounded by the fact that try-ing to accommodate all these together makes ML models susceptible to adversarial attacks [161, 243, 353]. \\n6. Tabular data: Numerous attacks against ML models working on tabular data in f-\\nnance, business, and healthcare applications have been demonstrated. For example, poisoning availability attacks have been shown against healthcare and business ap-\\n12 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='21a95f34-8f27-4afc-b4df-b024a719820d', embedding=None, metadata={'page_label': '13', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nplications [143]; privacy attacks have been shown against healthcare data [333]; and \\nevasion attacks have been shown against fnancial applications [117]. \\nRecently, the use of ML models trained on multimodal data has gained traction, particu-\\nlarly the combination of image and text data modalities. Several papers have shown that multimodal models may provide some resilience against attacks [328], but other papers \\nshow that multimodal models themselves could be vulnerable to attacks mounted on all modalities at the same time [63, 261, 326]. See Section 4.6 for additional discussion. \\nAn interesting open challenge is to test and characterize the resilience of a variety of multimodal ML against evasion, poisoning, and privacy attacks. \\n13 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='0c66930e-46d9-42e8-ace7-316ef6d7a4db', embedding=None, metadata={'page_label': '14', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n2.2. Evasion Attacks and Mitigations \\nThe \\ndiscovery of evasion attacks against machine learning models has generated increased \\ninterest in adversarial machine learning, leading to signifcant growth in this research space \\nover the last decade. In an evasion attack, the adversary’s goal is to generate adversar -\\nial examples, which are defned as testing samples whose classifcation can be changed at deployment time to an arbitrary class of the attacker’s choice with only minimal pertur -\\nbation [287]. Early known instances of evasion attacks date back to 1988 with the work \\nof Kearns and Li [155], and to 2004, when Dalvi et al. [82], and Lowd and Meek [188] \\ndemonstrated the existence of adversarial examples for linear classifers used in spam fl-ters. Adversarial examples became even more intriguing to the research community when Szedegy et al. [287] showed that deep neural networks used for image classifcation can \\nbe easily manipulated, and adversarial examples were visualized. In the context of image classifcation, the perturbation of the original sample must be small so that a human cannot observe the transformation of the input. Therefore, while the ML model can be tricked to classify the adversarial example in the target class selected by the attacker, humans still recognize it as part of the original class. \\nIn 2013, Szedegy et al. [287] and Biggio et al. [26] independently discovered an effective \\nmethod for generating adversarial examples against linear models and neural networks by \\napplying gradient optimization to an adversarial objective function. Both of these tech-niques require white-box access to the model and were improved by subsequent methods that generated adversarial examples with even smaller perturbations [10, 53, 194]. Adver -\\nsarial examples are also applicable in more realistic black-box settings in which attackers only obtain query access capabilities to the trained model. Even in the more challeng-ing black-box setting in which attackers obtain the model’s predicted labels or confdence scores, deep neural networks are still vulnerable to adversarial examples. Methods for cre-ating adversarial examples in black-box settings include zeroth-order optimization [66], \\ndiscrete optimization [210], and Bayesian optimization [271], as well as transferability, \\nwhich involves the white-box generation of adversarial examples on a different model ar -\\nchitecture before transferring them to the target model [232, 233, 299]. Cybersecurity and image classifcations were the frst application domains that showcased evasion attacks. However, with the increasing interest in adversarial machine learning, ML technology used in many other application domains went under scrutiny, including speech recognition [54], natural language processing [149], and video classifcation [177, 317]. \\nMitigating adversarial examples is a well-known challenge in the community and deserves \\nadditional research and investigation. The feld has a history of publishing defenses evalu-ated under relatively weak adversarial models that are subsequently broken by more power -\\nful attacks, a process that appears to iterate in perpetuity. Mitigations need to be evaluated against strong adaptive attacks, and guidelines for the rigorous evaluation of newly pro-posed mitigation techniques have been established [81, 297]. The most promising direc-\\ntions for mitigating the critical threat of evasion attacks are adversarial training [120, 194] \\n(iteratively generating and inserting adversarial examples with their correct labels at train-\\n14 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5c4d3d05-3b2a-46a6-af17-2d3fa5970625', embedding=None, metadata={'page_label': '15', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\ning time); certifed techniques, such as randomized smoothing [79] (evaluating ML predic-\\ntion under noise); and formal verifcation techniques [112, 154] (applying formal method \\ntechniques to verify the model’s output). Nevertheless, these methods come with different limitations, such as decreased accuracy for adversarial training and randomized smoothing, and computational complexity for formal methods. There is an inherent trade-off between robustness and accuracy [296, 301, 342]. Similarly, there are trade-offs between a model’s \\nrobustness and fairness guarantees [59]. \\nThis section discusses white-box and black-box evasion attack techniques, attack transfer -\\nability, and the potential mitigation of adversarial examples in more detail. \\n2.2.1. White-Box Evasion Attacks \\nThere \\nare several optimization-based methods for designing evasion attacks that generate \\nadversarial examples at small distances from the original testing samples. There are also \\nseveral choices for distance metrics, universal evasion attacks, and physically realizable attacks, as well as examples of evasion attacks developed for multiple data modalities, including NLP, audio, video, and cybersecurity domains. \\nOptimization-based methods. Szedegy et al. [287] and Biggio et al. [26] independently \\nproposed the use of optimization techniques to generate adversarial examples. In their \\nthreat models, the adversary is allowed to inspect the entirety of the ML model and com-pute gradients relative to the model’s loss function. These attacks can be targeted, in which the adversarial example’s class is selected by the attacker, or untargeted, in which the ad-versarial examples are misclassifed to any other incorrect class. \\nSzedegy et al. [287] coined the widely used term adversarial examples. They considered \\nan objective that minimized the ℓ\\n2 norm of the perturbation, subject to the model predic-\\ntion changing to the target class. The optimization is solved using the Limited-memory \\nBroyden–Fletcher–Goldfarb–Shanno (L-BFGS) method. Biggio et al. [26] considered the \\nsetting of a binary classifer with malicious and benign classes with continuous and dif-ferentiable discriminant function. The objective of the optimization is to minimize the discriminant function in order to generate adversarial examples of maximum confdence. \\nWhile Biggio et al. [26] apply their method to linear classifers, kernel SVM, and multi-\\nlayer perceptrons, Szedegy et al. [287] show the existence of adversarial examples on deep \\nlearning models used for image classifcation. Goodfellow et al. [120] introduced an ef-\\nfcient method for generating adversarial examples for deep learning: the Fast Gradient Sign Method (FGSM), which performs a single iteration of gradient descent for solving the optimization. This method has been extended to an iterative FGSM attack by Kurakin et al. [163]. \\nSubsequent work on generating adversarial examples have proposed new objectives and \\nmethods for optimizing the generation of adversarial examples with the goals of minimizing the perturbations and supporting multiple distance metrics. Some notable attacks include: \\n15 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='18c5f8a9-b940-47d2-aa34-ec4f80c90c33', embedding=None, metadata={'page_label': '16', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n1. DeepFool is an untargeted evasion attack for ℓ2 norms, which uses a linear approxi-\\nmation of the neural network to construct the adversarial examples [212]. \\n2. The Carlini-Wagner attack uses multiple objectives that minimize the loss or logits \\non the target class and the distance between the adversarial example and original sample. The attack is optimized via the penalty method [53] and considers three \\ndistance metrics to measure the perturbations of adversarial examples: ℓ\\n0, ℓ2, and ℓ∞. \\nThe attack has been effective against the defensive distillation defense [234]. \\n3. The Projected Gradient Descent (PGD) attack [194] minimizes the loss function and projects the adversarial examples to the space of allowed perturbations at each iter -\\nation of gradient descent. PGD can be applied to the ℓ\\n2 and ℓ∞ distance metrics for \\nmeasuring the perturbation of adversarial examples. \\nUniversal evasion attacks. Moosavi-Dezfooli et al. [211] showed how to construct small \\nuniversal perturbations (with respect to some norm), which can be added to most images and induce a misclassifcation. Their technique relies on successive optimization of the universal perturbation using a set of points sampled from the data distribution. This is a form of F\\nUNCTIONAL ATTACKS . An interesting observation is that the universal pertur -\\nbations generalize across deep network architectures, suggesting similarity in the decision boundaries trained by different models for the same task. \\nPhysically realizable attacks. These are attacks against machine learning systems that \\nbecome feasible in the physical world [11, 163, 189]. One of the frst physically realizable \\nattacks in the literature is the attack on facial recognition systems by Sharif et al. [260]. \\nThe attack can be realized by printing a pair of eyeglass frames, which misleads facial \\nrecognition systems to either evade detection or impersonate another individual. Eykholt et al. [100] proposed an attack to generate robust perturbations under different conditions, \\nresulting in adversarial examples that can evade vision classifers in various physical en-vironments. The attack is applied to evade a road sign detection classifer by physically applying black and white stickers to the road signs. \\nThe ShapeShifter [67] attack is designed to evade object detectors, which is a more chal-\\nlenging problem than attacking image classifers since the attacker needs to evade the clas-\\nsifcation in multiple bounding boxes with different scales. In addition, this attack requires the perturbation to be robust enough to survive real-world distortions due to different view-ing distances and angles, lighting conditions, and camera limitations. \\nOther data modalities. In computer vision applications, adversarial examples must be im-\\nperceptible to humans. Therefore, the perturbations introduced by attackers need to be so \\nsmall that a human correctly recognizes the images, while the ML classifer is tricked into changing its prediction. Alternatively, there may be a trigger object in the image that is still imperceptible to humans but causes the model to misclassify. The concept of adversarial examples has been extended to other domains, such as audio, video, natural language pro-cessing (NLP), and cybersecurity. In some of these settings, there are additional constraints \\n16 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='618cf6b4-bf88-43e5-819c-765f4dc6bda2', embedding=None, metadata={'page_label': '17', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nthat need to be respected by adversarial examples, such as text semantics in NLP and the \\napplication constraints in cybersecurity. Several representative works are discussed below: \\n• Audio: Carlini and Wagner [54] showed a targeted attack on models that generate \\ntext from speech. They can generate an audio waveform that is very similar to an existing one but that can be transcribed to any text of the attacker’s choice. \\n• Video: Adversarial evasion attacks against video classifcation models can be split \\ninto sparse attacks that perturb a small number of video frames [317] and dense \\nattacks that perturb all of the frames in a video [177]. The goal of the attacker is to \\nchange the classifcation label of the video. \\n• NLP: Jia and Liang [149] developed a methodology for generating adversarial NLP \\nexamples. This pioneering work was followed by many advances in developing ad-versarial attacks on NLP models (see a comprehensive survey on the topic [347]). \\nRecently, La Malfa and Kwiatkowska [164] proposed a method for formalizing per -\\nturbation defnitions in NLP by introducing the concept of semantic robustness. The main challenges in NLP are that the domain is discrete rather than continuous (e.g., image, audio, and video classifcation), and adversarial examples need to respect text semantics. \\n• Cybersecurity: In cybersecurity applications, adversarial examples must respect the \\nconstraints imposed by the application semantics and feature representation of cyber data, such as network traffc or program binaries. FENCE is a general framework for crafting white-box evasion attacks using gradient optimization in discrete domains and supports a range of linear and statistical feature dependencies [73]. FENCE \\nhas been applied to two network security applications: malicious domain detection and malicious network traffc classifcation. Sheatsley et al. [262] propose a method \\nthat learns the constraints in feature space using formal logic and crafts adversar -\\nial examples by projecting them onto a constraint-compliant space. They apply the technique to network intrusion detection and phishing classifers. Both papers ob-serve that attacks from continuous domains cannot be readily applied in constrained environments, as they result in infeasible adversarial examples. Pierazzi et al. [239] \\ndiscuss the diffculty of mounting feasible evasion attacks in cyber security due to constraints in feature space and the challenge of mapping attacks from feature space to problem space. They formalize evasion attacks in problem space and construct feasible adversarial examples for Android malware. \\n2.2.2. Black-Box Evasion Attacks \\nBlack-box \\nevasion attacks are designed under a realistic adversarial model, in which the \\nattacker has no prior knowledge of the model architecture or training data. Instead, the adversary can interact with a trained ML model by querying it on various data samples and obtaining the model’s predictions. Similar APIs are provided by machine learning as a ser -\\n17 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ff980894-d74f-4ebc-86b8-f7bdd733ec59', embedding=None, metadata={'page_label': '18', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nvice (MLaaS) offered by public cloud providers, in which users can obtain the model’s pre-\\ndictions on selected queries without information about how the model was trained. There are two main classes of black-box evasion attacks in the literature: \\n• Score-based attacks: In this setting, attackers obtain the model’s confdence scores \\nor logits and can use various optimization techniques to create the adversarial exam-ples. A popular method is zeroth-order optimization, which estimates the model’s gradients without explicitly computing derivatives [66, 137]. Other optimization \\ntechniques include discrete optimization [210], natural evolution strategies [136], \\nand random walks [216]. \\n• Decision-based attacks: In this more restrictive setting, attackers obtain only the \\nfnal predicted labels of the model. The frst method for generating evasion attacks was the Boundary Attack based on random walks along the decision boundary and rejection sampling [35], which was extended with an improved gradient estimation to reduce the number of queries in the HopSkipJumpAttack [65]. More recently, several optimization methods search for the direction of the nearest decision boundary (the OPT attack [71]), use sign SGD instead of binary searches (the Sign-OPT attack \\n[72]), or use Bayesian optimization [271]. \\nThe main challenge in creating adversarial examples in black-box settings is re-ducing the number of queries to the ML models. Recent techniques can success-fully evade the ML classifers with a relatively small number of queries, typically less than 1000 [271]. \\n2.2.3. Transferability of Attacks \\nAnother \\nmethod for generating adversarial attacks under restrictive threat models is via \\ntransferability of an attack crafted on a different ML model. Typically, an attacker trains a substitute ML model, generates white-box adversarial attacks on the substitute model, and transfers the attacks to the target model. Various methods differ in how the substitute models are trained. For example, Papernot et al. [232, 233] train the substitute model with \\nscore-based queries to the target model, while several papers train an ensemble of models without explicitly querying the target model [181, 299, 315]. \\nAttack transferability is an intriguing phenomenon, and existing literature attempts to un-\\nderstand the fundamental reasons why adversarial examples transfer across models. Several papers have observed that different models learn intersecting decision boundaries in both benign and adversarial dimensions, which leads to better transferability [120, 211, 299]. \\nDemontis et al. [85] identifed two main factors that contribute to attack transferability for \\nboth evasion and poisoning: the intrinsic adversarial vulnerability of the target model and the complexity of the surrogate model used to optimize the attack. \\nE\\nXPECTATION OVER TRANSFORMATION aims to make adversarial examples sustain im-\\n18 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='910b2d5a-7b34-40fb-8199-7272d5e4c5ca', embedding=None, metadata={'page_label': '19', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nage transformations that occur in the real world, such as angle and viewpoint changes [11]. \\n2.2.4. Mitigations \\nMitig\\nating evasion attacks is challenging because adversarial examples are widespread in \\na variety of ML model architectures and application domains, as discussed above. Pos-\\nsible explanations for the existence of adversarial examples are that ML models rely on non-robust features that are not aligned with human perception in the computer vision do-main [138]. In the last few years, many of the proposed mitigations against adversarial \\nexamples have been ineffective against stronger attacks. Furthermore, several papers have performed extensive evaluations and defeated a large number of proposed mitigations: \\n• Carlini and Wagner showed how to bypass 10 methods for detecting adversarial ex-amples and described several guidelines for evaluating defenses [52]. Recent work \\nshows that detecting adversarial examples is as diffcult as building a defense [295]. \\nTherefore, this direction for mitigating adversarial examples is similarly challenging when designing defenses. \\n• The Obfuscated Gradients attack [10] was specifcally designed to defeat several pro-posed defenses that mask the gradients using the ℓ\\n0 and ℓ∞ distance metrics. It relies \\non a new technique, Backward Pass Differentiable Approximation, which approxi-mates the gradient during the backward pass of backpropagation. It bypasses seven proposed defenses. \\n• Tramer ` et al. [297] described a methodology for designing adaptive attacks against \\nproposed defenses and circumvented 13 existing defenses. They advocate design-ing adaptive attacks to test newly proposed defenses rather than merely testing the defenses against well-known attacks. \\nFrom the wide range of proposed defenses against adversarial evasion attacks, three main classes have proved resilient and have the potential to provide mitigation against evasion attacks: \\n1. Adversarial training: Introduced by Goodfellow et al. [120] and further developed \\nby Madry et al. [194], adversarial training is a general method that augments the \\ntraining data with adversarial examples generated iteratively during training using their correct labels. The stronger the adversarial attacks for generating adversarial examples are, the more resilient the trained model becomes. Interestingly, adversarial training results in models with more semantic meaning than standard models [301], \\nbut this beneft usually comes at the cost of decreased model accuracy on clean data. Additionally, adversarial training is expensive due to the iterative generation of ad-versarial examples during training. \\n2. Randomized smoothing: Proposed by Lecuyer et al. [169] and further improved by \\nCohen et al. [79], randomized smoothing is a method that transforms any classifer \\n19 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2ebefa28-9ab7-4ba2-9893-177a741cd192', embedding=None, metadata={'page_label': '20', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\ninto a certifable robust smooth classifer by producing the most likely predictions \\nunder Gaussian noise perturbations. This method results in provable robustness for ℓ2 \\nevasion attacks, even for classifers trained on large-scale datasets, such as ImageNet. Randomized smoothing typically provides certifed prediction to a subset of testing samples (the exact number depends on the radius of the ℓ\\n2 ball and the characteristics \\nof the training data and model). Recent results have extended the notion of certifed adversarial robustness to ℓ\\n2-norm bounded perturbations by combining a pretrained \\ndenoising diffusion probabilistic model and a standard high-accuracy classifer [50]. \\n3. Formal verifcation: Another method for certifying the adversarial robustness of \\na neural network is based on techniques from FORMAL METHODS . Reluplex uses \\nsatisfability modulo theories (SMT) solvers to verify the robustness of small feed-forward neural networks [154]. AI\\n2 is the frst verifcation method applicable to \\nconvolutional neural networks using abstract interpretation techniques [112]. These \\nmethods have been extended and scaled up to larger networks in follow-up verifca-tion systems, such as DeepPoly [274], ReluVal [313], and Fast Geometric Projections (FGP) [108]. Formal verifcation techniques have signifcant potential for certifying \\nneural network robustness, but their main limitations are their lack of scalability, computational cost, and restriction in the type of supported operations. \\nAll of these proposed mitigations exhibit inherent trade-offs between robustness and accu-racy, and they come with additional computational costs during training. Therefore, design-ing ML models that resist evasion while maintaining accuracy remains an open problem. \\n20 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a3e3e418-97c3-4754-95fc-618543d72cd9', embedding=None, metadata={'page_label': '21', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n2.3. Poisoning Attacks and Mitigations \\nAnother \\nrelevant threat against machine learning systems is the risk of adversaries mount-\\ning poisoning attacks, which are broadly defned as adversarial attacks during the training \\nstage of the ML algorithm. Poisoning attacks have a long history in cybersecurity, as the frst known poisoning attack was developed for worm signature generation in 2006 [236]. \\nSince then, poisoning attacks have been studied extensively in several application domains: computer security (for spam detection [222]), network intrusion detection [305], vulnera-bility prediction [251], malware classifcation [257, 323]), computer vision [113, 124, 258], natural language processing [68, 175, 309], and tabular data in healthcare and fnancial do-mains [143]. Recently, poisoning attacks have gained more attention in industrial applica-\\ntions as well. A Microsoft report revealed that they are considered to be the most critical vulnerability of machine learning systems deployed in production [162]. Recently. it has \\nbeen shown how poisoning could be orchestrated at scale so that an adversary with limited fnancial resources can control a fraction of public datasets used for model training [46]. \\nPoisoning attacks are very powerful and can cause either an availability violation or an \\nintegrity violation. In particular, availability poisoning attacks cause indiscriminate degra-dation of the machine learning model on all samples, while targeted and backdoor poison-ing attacks are stealthier and induce integrity violations on a small set of target samples. Poisoning attacks leverage a wide range of adversarial capabilities, such as data poisoning, model poisoning, label control, source code control, and test data control, resulting in sev-eral subcategories of poisoning attacks. They have been developed in white-box adversarial scenarios [28, 143, 323], gray-box settings [143], and black-box models [27]. This section discusses the threat of availability poisoning, targeted poisoning, backdoor poisoning, and model poisoning attacks classifed according to their adversarial objective. For each poi-soning attack category, techniques for mounting the attacks as well as existing mitigations and their limitations are also discussed. Our classifcation of poisoning attacks is inspired by the framework developed by Cin `a et al. [76], which includes additional references to \\npoisoning attacks and mitigations. \\n2.3.1. Availability Poisoning \\nThe \\nfrst poisoning attacks discovered in cybersecurity applications were availability at-\\ntacks against worm signature generation and spam classifers, which indiscriminately im-pact the entire machine learning model and, in essence, cause a denial-of-service attack on users of the AI system. Perdisci et al. [236] generated suspicious fows with fake in-\\nvariants that mislead the worm signature generation algorithm in Polygraph [224]. Nelson \\net al. [222] designed poisoning attacks against Bayes-based spam classifers, which gen-\\nerate spam emails that contain long sequences of words appearing in legitimate emails to induce the misclassifcation of spam emails. Both of these attacks were conducted under the white-box setting in which adversaries are aware of the ML training algorithm, feature representations, training datasets, and ML models. ML-based methods have been proposed \\n21 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='75e49a89-50b6-4bad-a8d5-e76e079c4fb2', embedding=None, metadata={'page_label': '22', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nfor the detection of cybersecurity attacks targeting ICS. Such detectors are often retrained \\nusing data collected during system operation to account for plant operational drift of the monitored signals. This retraining procedure creates opportunities for an attacker to mimic the signals of corrupted sensors at training time and poison the learning process of the detector such that attacks remain undetected at deployment time [161]. \\nA simple black-box poisoning attack strategy is \\nLABEL FLIPPING , which generates train-\\ning examples with a victim label selected by the adversary [27]. This method requires a \\nlarge percentage of poisoning samples for mounting an availability attack, and it has been \\nimproved via optimization-based poisoning attacks introduced for the frst time against \\nSUPPORT VECTOR MACHINES (SVM) [28]. In this approach, the attacker solves a bilevel \\noptimization problem to determine the optimal poisoning samples that will achieve the adversarial objective (i.e., maximize the hinge loss for SVM [28] or maximize the mean \\nsquare error [MSE] for regression [143]). These optimization-based poisoning attacks have been subsequently designed against linear regression [143] and neural networks [215], and they require white-box access to the model and training data. In gray-box adversarial set-tings, the most popular method for generating availability poisoning attacks is transferabil-ity, in which poisoning samples are generated for a surrogate model and transferred to the target model [85, 283]. \\nA realistic threat model for supervised learning is that of clean-label poisoning attacks in \\nwhich adversaries can only control the training examples but not their labels. This case models scenarios in which the labeling process is external to the training algorithm, as in malware classifcation where binary fles can be submitted by attackers to threat intel-ligence platforms, and labeling is performed using anti-virus signatures or other external methods. Clean-label availability attacks have been introduced for neural network clas-sifers by training a generative model and adding noise to training samples to maximize the adversarial objective [105]. A different approach for clean-label poisoning is to use \\ngradient alignment and minimally modify the training data [106]. \\nAvailability poisoning attacks have also been designed for unsupervised learning against centroid-based anomaly detection [159] and behavioral clustering for malware [29]. In \\nfederated learning, an adversary can mount a model poisoning attack to induce availability violations in the globally trained model [101, 263, 264]. More details on model poisoning \\nattacks are provided in Section 2.3.4. \\nMitigations. Availability poisoning attacks are usually detectable by monitoring the stan-\\ndard performance metrics of ML models – such as precision, recall, accuracy, F1 scores, \\nand area under the curve – as they cause a large degradation in the classifer metrics. Nev-ertheless, detecting these attacks during the testing or deployment stages of ML is less desirable, and existing mitigations aim to proactively prevent these attacks during the train-ing stage to generate robust ML models. Among the existing mitigations, some generally promising techniques include: \\n• Training data sanitization: These methods leverage the insight that poisoned sam-\\n22 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='68044358-7ae1-4435-837e-13d629e49b02', embedding=None, metadata={'page_label': '23', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nples are typically different than regular training samples not controlled by adver -\\nsaries. As such, data sanitization techniques are designed to clean the training set \\nand remove the poisoned samples before the machine learning training is performed. Nelson et al. [222] propose the Region of Non-Interest (RONI) method, which ex-\\namines each sample and excludes it from training if the accuracy of the model de-creases when the sample is added. Subsequently proposed sanitization methods im-proved upon this early approach by reducing its computational complexity. Paudice et al. [235] introduced a method for label cleaning that was specifcally designed \\nfor label fipping attacks. Steinhardt et al. [280] propose the use of outlier detection \\nmethods for identifying poisoned samples. Clustering methods have also been used for detecting poisoned samples [165, 288]. In the context of network intrusion de-\\ntection, computing the variance of predictions made by an ensemble of multiple ML models has proven to be an effective data sanitization method [305]. Once sanitized, \\nthe datasets should be protected by cybersecurity mechanisms for provenance and integrity attestation [220]. \\n• Robust training: An alternative approach to mitigating availability poisoning at-\\ntacks is to modify the ML training algorithm and perform robust training instead of regular training. The defender can train an ensemble of multiple models and generate predictions via model voting [25, 172, 314]. Several papers apply techniques from \\nrobust optimization, such as using a trimmed loss function [88, 143]. Rosenfeld et \\nal. [248] proposed the use of randomized smoothing for adding noise during training and obtaining certifcation against label fipping attacks. \\n2.3.2. Targeted Poisoning \\nIn \\ncontrast to availability attacks, targeted poisoning attacks induce a change in the ML \\nmodel’s prediction on a small number of targeted samples. If the adversary can control the labeling function of the training data, then label fipping is an effective targeted poisoning attack. The adversary simply inserts several poisoned samples with the target label, and the model will learn the wrong label. Therefore, targeted poisoning attacks are mostly studied in the clean-label setting in which the attacker does not have access to the labeling function. \\nSeveral techniques for mounting clean-label targeted attacks have been proposed. Koh and \\nLiang [160] showed how infuence functions – a statistical method that determines the most \\ninfuential training samples for a prediction – can be leveraged for creating poisoned sam-ples in the fne-tuning setting in which a pre-trained model is fne-tuned on new data. Suciu et al. [283] designed StingRay, a targeted poisoning attack that modifes samples in feature space and adds poisoned samples to each mini batch of training. An optimization proce-dure based on feature collision was crafted by Shafahi et al. [258] to generate clean-label \\ntargeted poisoning for fne-tuning and end-to-end learning. ConvexPolytope [352] and \\nBullseyePolytope [2] optimized the poisoning samples against ensemble models, which \\noffers better advantages for attack transferability. MetaPoison [133] uses a meta-learning \\n23 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='113890af-f9ba-4c08-b381-2181ca7387ff', embedding=None, metadata={'page_label': '24', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nalgorithm to optimize the poisoned samples, while Witches’ Brew [113] performs opti-\\nmization by gradient alignment, resulting in a state-of-the-art targeted poisoning attack. \\nAll of the above attacks impact a small set of targeted samples that are selected by the \\nattacker during training, and they have only been tested for continuous image datasets (with the exception of StingRay, which requires adversarial control of a large fraction of the training set). Subpopulation poisoning attacks [144] were designed to poison samples from an entire subpopulation, defned by matching on a subset of features or creating clusters in representation space. Poisoned samples are generated using label fipping (for NLP and tabular modalities) or a frst-order optimization method (for continuous data, such as images). The attack generalizes to all samples in a subpopulation and requires minimal knowledge about the ML model and a small number of poisoned samples (proportional to the subpopulation size). \\nTargeted poisoning attacks have also been introduced for semi-supervised learning algo-\\nrithms [42], such as MixMatch [22], FixMatch [275], and Unsupervised Data Augmenta-\\ntion (UDA) [324] in which the adversary poisons a small fraction of the unlabeled training \\ndataset to change the prediction on targeted samples at deployment time. \\nMitigations. Targeted poisoning attacks are notoriously challenging to defend against. \\nJagielski et al. [144] showed an impossibility result for subpopulation poisoning attacks. \\nTo mitigate some of the risks associated with such attacks, cybersecurity mechanisms \\nfor dataset provenance and integrity attestation [220] should be used judiciously. Ma et \\nal. [192] proposed the use of differential privacy (DP) as a defense (which follows directly \\nfrom the defnition of differential privacy), but it is well known that differentially private ML models have lower accuracy than standard models. The trade-off between robustness and accuracy needs to be considered in each application. If the application has strong data privacy requirements, and differentially private training is used for privacy, then an ad-ditional beneft is protection against targeted poisoning attacks. However, the robustness offered by DP starts to fade once the targeted attack requires multiple poisoning samples (as in subpopulation poisoning attacks) because the group privacy bound will not provide meaningful guarantees for large poisoned sets. \\n2.3.3. Backdoor Poisoning \\nIn \\n2017, Gu et al. [124] proposed BadNets, the frst backdoor poisoning attack. They \\nobserved that image classifers can be poisoned by adding a small patch trigger in a subset of images at training time and changing their label to a target class. The classifer learns to associate the trigger with the target class, and any image – including the trigger or backdoor pattern – will be misclassifed to the target class at testing time. Concurrently, Chen et al. [69] introduced backdoor attacks in which the trigger is blended into the training data. Follow-up work introduced the concept of clean-label backdoor attacks [302] in which \\nthe adversary is restricted in preserving the label of the poisoned examples. Clean-label attacks typically require more poisoning samples to be effective, but the attack model is \\n24 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='db382d57-f728-468a-aa3c-5024898d08aa', embedding=None, metadata={'page_label': '25', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nmore realistic. \\nIn the last few years, backdoor attacks have become more sophisticated and stealthy, mak-\\ning them harder to detect and mitigate. Latent backdoor attacks were designed to survive even upon model fne-tuning of the last few layers using clean data [331]. Backdoor Gener -\\nating Network (BaN) [253] is a dynamic backdoor attack in which the location of the trigger changes in the poisoned samples so that the model learns the trigger in a location-invariant manner. Functional triggers, a.k.a. F\\nUNCTIONAL ATTACKS , are embedded throughout the \\nimage or change according to the input. For instance, Li et al. [176] used steganography \\nalgorithms to hide the trigger in the training data. Liu et al. [186] introduced a clean-label \\nattack that uses natural refection on images as a backdoor trigger. Wenger et al. [320] poi-\\nsoned facial recognition systems by using physical objects as triggers, such as sunglasses and earrings. \\nOther data modalities. While the majority of backdoor poisoning attacks are designed \\nfor computer vision applications, this attack vector has been effective in other application \\ndomains with different data modalities, such as audio, NLP, and cybersecurity settings. \\n• Audio: In audio domains, Shi et al. [268] showed how an adversary can inject an \\nunnoticeable audio trigger into live speech, which is jointly optimized with the target model during training. \\n• NLP: In natural language processing, the construction of meaningful poisoning sam-\\nples is more challenging as the text data is discrete, and the semantic meaning of sentences would ideally be preserved for the attack to remain unnoticeable. Recent work has shown that backdoor attacks in NLP domains are becoming feasible. For instance, Chen et al. [68] introduced semantic-preserving backdoors at the charac-\\nter, word, and sentence level for sentiment analysis and neural machine translation applications. Li et al. [175] generated hidden backdoors against transformer mod-\\nels using generative language models in three NLP tasks: toxic comment detection, neural machine translation, and question answering. \\n• Cybersecurity: Early poisoning attacks in cybersecurity were designed against worm \\nsignature generation in 2006 [236] and spam detectors in 2008 [222], well before \\nrising interest in adversarial machine learning. More recently, Severi et al. [257] \\nshowed how AI explainability techniques can be leveraged to generate clean-label poisoning attacks with small triggers against malware classifers. They attacked mul-tiple models (i.e., neural networks, gradient boosting, random forests, and SVMs), using three malware datasets: Ember for Windows PE fle classifcation, Contagio for PDF fle classifcation, and DREBIN for Android app classifcation. Jigsaw Puz-zle [329] designed a backdoor poisoning attack for Android malware classifers that \\nuses realizable software triggers harvested from benign code. \\nMitigations. The literature on backdoor attack mitigation is vast compared to other poi-\\nsoning attacks. Below we discuss several classes of defenses, including data sanitization, \\n25 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='165b4c0f-b0e6-4fe5-b766-8e1ef1888da8', embedding=None, metadata={'page_label': '26', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\ntrigger reconstruction, model inspection and sanitization, and also their limitations. \\n• Training Data Sanitization: Similar to poisoning availability attacks, training data \\nsanitization can be applied to detecting backdoor poisoning attacks. For instance, \\noutlier detection in the latent feature space [129, 238, 300] has been effective for con-volutional neural networks used for computer vision applications. Activation Clus-tering [62] performs clustering of training data in representation space with the goal of isolating the backdoored samples in a separate cluster. Data sanitization achieves better results when the poisoning attack controls a relatively large fraction of training data, but is not that effective against stealthy poisoning attacks. Overall, this leads to a trade-off between attack success and detectability of malicious samples. \\n• Trigger reconstruction: This class of mitigations aims to reconstruct the backdoor \\ntrigger, assuming that it is at a fxed location in the poisoned training samples. Neu-ralCleanse by Wang et al. [310] developed the frst trigger reconstruction approach \\nand used optimization to determine the most likely backdoor pattern that reliably misclassifes the test samples. The initial technique has been improved to reduce performance time on several classes and simultaneously support multiple triggers in-serted into the model [131, 322]. A representative system in this class is Artifcial \\nBrain Simulation (ABS) by Liu et al. [184], which stimulates multiple neurons and \\nmeasures the activations to reconstruct the trigger patterns. Khaddaj et al. [156] de-\\nveloped a new primitive for detecting backdoor attacks and a corresponding effective detection algorithm with theoretical guarantees. \\n• Model inspection and sanitization: Model inspection analyzes the trained ML \\nmodel before its deployment to determine whether it was poisoned. An early work in this space is NeuronInspect [134], which is based on explainability methods to deter -\\nmine different features between clean and backdoored models that are subsequently used for outlier detection. DeepInspect [64] uses a conditional generative model to \\nlearn the probability distribution of trigger patterns and performs model patching to remove the trigger. Xu et al. [327] proposed the Meta Neural Trojan Detection \\n(MNTD) framework, which trains a meta-classifer to predict whether a given ML model is backdoored (or Trojaned, in the authors’ terminology). This technique is general and can be applied to multiple data modalities, such as vision, speech, tabular data, and NLP. Once a backdoor is detected, model sanitization can be performed via pruning [321], retraining [340], or fne-tuning [180] to restore the model’s accuracy. \\nMost of these mitigations have been designed against computer vision classifers based on convolutional neural networks using backdoors with fxed trigger patterns. Severi et al. [257] showed that some of the data sanitization techniques (e.g., spectral signatures [300] and Activation Clustering [62]) are ineffective against clean-label backdoor poisoning on \\nmalware classifers. Most recent semantic and functional backdoor triggers would also pose challenges to approaches based on trigger reconstruction or model inspection, which generally assume fxed backdoor patterns. The limitation of using meta classifers for pre-\\n26 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='565aafda-4edc-447e-b825-b17da70fe225', embedding=None, metadata={'page_label': '27', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\ndicting a Trojaned model [327] is the high computational complexity of the training stage \\nof the meta classifer, which requires training thousands of SHADOW MODELS . Additional \\nresearch is required to design strong backdoor mitigation strategies that can protect ML \\nmodels against this important attack vector without suffering from these limitations. \\nIn cybersecurity, Rubinstein et al. [249] proposed a principal component analysis (PCA)-\\nbased approach to mitigate poisoning attacks against PCA subspace anomaly detection \\nmethod in backbone networks. It maximized Median Absolute Deviation (MAD) instead of variance to compute principal components, and used a threshold value based on Laplace distribution instead of Gaussian. Madani and Vlajic [193] built an autoencoder-based in-\\ntrusion detection system, assuming malicious poisoning attack instances were under 2%. \\nA recent paper [156] provides a different perspective on backdoor mitigation, by showing \\nthat backdoors are indistinguishable from naturally occurring features in the data, if no \\nadditional assumptions are made about the attack. However, assuming that the backdoor creates the strongest feature in the data, the paper proposes an optimization technique to identify and remove the training samples corresponding to the backdoor. \\nTo complement existing mitigations that are not always resilient in face of evolving attacks, \\npoison forensics [259] is a technique for root cause analysis that identifes the malicious \\ntraining samples. Poison forensics adds another layer of defense in an ML system: Once a poisoning attack is detected at deployment time, poison forensics can trace back the source of attack in the training set. \\n2.3.4. Model Poisoning \\nModel \\npoisoning attacks attempt to directly modify the trained ML model to inject mali-\\ncious functionality into the model. In centralized learning, TrojNN [185] reverse engineers the trigger from a trained neural network and then retrains the model by embedding the trigger in external data to poison it. Most model poisoning attacks have been designed in the federated learning setting in which clients send local model updates to a server that aggregates them into a global model. Compromised clients can send malicious updates to poison the global model. Model poisoning attacks can cause both availability and integrity violation in federated models: \\n• Poisoning availability attacks that degrade the global model’s accuracy have been effective, but they usually require a large percentage of clients to be under the control of the adversary [101, 263]. \\n• Targeted model poisoning attacks induce integrity violations on a small set of sam-ples at testing time. They can be mounted by a model replacement or model boosting attack in which the compromised client replaces the local model update according to the targeted objective [13, 23, 285]. \\n• Backdoor model poisoning attacks introduce a trigger via malicious client updates \\n27 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='23577a1b-bdc2-4928-86cd-523e027a054d', embedding=None, metadata={'page_label': '28', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nto induce the misclassifcation of all samples with the trigger at testing time [13, 23, \\n285, 312]. Most of these backdoors are forgotten if the compromised clients do not \\nregularly participate in training, but the backdoor becomes more durable if injected in the lowest utilized model parameters [349]. \\nModel poisoning attacks are also possible in supply-chain scenarios where models or com-ponents of the model provided by suppliers are poisoned with malicious code. A recent supply-chain attack, Dropout Attack [336], shows how an adversary who manipulates the \\nrandomness used in neural network training (in particular in dropout regularization), might poison the model to decrease accuracy, precision, or recall on a set of targeted classes. \\nMitigations. To defend federated learning from model poisoning attacks, a variety of \\nByzantine-resilient aggregation rules have been designed and evaluated. Most of them \\nattempt to identify and exclude the malicious updates when performing the aggregation at the server [3, 31, 40, 125, 203–205, 284, 334]. However, motivated adversaries can bypass these defenses by adding constraints in the attack generation optimization problem [13, \\n101, 263]. Gradient clipping and differential privacy have the potential to mitigate model \\npoisoning attacks to some extent [13, 225, 285], but they usually decrease accuracy and do \\nnot provide complete mitigation. \\nFor specifc model poisoning vulnerabilities, such as backdoor attacks, there are some tech-\\nniques for model inspection and sanitization, as discussed in Section 2.3.3. However, miti-gating supply-chain attacks in which adversaries might control the source code of the train-ing algorithm or the ML hyperparameters, remains challenging. Program verifcation tech-niques used in other domains (such as cryptographic protocol verifcation [241]) might be \\nadapted to this setting, but ML algorithms have intrinsic randomness and non-deterministic behavior, which enhances the diffculty of verifcation. \\nDesigning ML models robust in face of supply-chain vulnerabilities is a critical open problem that needs to be addressed by the community. \\n28 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='43a20f47-241f-4e76-9968-77a203117aaf', embedding=None, metadata={'page_label': '29', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n2.4. Privacy Attacks \\nAlthough \\nprivacy issues have long been a concern, privacy attacks against aggregate infor -\\nmation collected from user records started with the seminal work of Dinur and Nissim [89] \\non DATA RECONSTRUCTION attacks. The goal of reconstruction attacks is to reverse engi-\\nneer private information about an individual user record or sensitive critical infrastructure data from access to aggregate information. More recently, data reconstruction attacks have been designed for binary and multi-class neural network classifers [39, 128]. Another \\nprivacy attack is that of \\nMEMBERSHIP -INFERENCE ATTACKS in which an adversary can \\ndetermine whether a particular record was included in the dataset used for computing sta-tistical information or training a machine learning model. Membership inference attacks were frst introduced by Homer et al. [130] for genomic data. Recent literature focuses on \\nmembership attacks against ML models in mostly black-box settings in which adversaries have query access to a trained ML model [43, 269, 333]. A different privacy violation \\nfor MLaaS is model extraction attacks, which are designed to extract information about an ML model such as its architecture or model parameters [47, 58, 141, 298]. Property infer -\\nence attacks [9, 61, 110, 195, 286, 346] aim to extract global information about a training \\ndataset, such as the fraction of training examples with a certain sensitive attribute. \\nThis section discusses privacy attacks related to data reconstruction, the memorization of \\ntraining data, membership inference, model extraction, and property inference, as well as mitigations for some of these attacks and open problems in designing general mitigation strategies. \\n2.4.1. Data Reconstruction \\nData \\nreconstruction attacks are the most concerning privacy attacks as they have the ability \\nto recover an individual’s data from released aggregate information. Dinur and Nissim [89] were the frst to introduce reconstruction attacks that recover user data from linear statis-tics. Their original attack requires an exponential number of queries for reconstruction, but subsequent work has shown how to perform reconstruction with a polynomial number of queries [96]. A survey of privacy attacks, including reconstruction attacks, is given by \\nDwork et al. [94]. More recently, the U.S. Census Bureau performed a large-scale study \\non the risk of data reconstruction attacks on census data [111], which motivated the use of \\ndifferential privacy in the decennial release of the U.S. Census in 2020. \\nIn the context of ML classifers, Fredrickson et al. [107] introduced model inversion attacks \\nthat reconstruct class representatives from the training data of an ML model. While model inversion generates semantically similar images with those in the training set, it cannot directly reconstruct the training data of the model. Recently, Balle et al. [15] trained a re-\\nconstructor network that can recover a data sample from a neural network model, assuming a powerful adversary with information about all other training samples. Haim et al. [128] \\nshowed how the training data of a binary neural network classifer can be reconstructed from access to the model parameters by leveraging theoretical insights about implicit bias \\n29 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='cd019395-1072-4c88-8f66-bb2872c568c1', embedding=None, metadata={'page_label': '30', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nin neural networks. This work has been recently extended to reconstruct training samples \\nof multi-class multi-layer perceptron classifers [39]. In another relevant privacy attack, \\nattribute inference, the attacker extracts a sensitive attribute of the training set, assuming partial knowledge about other features in the training data [147]. \\nThe ability to reconstruct training samples is partially explained by the tendency of neural \\nnetworks to memorize their training data. Zhang et al. [341] discussed how neural networks can memorize randomly selected datasets. Feldman [103] showed that the memorization of training labels is necessary to achieving almost optimal generalization error in ML. Brown et al. [36] constructed two learning tasks based on next-symbol prediction and cluster la-\\nbeling in which memorization is required for high-accuracy learning. Feldman and Zhang empirically evaluated the beneft of memorization for generalization using an infuence es-timation method [104]. We will discuss data reconstruction attacks and their connection to \\nmemorization for generative AI in Section 3.3.1. \\n2.4.2. Membership Inference \\nMembership \\ninference attacks also expose private information about an individual, like \\nreconstruction or memorization attacks, and are still of great concern when releasing ag-gregate information or ML models trained on user data. In certain situations, determining that an individual is part of the training set already has privacy implications, such as in a medical study of patients with a rare disease. Moreover, membership inference can be used as a building block for mounting data extraction attacks [48, 51]. \\nIn membership inference, the attacker’s goal is to determine whether a particular record or \\ndata sample was part of the training dataset used for the statistical or ML algorithm. These attacks were introduced by Homer et al. [130] for statistical computations on genomic \\ndata under the name tracing attacks. Robust tracing attacks have been analyzed when an \\nadversary gains access to noisy statistical information about the dataset [95]. In the last fve years, the literature has used the terminology membership inference for attacks against ML \\nmodels. Most of the attacks in the literature are performed against deep neural networks used for classifcation [43, 74, 171, 269, 332, 333]. Similar to other attacks in adversarial \\nmachine learning, membership inference can be performed in white-box settings [171, 218, 250] in which attackers have knowledge of the model’s architecture and parameters, but \\nmost of the attacks have been developed for black-box settings in which the adversary generates queries to the trained ML model [43, 74, 269, 332, 333]. \\nThe attacker’s success in membership inference has been formally defned using a cryp-\\ntographically inspired privacy game in which the attacker interacts with a challenger and needs to determine whether a target sample was used in training the queried ML model [146, 252, 333]. In terms of techniques for mounting membership inference attacks, the loss-\\nbased attack by Yeom et al. [333] is one of the most effcient and widely used method. \\nUsing the knowledge that the ML model minimizes the loss on training samples, the attack determines that a target sample is part of training if its loss is lower than a fxed threshold \\n30 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d2ce0118-e6fd-4fa5-a857-7f8c6d63ceff', embedding=None, metadata={'page_label': '31', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n(selected as the average loss of training examples). Sablayrolles et al. [250] refned the loss-\\nbased attack by scaling the loss using a per-example threshold. Another popular technique introduced by Shokri et al. [269] is that of shadow models, which trains a meta-classifer \\non examples in and out of the training set obtained from training thousands of shadow ML models on the same task as the original model. This technique is generally expensive, and while it might improve upon the simple loss-based attack, its computational cost is high and requires access to many samples from the distribution to train the shadow models. These two techniques are at opposite ends of the spectrum in terms of their complexity, but they perform similarly in terms of precision at low false positive rates [43]. \\nAn intermediary method that is obtains good performance in terms of the A\\nREA UNDER \\nTHE CURVE (AUC) metric is the LiRA attack by Carlini et al. [43], which trains a smaller \\nnumber of shadow models to learn the distribution of model logits on examples in and out \\nof the training set. Using the assumption that the model logit distributions are Gaussian, LiRA performs a hypothesis test for membership inference by estimating the mean and standard deviation of the Gaussian distributions. Ye et al. [332] designed a similar attack \\nthat performs a one-sided hypothesis test, which does not make any assumptions on the loss distribution but achieves slightly lower performance than LiRA. Recently, Lopez et al. [187] propose a more effcient membership inference attack that requires training a \\nsingle model to predict the quantiles of the confdence score distribution of the model under attack. Membership inference attacks have also been designed under the stricter label-only threat model in which the adversary only has access to the predicted labels of the queried samples [74]. \\nThere are several public privacy libraries that offer implementations of membership infer -\\nence attacks: the TensorFlow Privacy library [278] and the ML Privacy Meter [214]. \\n2.4.3. Model Extraction \\nIn \\nMLaaS scenarios, cloud providers typically train large ML models using proprietary data \\nand would like to keep the model architecture and parameters confdential. The goal of an \\nattacker performing a model extraction attack is to extract information about the model architecture and parameters by submitting queries to the ML model trained by an MLaaS provider. The frst model stealing attacks were shown by Tramer at al. [298] on several \\nonline ML services for different ML models, including logistic regression, decision trees, and neural networks. However, Jagielski et al. [141] have shown the exact extraction of \\nML models to be impossible. Instead, a functionally equivalent model can be reconstructed that is different than the original model but achieves similar performance at the prediction task. Jagielski et al. [141] have shown that even the weaker task of extracting functionally \\nequivalent models is NP-hard. \\nSeveral techniques for mounting model extraction attacks have been introduced in the lit-\\nerature. The frst method is that of direct extraction based on the mathematical formulation of the operations performed in deep neural networks, which allows the adversary to com-\\n31 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3344bdea-974a-46cb-9e5f-5f35879073e0', embedding=None, metadata={'page_label': '32', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\npute model weights algebraically [47, 141, 298]. A second technique explored in a series \\nof papers is to use learning methods for extraction. For instance, active learning [58] can \\nguide the queries to the ML model for more effcient extraction of model weights, and rein-\\nforcement learning can train an adaptive strategy that reduces the number of queries [231]. \\nA third technique is the use of SIDE CHANNEL information for model extraction. Batina et \\nal. [18] used electromagnetic side channels to recover simple neural network models, while Rakin et al. [245] showed how \\nROWHAMMER ATTACKS can be used for model extraction \\nof more complex convolutional neural network architectures. \\nNote that model extraction is often not an end goal but a step towards other attacks. As the \\nmodel weights and architecture become known, attackers can launch more powerful attacks typical for the white-box or gray-box settings. Therefore, preventing model extraction can mitigate downstream attacks that depend on the attacker having knowledge of the model architecture and weights. \\n2.4.4. Property Inference \\nIn \\nproperty inference attacks, the attacker tries to learn global information about the training \\ndata distribution by interacting with an ML model. For instance, an attacker can determine the fraction of the training set with a certain sensitive attribute, such as demographic infor -\\nmation, that might reveal potentially confdential information about the training set that is not intended to be released. \\nProperty inference attacks were introduced by Ateniese et al. [9] and formalized as a distin-\\nguishing game between the attacker and the challenger training two models with different fractions of the sensitive data [286]. Property inference attacks were designed in white-box settings in which the attacker has access to the full ML model [9, 110, 286] and black-box \\nsettings in which the attacker issues queries to the model and learns either the predicted labels [195] or the class probabilities [61, 346]. These attacks have been demonstrated for \\nHIDDEN MARKOV MODELS , SUPPORT VECTOR MACHINES [9], FEED -FORWARD NEU -\\nRAL NETWORKS [110, 195, 346], CONVOLUTIONAL NEURAL NETWORKS [286], FED-\\nERATED LEARNING MODELS [200], GENERATIVE ADVERSARIAL NETWORKS [351], and \\nGRAPH NEURAL NETWORKS [350]. Mahloujifar et al. [195] and Chaudhauri et al. [61] \\nshowed that poisoning the property of interest can help design a more effective distin-guishing test for property inference. Moreover, Chaudhauri et al. [61] designed an effcient property size estimation attack that recovers the exact fraction of the population of interest. \\n2.4.5. Mitigations \\nThe \\ndiscovery of reconstruction attacks against aggregate information motivated the rig-\\norous defnition of differential privacy (DP) [92, 93]. Differential privacy is an extremely \\nstrong defnition of privacy that guarantees a bound on how much an attacker with access to the algorithm output can learn about each individual record in the dataset. The original pure defnition of DP has a privacy parameter ε (i.e., privacy budget), which bounds the \\n32 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f60253ec-8ff1-404a-b6f6-9ca52aec576f', embedding=None, metadata={'page_label': '33', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nprobability that the attacker with access to the algorithm’s output can determine whether \\na particular record was included in the dataset. DP has been extended to the notions of approximate DP, which includes a second parameter δ that is interpreted as the probability \\nof information accidentally being leaked in addition to ε and R `enyi DP [208]. \\nDP has been widely adopted due to several useful properties: group privacy (i.e., the exten-sion of the defnition to two datasets differing in k records), post-processing (i.e., privacy \\nis preserved even after processing the output), and composition (i.e., privacy is composed if multiple computations that are performed on the dataset). DP mechanisms for statisti-cal computations include the Gaussian mechanism [93], the Laplace mechanism [93], and the Exponential mechanism [198]. The most widely used DP algorithm for training ML \\nmodels is DP-SGD [1], with recent improvements such as DP-FTRL [151] and DP matrix \\nfactorization [86]. \\nBy defnition, DP provides mitigation against data reconstruction and membership infer -\\nence attacks. In fact, the defnition of DP immediately implies an upper bound on the \\nsuccess of an adversary in mounting a membership inference attack. Tight bounds on the success of membership inference have been derived by Thudi et al. [291]. However, DP \\ndoes not provide guarantees against model extraction attacks, as this method is designed to protect the training data, not the model. Several papers reported negative results on us-ing differential privacy to protect against property inference attacks which aim to extract properties of subpopulations in the training set [61, 195]. \\nOne of the main challenges of using DP in practice is setting up the privacy parameters to \\nachieve a trade-off between the level of privacy and the achieved utility, which is typically measured in terms of accuracy for ML models. Analysis of privacy-preserving algorithms, such as DP-SGD, is often worst case and not tight, and selecting privacy parameters based purely on theoretical analysis results in utility loss. Therefore, large privacy parameters are often used in practice (e.g., the 2020 U.S. Census release used ε = 19.61), and the exact \\nprivacy obtained in practice is diffcult to estimate. Recently, a promising line of work is that of privacy auditing introduced by Jagielski et al. [145] with the goal of empirically \\nmeasuring the actual privacy guarantees of an algorithm and determining privacy lower bounds by mounting privacy attacks. Auditing can be performed with membership infer -\\nence attacks [146, 338], but poisoning attacks are much more effective and result in better \\nestimates of the privacy leakage [145, 219]. Recent advances in privacy auditing include \\ntighter bounds for the Gaussian mechanism [217], as well as rigorous statistical methods \\nthat allow the use of multiple canaries to reduce the sample complexity of auditing [240]. \\nAdditionally, two effcient methods for privacy auditing with training a single model have been proposed: Steinke et al. [281] use multiple random data canaries without incurring \\nthe cost of group privacy; and Andrew et al. [4] use multiple random client canaries and a \\ncosine similarity test statistics to audit user-level private federated learning. \\n33 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d0219487-94fb-40d0-ba7c-1d16dfb2b901', embedding=None, metadata={'page_label': '34', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nDifferential privacy provides a rigorous notion of privacy, protecting against mem-\\nbership inference and data reconstruction attacks. To achieve the best balance between privacy and utility, empirical privacy auditing is recommended to com-plement the theoretical analysis of private training algorithms. \\nOther mitigation techniques against model extraction, such as limiting user queries to the model, detecting suspicious queries to the model, or creating more robust architectures to prevent side channel attacks exist in the literature. However, these techniques can be cir -\\ncumvented by motivated and well-resourced attackers and should be used with caution. We refer the reader to available practice guides for securing machine learning deploy-ments [57, 226]. A completely different approach to potentially mitigating privacy leakage of a user’s data is to perform \\nMACHINE UNLEARNING , a technique that enables a user to \\nrequest removal of their data from a trained ML model. Existing techniques for machine unlearning are either exact (e.g., retraining the model from scratch or from a certain check-point) [34, 41] or approximate (updating the model parameters to remove the infuence of \\nthe unlearned records) [115, 139, 221]. \\n34 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='02efb3ad-0aa3-4dcc-b22e-c06b8c4a114c', embedding=None, metadata={'page_label': '35', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n3. Generative AI Taxonomy \\nGenerati\\nve AI is a branch of AI that develops generative models with the capability of \\nlearning to generate content such as images, text, and other media with similar proper -\\nties as their training data. Generative AI includes several different types of AI technolo-\\ngies with distinct origins, modeling approaches and related properties: GENERATIVE AD -\\nVERSARIAL NETWORKS , GENERATIVE PRE-TRAINED TRANSFORMER , and D IFFUSION \\nMODEL . Recently, multi-modal AI systems have started to combine two or more technolo-\\ngies to enable multi-modal content generation capabilities [289]. \\n3.1. Attack Classifcation \\nWhile \\nmany attack types in the PredAI taxonomy apply to GenAI (e.g., model poisoning, \\ndata poisoning, evasion, model extraction, etc.), a substantial body of recent work on the security of GenAI merits particular focus on novel security violations. \\nFigure 2 introduces a taxonomy of attacks in adversarial machine learning for GenAI sys-\\ntems. Similar to the PredAI taxonomy in Figure 1, this taxonomy is frst categorized by \\nthe attacker’s objectives, which include availability breakdowns, integrity violations, and \\nprivacy compromise. For GenAI systems, violations of abuse are also especially relevant. \\nThe capabilities that an adversary must leverage to achieve their objectives are shown in \\nthe outer layer of the objective circles. Attack classes are shown as callouts connected to the capabilities required to mount each attack. Multiple attack classes that require the same capabilities to reach the same objective are shown in a single callout. \\n35 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='07006a3e-a387-4051-bced-635f39f1e21b', embedding=None, metadata={'page_label': '36', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nIntegrity\\nPrivacyAvailability\\nAbuse\\nTraining Data\\nTraining DataIndirect  \\nPrompt Injection\\nIndirect  \\nPrompt InjectionData Poisoning\\nData Poisoning\\nBackdoor\\nPoisoningQuery AccessIncreased\\nComputation\\nModel\\nDataIndirect  \\nPrompt InjectionPrompt Extraction\\nInformation\\nGatheringResourceQuery AccessQuery Access Resource\\nMembership InferenceTargeting\\nPoisoningIndirect  \\nPrompt Injection\\nResource\\nControlControlQuery AccessSource Data\\nControlResource\\nControlControlSource Data\\nControl\\nResource\\nControlTraining DataControlQuery AccessSource Data\\nControlDenial of Service\\nMisaligned\\nInputs\\nPrompt Injection\\nUnauthorized\\nDisclosureControlControlPrompt Injection\\nBackdoor\\nPoisoningPrompt Injection\\nTraining Data\\nAttacksPrompt Injection\\nData Extraction\\nFigure 2. Taxonomy of attacks on Generative AI systems \\nAn attack can be further categorized by the learning stage to which it applies and, sub-\\nsequently, by the attacker’s knowledge and access. These are reviewed in the following sections. Where possible, the discussion broadly applies to GenAI with some specifc areas that apply to LLMs (e.g., R\\nETRIEVAL AUGMENTED GENERATION [RAG], which \\ndominates many of the deployment stage attacks described below). \\n3.1.1. GenAI Stages of Learning \\nDue \\nto the size of the models and training sets, predominant patterns in GenAI model \\ndevelopment have departed from historical processes in which the entire process of data collection, labeling, model training, model validation, and model deployment are accom-plished in a single pipeline by a single organization. Instead, foundation models are created during a pre-training stage that makes heavy use of unsupervised learning. The foundation \\n36 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7c1f7db3-0fe1-47a5-a825-01ec25846f84', embedding=None, metadata={'page_label': '37', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nmodel encodes patterns (e.g., in text, images, etc.) that are useful for downstream tasks. \\nThe foundation models themselves are then the basis for creating task-specifc applications via fne-tuning. In many cases, application developers begin with a foundation model devel-\\noped by a third party and fne-tune it for their specifc application. Attacks that correspond to the stages of GenAI application development are described in more detail below. \\nTraining-time attacks. The \\nTRAINING STAGE for GenAI often consists of two distinct \\nstages: foundation model PRE-TRAINING and model FINE -TUNING . This pattern exists \\nfor generative image models, text models, audio models, and multimodal models, among \\nothers. Since foundation models are most effective when trained on large datasets, it has become common to scrape data from a wide range of public sources. This makes founda-tion models especially susceptible to \\nPOISONING ATTACKS , in which an adversary controls \\na subset of the training data. Researchers have demonstrated that an attacker can induce targeted failures in models by arbitrarily poisoning only 0.001% of uncurated web-scale training datasets [42]. Executing web-scale dataset poisoning can be as simple as purchas-\\ning a small fraction of expired domains from known data sources [46]. Model fne-tuning \\nmay also be susceptible to poisoning attacks under the more common attacker knowledge and capabilities outlined in Section 2.1. \\nInference-time attacks. The \\nDEPLOYMENT STAGE for GenAI also differs from PredAI. \\nHow a model is used during deployment is application-specifc. However, underlying many \\nof the security vulnerabilities in LLMs and RAG applications is the fact that data and instructions are not provided in separate channels to the LLM, which allows attackers to use data channels to conduct inference-time attacks that are similar to decades-old SQL injection. Acknowledging a particular emphasis on LLMs, specifcally for question-and-answering and text-summarization tasks, many of the attacks in this stage are due to the following practices that are common to applications of text-based generative models: \\n1. Alignment via model instructions: LLM behaviors are aligned at inference time \\nthrough instructions that are pre-pended to the model’s input and context. These in-structions comprise a natural language description of the model’s application-specifc use case (e.g., “You are a helpful fnancial assistant that responds gracefully and concisely....”). A \\nJAILBREAK overrides this explicit alignment and other safeguards. \\nSince these prompts have been carefully crafted through prompt engineering, a PROMPT \\nEXTRACTION attack may attempt to steal these system instructions. These attacks are \\nalso relevant to multimodal and text-to-image models. \\n2. Contextual few-shot learning: Since LLMs are autoregressive predictors, their per -\\nformance in applications can be improved by providing examples of the inputs and outputs expected for the application in the model’s context that is prepended to the user query before evaluation by the LLM. This allows the model to more naturally complete the autoregressive tasks [37]. \\n3. Runtime data ingestion from third-party sources: As is typical in R\\nETRIEVAL \\nAUGMENTED GENERATION applications, context is crafted at runtime in a query-\\n37 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c239dd69-8226-45e5-a4bc-d35c8d0c487d', embedding=None, metadata={'page_label': '38', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\ndependent way and populated from external data sources (e.g., documents, web \\npages, etc.) that are to be summarized as part of the application. INDIRECT PROMPT \\nINJECTION attacks depend on the attacker’s ability to modify the context using out-\\nside sources of information that are ingested by the system, even if not directly by the user. \\n4. Output handling: The output of an LLM may be used to populate an element on a \\nweb-page or to construct a command. \\n5. Agents: Plugins, functions, agents, and other concepts all rely on processing the \\noutput of the LLM (item 4) to perform some additional task and provide additional \\ncontext to its input (item 3). In some cases, the LLM selects from among an ap-\\npropriate set of these external dependencies based on a confguration provided in natural language and invokes that code with templates flled out by the LLM using information in the context. \\nFigure 3. Retrieval-augmented generation relies on system instructions, context, and \\ndata from third-party sources, often through a vector database, to produce relevant \\nresponses for users \\nsystem\\nuser\\nassistant\\nuserYou are a helpful financial \\nassistant. Be concise.\\nWhat was the net income of \\nAMZN in 2020 Q1?\\n$2.5B, down from $3.6B in \\n2019 Q1 \\nWhat was their operating \\ncash flow?context\\nLLM\\nassistantinput\\nresources\\n(e.g., vector db)assistantdoc1: AMAZON.COM ANNOUNCES \\nFIRST QUARTER RESULTS ...doc2: Financials results ending Mar 31, 2020 ...\\n3.1.2. Attacker Goals and Objectives \\nAs with PredAI, attacker objectives can be classifed broadly along the dimensions of avail-ability, integrity, and privacy, as described in Section 2.1.2. However, there is a fourth \\nattacker objective of abuse that is specifc to GenAI: \\nAbuse violations. Abuse violations occur when an attacker repurposes a GenAI system’s \\nintended use to achieve their own objectives. Attackers can use the capabilities of GenAI \\nmodels to promote hate speech or discrimination, generate media that incites violence against specifc groups, or scale offensive cybersecurity operations by creating images, text, or malicious code that enable a cyber attack. \\n38 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7642fef1-ca25-4b50-89cc-43381fa1a86b', embedding=None, metadata={'page_label': '39', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n3.1.3. Attacker Capabilities \\nNo\\nvel attacker capabilities that enable GenAI attacker objectives include: \\n• TRAINING DATA CONTROL : The attacker might take control of a subset of the train-\\ning data by inserting or modifying training samples. This capability is used in data \\npoisoning attacks. \\n• QUERY ACCESS : Many GenAI models and their applications (e.g., RETRIEVAL \\nAUGMENTED GENERATION ) are deployed as cloud-hosted services with access con-\\ntrolled through API keys. In this case, the attacker can submit queries to the model to receive an output. In GenAI, the purpose of submitting attacker-tuned inputs is to elicit a specifc behavior from the model. This capability is used for \\nPROMPT \\nINJECTION , PROMPT EXTRACTION , and model stealing attacks. \\n• SOURCE CODE CONTROL : The attacker might modify the source code of the ML al-\\ngorithm, such as the random number generator or any third-party libraries, which are often open source. The advent of open-source model repositories, like HuggingFace, allows attackers to create malicious models or wrap benign models with malicious code embedded in the deserialization format. \\n• \\nRESOURCE CONTROL : The attacker might modify resources (e.g., documents, web \\npages) that will be ingested by the GenAI model at runtime. This capability is used for \\nINDIRECT PROMPT INJECTION attacks. \\n3.2. AI Supply Chain Attacks and Mitigations \\nStudies \\non real-world security vulnerabilities against ML show that security is best ad-\\ndressed comprehensively, including software, data and model supply chains, and network and storage systems [7, 292]. Since AI is software, it inherits many of the vulnerabilities \\nof the traditional software supply chain. However, many practical GenAI tasks begin with open-source models or data that have typically been out of scope for traditional cybersecu-rity. For example, ML repositories with the largest software vulnerability exposure include TensorFlow and OpenCV [292]. \\n3.2.1. Deserialization Vulnerability \\nMan\\ny ML projects begin by downloading an open-source GenAI model for use in a down-\\nstream application. Most often, these models exist as artifacts persisted in pickle, pytorch, \\njoblib, numpy, or tensorflow formats. Each of these formats allow for serialiazation \\npersistence mechanisms that, in turn, allow for arbitrary code execution (ACE) when de-\\nserialized. ACE via deserialization is typically categorized as a critical vulnerability (e.g., CVE-2022-29216 for tensorfow, or CVE-2019-6446 for pickle in neural network tools) [292]. \\n39 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e8b12549-8e42-4051-83da-c267f7bf7f03', embedding=None, metadata={'page_label': '40', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n3.2.2. Poisoning Attacks \\nThe \\nperformance of GenAI text-to-image and language models scales with model size and \\ndataset size and quality. For example, scaling laws indicate that training a 500 billion \\nparameter models would require 11 trillion tokens of training data [46]. Thus, it has become common for GenAI foundation model developers to scrape data from a wider range of uncurated sources. Dataset publishers only provide a list of URLs to constitute the dataset, and the domains serving those URLs can expire or be purchased, and the resources can be replaced by an attacker. As with PredAI models (discussed in Section 2.1), this could \\nlead to \\nTARGETED POISONING ATTACKS , BACKDOOR POISONING ATTACKS , and MODEL \\nPOISONING . A simple mitigation is for datasets to list both the URL and a cryptographic \\nhash of the content that can be verifed by the downloader. However, this technique may not scale up well for some of the large distributed datasets on the Internet - see Section 4.1 \\nfor further information. \\n3.2.3. Mitigations \\nAI \\nsupply chain attacks can be mitigated by supply chain assurance practices. For model \\nfle dependencies, this includes regular vulnerability scanning of the model artifacts used in the ML pipeline [292], and by adopting safe model persistence formats like safetensors . \\nFor webscale data dependencies, this includes verifying web downloads by publishing (by the provider) cryptographic hashes and verifying (by the downloader) training data as a ba-sic integrity check to ensure that domain hijacking has not injected new sources of data into the training dataset [46]. Another approach to mitigating risks associated with malicious \\nimage editing by large diffusion models is immunizing images to make them resistant to \\nmanipulation by these models [254]. However, this approach requires an additional policy \\ncomponent to make it effective and practical. \\n3.3. Direct Prompt Injection Attacks and Mitigations \\nA \\ndirect prompt injection occurs when a user injects text that is intended to alter the behav-\\nior of the LLM. \\nAttacker goals. An attacker may have a variety of goals with a prompt injection [182, 183, \\n265], such as: \\n• Abuse. Attackers use direct prompt injections to bypass safeguards in order to cre-\\nate misinformation, propaganda, hurtful content, sexual content, malware (code), or \\nphishing content. Often, the list of prohibited scenarios is explicitly safeguarded by the model creator [5, 121, 230]. A direct prompt injection for the purpose of model \\nabuse is also called a \\nJAILBREAK . \\n• Invade privacy. Attackers may wish to extract the system prompt or reveal private \\ninformation provided to the model in the context but not intended for unfltered ac-cess by the user. This is discussed further in Section 3.3.1. \\n40 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a3521eed-0a7d-40cc-a952-5e2bb37f7022', embedding=None, metadata={'page_label': '41', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nAttacker techniques. Attacker techniques for launching direct prompt injection attacks \\nare numerous but tend to fall into several broad categories [319]: \\n• Gradient-based attacks are white-box optimization-based methods for designing \\njailbreaks that are very similar to the PredAI attacks discussed in Section 2.2.1. A \\ngradient-based distributional attack uses an approximation to make the adversarial \\nloss for generative transformer models differentiable, which aims to minimize lex-ical changes by enforcing perceptibility and fuency via BERTScore and perplexity [127]. HotFlip encodes modifcations of text into a binary vector and gradient steps \\nto minimize adversarial loss [97]. Originally designed to create adversarial examples for PredAI language classifers (e.g., sentiment analysis), subsequent works have leveraged HotFlip for GenAI using the following trick: since these autoregressive tokens generate a single token at a time, optimizing the frst generated token to pro-duce an affrmative response is often suffcient to prime the autoregressive generative process to complete a fully affrmative utterance [49]. Universal adversarial triggers \\nare a special class of these gradient-based attacks against generative models that seek to fnd input-agnostic prefxes (or suffxes) that, when included, produce the desired \\naffrmative response regardless of the remainder of the input [308, 354]. That these \\nuniversal triggers transfer to other models makes open-source models — for which there is ready white-box access — feasible attack vectors for transferability attacks to closed systems where only API access is available [354]. \\n• Manual methods for jailbreaking an LLM generally fall into two categories: com-\\npeting objectives and mismatched generalization [316]. These methods often exploit the model’s susceptibility to certain linguistic manipulations and extend beyond con-ventional adversarial inputs. In the category of competing objectives, additional in-structions are provided that compete with the instructions originally provided by the author. \\n1. Prefx injection: This method involves prompting the model to commence re-sponses with an affrmative confrmation. By conditioning the model to begin its output in a predetermined manner, adversaries attempt to infuence its subse-quent language generation toward specifc, predetermined patterns or behaviors. \\n2. Refusal suppression: Adversaries provide explicit instructions to the model, compelling it to avoid generating refusals or denials in its output. By limiting or prohibiting the generation of negative responses, this tactic aims to ensure the model’s compliance with the provided instructions, potentially compromising safety measures. \\n3. Style injection: In this approach, adversaries instruct the model not to use long words or adopt a particular style. By constraining the model’s language to sim-plistic or non-professional tones, it aims to limit the sophistication or accuracy of the model’s responses, thereby potentially compromising its overall perfor -\\nmance. \\n41 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6a271c55-4226-4622-a302-2be6422ab6e6', embedding=None, metadata={'page_label': '42', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n4. Role-play: Utilizing role-play strategies, such as “Do Anything Now” (DAN) \\nor “Always Intelligent and Machiavellian” (AIM), adversaries guide the model to adopt specifc personas or behavioral patterns that confict with the original intent. This manipulation aims to exploit the model’s adaptability to varied roles or characteristics, potentially compromising its adherence to safety protocols. \\nTechniques in the mismatched generalization category diverge signifcantly from any safety training or guardrails, positioning inputs to be out of distribution from the model’s standard training data. Approaches include: \\n1. Special encoding: Adversarial inputs often employ encoding techniques like base64 encoding. This method alters the representation of input data and ren-ders it unrecognizable to standard recognition algorithms. By encoding infor -\\nmation, adversaries aim to deceive the model’s understanding of the input and bypass its safety mechanisms. \\n2. Character transformation: Techniques like ROT13 cipher, symbol replacement (e.g., l33tspeak), and Morse code manipulate the characters of the input text. These transformations aim to obscure the original meaning of the text, poten-tially confusing the model’s interpretation and enabling adversarial inputs to evade detection. \\n3. Word transformation: Strategies that aim to alter the linguistic structure may include Pig Latin, synonym swapping (e.g., using “pilfer” for “steal”), and pay-load splitting (or “token smuggling”) to break down sensitive words into sub-strings. These manipulations intend to deceive the model’s safeguards in a way that is still comprehended by the LLM. \\n4. Prompt-level obfuscation: Adversaries employ methods like translation to other languages to make the model obfuscate or summarize content in a manner that it may not fully comprehend. These obfuscations introduce ambiguity or al-tered linguistic contexts and create input scenarios in which the model’s safety mechanisms are less effective due to a lack of clarity or misinterpretation. \\n• Automated model-based red teaming employs three models: an attacker model, a \\ntarget model, and a judge [60, 199, 237]. When the attacker has access to a high \\nquality classifer to judge whether model output is harmful, it may be used as a re-ward function to train a generative model to generate jailbreaks of another generative model. Only query access is required for each of the models, and no human inter -\\nvention is required to update or refne a candidate jailbreak. Empirically, these al-gorithms may be orders of magnitude more query-effcient than existing algorithms, requiring only dozens of queries per successful jailbreak. The prompts have also been shown to be transferable from the target model to other closed-source LLMs [60]. \\n42 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='722bd4cf-42c8-4c80-8f9c-c04cb4dfb872', embedding=None, metadata={'page_label': '43', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n3.3.1. Data Extraction \\nGenAI \\nmodels are trained on data that may include proprietary or sensitive information. \\nGenAI applications may also be instrumented with carefully crafted prompts or — as with \\nRAG — be supplied with sensitive information in their context for summarization or other task completion. Attacker techniques for extracting this information are the subject of ongoing research for both LLMs [174, 348] and text-to-image models [266]. \\nLeaking sensitive information. Carlini et al. [48] were the frst to practically demonstrate \\ndata extraction attacks in generative language models. By inserting synthetic canaries in \\nthe training data, they developed a methodology for extracting the canaries and introduced a metric called exposure to measure memorization. Subsequent work demonstrated the risk \\nof data extraction in large language models based on transformers, such as GPT-2 [51], by \\nprompting the model with different prefxes and mounting a membership inference attack to determine which generated content was part of the training set. Since these decoder stack transformers are autoregressive models, a verbatim textual prefx about personal in-formation can result in the model completing the text input with sensitive information that includes email addresses, phone numbers, and locations [191]. This behavior of verbatim \\nmemorization of sensitive information in GenAI language models has also been observed in more recent transformer models with the additional characterization of extraction meth-ods [132]. Unlike PredAI models, in which carefully crafted tools like Text Revealer are \\ncreated to reconstruct text from transformer-based text classifers [343], GenAI models \\ncan simply be asked to repeat private information that exists in the context as part of the conversation. Results show that information like email addresses can be revealed at rates exceeding 8%. However, their responses may wrongly assign the owner of the informa-tion. In general, extraction attacks are more successful when the model is seeded with more specifc and complete information — the more the attacker knows, the more they can extract. Intuitively, larger models with more capacity are more susceptible to exact reconstruction [45]. \\nPrompt and context stealing. Prompts are vital to align LLMs to a specifc use case and \\nare a key ingredient to their utility in following human instructions. Well-crafted prompts \\nenable LLMs to be smart assistants with external applications and provide instructions for human alignment. These prompts are of high value and are usually regarded as commer -\\ncial secrets. Successful prompt-stealing attacks may violate the intellectual property and privacy of prompt engineers or jeopardize the business model of prompt trading market-places. PromptStealer is a learning-based method that reconstructs prompts from text-to-image models using an image captioning model and a multi-label classifer to steal both the subject and the prompt modifers [266]. For LLMs, researchers have found that a small set of fxed attack queries (e.g., Repeat all sentences in our conversation ) were \\nsuffcient to extract more than 60% of prompts across all model and dataset pairs [348]. \\nIn RAG applications (see Figure 3), the same techniques can be used to extract sensitive \\ninformation provided in the LLMs context. For example, rows from a database or text \\nfrom a PDF document that are intended to be summarized generically by the LLM can be \\n43 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d03745ba-0207-4097-b2ad-85778186061d', embedding=None, metadata={'page_label': '44', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nverbosely extracted by simply asking for them via direct prompt injection. \\n3.3.2. Mitigations \\nV\\narious defense strategies have been proposed for prompt injection that provide a measure \\nof protection but not full immunity to all attacker techniques. These broadly fall into the \\nfollowing categories: \\n1. Training for alignment. Model providers continue to create built-in mechanisms \\nby training with stricter forward alignment [148]. For example, model alignment \\ncan be tuned by training on carefully curated and prealigned datasets. It can then be iteratively improved through reinforcement learning with human feedback [123]. \\n2. Prompt instruction and formatting techniques. LLM instructions can cue the \\nmodel to treat user input carefully [168, 182]. For example, by appending specifc \\ninstructions to the prompt, the model can be informed about subsequent content that may constitute a jailbreak. Positioning the user input before the prompt takes advan-tage of recency bias in following instructions. Encapsulating the prompt in random characters or special HTML tags provides cues to the model about what constitutes system instructions versus user prompts. \\n3. Detection techniques. Model providers continue to create built-in mechanisms by \\ntraining with stricter backward alignment via evaluation on specially crafted bench-\\nmark datasets or flters that monitor the input to and output of a protected LLM [148]. One proposed method is to evaluate a distinctly prompted LLM that can \\naid in distinguishing potentially adversarial prompts [168, 182]. Several commer -\\ncial products have begun offering tools to detect prompt injection, both by detecting potentially malicious user input and by moderating the output of the frewall for jail-break behavior [8, 166, 247]. These may provide supplementary assurance through \\na defense-in-depth philosophy. \\nSimilarly, defenses for prompt stealing have yet to be proven rigorous. A commonality in the methods is that they compare the model utterance to the prompt, which is known by the system provider. Defenses differ in how this comparison is made, which might include looking for a specifc token, word, or phrase, as popularized by [48], or comparing the \\nn-grams of the output to the input [348]. \\n3.4. Indirect Prompt Injection Attacks and Mitigations \\nA \\ndominant use case for LLMs is R ETRIEVAL AUGMENTED GENERATION , depicted in \\nFigure 3. Using LLMs in retrieval tasks has blurred the data and instruction channels \\nto an LLM. This allows for attacker techniques that leverage the data channel to affect system operation, similar to decades-old SQL injection attacks. However, the attacker need not directly manipulate the LLM. \\nINDIRECT PROMPT INJECTION attacks are enabled \\nby RESOURCE CONTROL so that an attacker can indirectly (or remotely) inject system \\n44 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f7eba3fc-0e0c-481f-aa8b-b4c79b45c27f', embedding=None, metadata={'page_label': '45', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nprompts without directly interacting with the RAG application [122]. As with direct prompt \\ninjection, indirect prompt injection attacks can result in violations across the four categories of attacker goals: 1) availability violations, 2) integrity violations, 3) privacy compromises, and 4) abuse violations. \\n3.4.1. Availability Violations \\nModel \\navailability violations are a disruption in service that can be caused by an attacker \\nprompting a model with maliciously crafted inputs that cause increased computation or by overwhelming the system with a number of inputs that causes a denial of service to users. As noted in [122], these attack vectors are particularly interesting when executed \\nvia indirect prompt injection so that a resource rather than a registered user is the source of the availability violation. Availability attacks that increase computation make the model or service perform unusually slow [122]. In PredAI, this has most commonly been done by \\noptimizing sponge examples — an energy latency attack on neural networks [33]. However, \\nwith recent LLMs, this could be done by simply instructing the model to complete a time-sensitive task in the background, slowing the model down [122]. Denial-of-service attacks \\ncan indiscriminately render a model unusable (e.g., failure to generate helpful outputs) or specifcally block certain capabilities (e.g., specifc APIs) [122]. \\nAttacker Techniques. Researchers have demonstrated the following attacks on a commer -\\ncial RAG service [122] via indirect prompt injection, in which a resource to be searched or \\nsummarized contained instructions with certain characteristics: \\n• Time-consuming background tasks. The prompt instructs the model to perform a \\ntime-consuming task prior to answering the request. The prompt itself can be brief and request looping behavior in evaluating models [122]. \\n• Muting. This attack exploits the fact that a model cannot fnish sentences when an \\n<|endoftext|> token appears in the middle of a user’s request. By including a \\nrequest to begin a sentence with this token, a search agent, for example, will return without any generated text [122]. \\n• Inhibiting capabilities. In this attack, an embedded prompt instructs the model that \\nit is not permitted to use certain APIs (e.g., the search functionality for Bing Chat). This selectively disarms key components of the service [122]. \\n• Disrupting input or output. In this attack, an indirect prompt injection instructs the \\nmodel to replace characters in retrieved text with homoglyph equivalents, disrupting calls to APIs that depend on the text. Alternatively, the prompt can instruct the model to corrupt the results of a query to result in a useless retrieval or summary [122]. \\n45 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='734b3097-c0bb-40fa-a11f-d9368ef09be1', embedding=None, metadata={'page_label': '46', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n3.4.2. Integrity Violations \\nInte\\ngrity violations are threats that cause GenAI systems to become untrustworthy. AI \\nchatbots have exacerbated online misinformation, as demonstrated by the tendencies of \\nMicrosoft’s Bing and Google’s Bard to perpetuate each other’s sources of misinformation [307]. LLMs’ inability to gauge reliable sources of news and information can be exploited \\nto produce factually unsound outputs. \\nAttacker Techniques. Researchers have demonstrated integrity attacks by manipulating \\nthe primary task of the LLM. This is different than the more common indirect prompt \\ninjection attacks that perform a malicious side task. \\n• Manipulation. The model is prompted to provide incorrect summaries of the search \\nresult (i.e., arbitrarily wrong summaries) when the wrong output is not chosen in ad-vance [122]. The manipulation attack instructs the model to provide wrong answers \\nand causes the model’s answer to make claims that contradict the cited sources [122]. Below are two examples of manipulation attacks: \\n1. Wrong summaries. A model can be prompted to produce adversarially chosen or arbitrarily wrong summaries of documents, emails, or search queries [122]. \\n2. Propagate disinformation. Search chatbots can be prompted to propagate dis-information by relying on or perpetuating untrustworthy news sources or the outputs of other search chatbots [307]. \\n3.4.3. Privacy Compromises \\nIndirect \\nprompt injections introduce a host of new privacy compromises and concerns. \\nFrom the beginning, LLMs raised concerns about privacy. Italy was early to ban the use of ChatGPT due to these very concerns, and there have been many reported cases of chatbots leaking sensitive information or chat histories [87, 197]. Attacker goals are divided into \\ntwo key categories: \\n1. Information gathering. Specifc attacks can heighten these risks. For example, human-in-the-loop indirect prompting can be used to extract user data (e.g., personal infor -\\nmation, credentials) or leak their chat histories by interacting in chat sessions and persuading users to divulge information or through side channels [122]. An attack \\nexample that does not involve a human-in-the-loop is an attack against a personal as-sistant that can access a user’s data (e.g., real emails), which similarly causes privacy concerns [122]. \\n2. Unauthorized disclosure. Models are commonly integrated into system infrastruc-tures, giving way to unauthorized disclosures or privileges to private user data. Mali-cious actors can leverage backdoor attacks to gain access to LLMs or systems using a variety of methods (e.g., issuing API calls, malicious code auto-completions) [122]. \\n46 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='63bcbe8d-5025-4933-82c2-3d06fcbf5b67', embedding=None, metadata={'page_label': '47', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nAttacker Techniques. To highlight privacy concerns, researchers have demonstrated a \\ndata-stealing attack by designing an injection that instructs the LLM to persuade the end \\nuser to divulge their real name [122]. Below are some attack techniques that researchers \\nhave used to achieve this data-stealing attack: \\n• Human-in-the-loop indirect prompting. Read operations (e.g., triggering a search \\nquery that then makes a request to the attacker, or retrieving URLs directly) are ex-ploited to send information to the attacker [122]. \\n• Interacting in chat sessions. The model persuades a user to follow a URL into \\nwhich the attacker inserts the user’s name [122]. \\n• Invisible markdown image. A prompt injection is performed on a chatbot by mod-\\nifying the chatbot answer with an invisible single-pixel markdown image that with-draws the user’s chat data to a malicious third party [255]. \\n3.4.4. Abuse Violations \\nAs \\npreviously discussed, GenAI introduces a new category of attacker goal: abuse viola-\\ntions. This broadly refers to when an attacker repurposes a system’s intended use to achieve their own objectives by way of indirect prompt injection. This attacker goal can be divided into the following primary categories: \\n1. Fraud. Recent advances in instruction-following LLMs have caused simultaneous advances in dual-use risks. \\n2. Malware. LLMs can prominently facilitate the spread of malware by suggesting malicious links to the user. Additionally, the proliferation of LLM-integrated ap-plications has led to new malware threats by forcing the prompts themselves to act as malware [122]. For example, LLM-augmented email clients that read emails are \\nlikely to deliver malicious prompts and then send emails proliferating those prompts [122]. \\n3. Manipulation. Models currently act as vulnerable intermediary layers between users and information outputs that are easy to manipulate. LLMs are now commonly part of a larger system and integrate with applications. This intermediary state can ex-pose the model to a host of vulnerabilities. For example, a) search chatbots can be prompted to generate disinformation b) and prompted to hide specifc information, sources, or search queries; and c) models can be prompted to provide adversari-ally chosen or arbitrarily wrong summaries of information sources (e.g., documents, emails, search queries) [122]. While users are already prone to trusting untrustwor -\\nthy sources on the web, the authoritative tone of LLMs and users’ over-reliance on their impartiality have the potential to cause users to succumb to these manipulation attempts more often [122]. \\n47 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fbbd0a6e-cc18-4e2f-a151-479ff4f66b9c', embedding=None, metadata={'page_label': '48', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nAttacker Techniques. Researchers have demonstrated examples of different abuse attack \\ntechniques by conducting experiments with chatbots (e.g., Microsoft’s Bing chatbot). \\n• Phishing. Previously, it had been demonstrated that LLMs could produce convincing \\nscams, such as phishing emails [153]. Now that LLMs can more easily integrate \\nwith applications, they can not only enable the creation of scams but also widely \\ndisseminate such attacks [122]. Users are likely to be more susceptible to these new \\nattacks, unlike phishing emails, because they lack experience and awareness of this new threat technique. \\n• Masquerading. LLMs can pretending to be an offcial request from a service provider \\nor recommend a fraudulent website as trusted [122]. \\n• Spreading injections. The LLM itself acts as a computer running and spreading \\nharmful code. For example, an automatic message processing tool that can read and compose emails and look at users’ personal data can spread an injection to other models that may be reading those inbound messages [122]. \\n• Spreading malware. Similar to phishing, LLMs can be exploited to persuade users \\nto visit malicious web pages that lead to “drive-by downloads.” This is further en-abled by markdown links that could be seamlessly generated as part of the answer [122]. \\n• Historical distortion. An attacker can prompt the model to output adversarially cho-\\nsen disinformation. Researchers have demonstrated this by successfully prompting Bing Chat to deny that Albert Einstein won a Nobel Prize [122]. \\n• Marginally related context prompting. Steering search results toward specifc ori-\\nentations instead of neutral stances can create an attack to achieve bias amplifcation. Researchers have demonstrated this by prompting an LLM with biographies of per -\\nsonas that are either “conservative” or “liberal” and instructing the model to generate answers that agree with the views of the described users without explicitly mention-ing topics [122]. \\n3.4.5. Mitigations \\nV\\narious mitigation techniques have been proposed for indirect prompt injection attacks that \\nhelp eliminate model risk but — like the suggestions made for direct prompt injections — do not offer full immunity to all attacker techniques. These mitigation strategies fall into the following categories: \\n1. Reinforcement learning from human feedback (RLHF). RLHF is a type of AI model training whereby human involvement is indirectly used to fne-tune a model. This can be leveraged to better align LLMs with human values and prevent unwanted behaviors [122]. OpenAI’s GPT-4 was fne-tuned using RLHF and has shown a \\nlesser tendency to produce harmful content or hallucinate [229]. \\n48 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9dc43dcd-0854-42e3-b6b6-98fe040fc531', embedding=None, metadata={'page_label': '49', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n2. Filtering retrieved inputs. A further defense mechanism proposed by Greshake et al. \\nsuggests processing the retrieved inputs to flter out instructions [122]. \\n3. An LLM moderator. An LLM can be leveraged to detect attacks beyond just fltering clearly harmful outputs. This could be benefcial in detecting attacks that do not depend on retrieved sources but could fail at detecting disinformation or other kinds of manipulation attacks [122]. \\n4. Interpretability-based solutions. These solutions perform outlier detection of predic-tion trajectories [122]. Researchers have demonstrated that the prediction trajectory \\nof the tuned lens on anomalous inputs could be used to detect anomalous inputs [20]. \\nUnfortunately, there is no comprehensive or foolproof solution for protecting models against adversarial prompting, and future work will need to be dedicated to investigating suggested defenses for their effcacy. \\n49 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='0a79aea2-28a5-41e8-9a73-70abf173aa40', embedding=None, metadata={'page_label': '50', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n4. Discussion and Remaining Challenges \\n4.1. The Scale Challenge \\nData is fundamentally important for training models. As models grow, the amount of train-\\ning data grows proportionally. This trend is clearly visible in the evolution of LLM’s. Very few of the LLM’s in use today publish a detailed list of the data sources used in training but those who do [209, 293] show the scale of the footprint and the massive amounts of data \\nconsumed in training. The most recent multi-modal GenAI systems exacerbate the demand further by requiring large amounts of data for each modality. \\nThere is no single organization, even nation, that contains the full data used for training a \\ngiven LLM. Data repositories are not monolithic data containers but a list of labels and data links to other servers that actually contain the corresponding data samples. This renders the classic defnition of the corporate cybersecurity perimeter obsolete and creates new hard-to-mitigate risks [46]. Recently published open source data poisoning tools [202] increase the risk of large scale attacks on image training data. Although created with noble intentions, to allow artists to protect the copyright of their work, these tools become harmful if they fall in the hands of people with malicious intent. \\nAnother scale-related problem is the ability to generate synthetic content at scale on the in-\\nternet. Although \\nWATERMARKING [158] may alleviate the situation, the existence of pow-\\nerful open or other ungoverned models creates opportunities to generate massive amounts of unmarked synthetic content that can have a negative impact on the capabilities of subse-quently trained LLMs [272], leading to model collapse. \\nFinding ways to tackle these challenges of scale is critically important for evolving the \\ncapabilities of foundation models in the future, especially if we want them aligned with human values. \\n4.2. Theo\\n retical Limitations on Adversarial Robustness \\nGiv\\nen the multitude of powerful attacks, designing appropriate mitigations is a challenge \\nthat needs to be addressed before deploying AI systems in critical domains. This challenge \\nis exacerbated by the lack of information-theoretically secure machine learning algorithms for many tasks in the feld, as we discussed in Section 1. This implies that presently de-\\nsigning mitigations is an inherently ad hoc and fallible process. We refer the readers to available practice guides for securing machine learning deployments [57, 226], as well as \\nexisting guidelines for mitigating AML attacks [98]. \\nOne of the ongoing challenges facing the AML feld is the ability to detect when the model \\nis under attack. Knowing this would provide an opportunity to counter the attack before any information is lost or an adverse behaviour is triggered in the model. Tram `er [295] \\nhas shown that designing techniques to detect adversarial examples is equivalent to robust classifcation, which is inherently hard to construct, up to computational complexity and a \\n50 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='03ce0b8c-53a3-419f-b55a-f0ea46507585', embedding=None, metadata={'page_label': '51', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nfactor of 2 in the robustness radius. \\nAdversarial examples may be from the same data distribution on which the model is trained \\nand to which it expects the inputs to belong or may be OUT-OF-DISTRIBUTION (OOD) in-\\nputs. Thus, the ability to detect OOD inputs is also an important challenge in AML. Fang et al. [102] established useful theoretical bounds on detectability, particularly an impossibility result when there is an overlap between the in-distribution and OOD data. \\nThe data and model sanitization techniques discussed in Section 2.3 reduce the impact of a \\nrange of poisoning attacks and should be widely used. However, they should be combined with cryptographic techniques for origin and integrity attestation to provide assurances downstream, as recommended in the fnal report of the National Security Commission on AI [220]. \\nAs pointed out in the Introduction, chatbots [70, 83, 206, 227] enabled by recent advances \\nin deep learning have emerged as a powerful technology with great potential for numerous business applications, from entertainment to more critical felds. Recently, specifc attacks using ”\\nPROMPT INJECTIONS ” have emerged as effective ways to trigger bad behaviour in \\nthe bot [306] and imposing guardrails has been the widely used approach to mitigating \\nthese risks. However, recent research has discovered theoretical limits on rigorous LLM censorship [116] that require employing other means of risk mitigation, e.g., setting up \\ncontrolled model gateways and other cybersecurity mechanisms. \\nDespite progress in the ability of chatbots to perform well on certain tasks [227], this technology is still emerging and should only be deployed in applications that require a high degree of trust in the information they generate with abundance of caution and continuous monitoring. \\nAs the development of AI-enabled chatbots continues and their deployment becomes more prevalent online and in business applications, these concerns will come to the forefront and be pursued by adversaries to discover and exploit vulnerabilities and by companies developing the technology to improve their design and implementation to protect against such attacks [354]. The identifcation and mitigation of a variety of risk factors, such as \\nvulnerabilities, include R\\nED TEAMING [56, 109] as part of pre-deployment testing and \\nevaluation of LLM’s. These processes vary and have included testing for traditional cy-bersecurity vulnerabilities, bias, and discrimination, generation of harmful content, privacy violations, and novel or emergent characteristics of large-scale models, as well as evalua-tions of larger societal impacts such as economic impacts, the perpetuation of stereotypes, long-term over-reliance, and erosion of democratic norms [157, 267]. \\nRealistic risk management throughout the entire life cycle of the technology is critically \\nimportant to identify risks and plan early corresponding mitigation approaches [226]. For \\nexample, incorporating human adversarial input in the process of training the system (i.e., \\nR\\nED TEAMING ) or employing reinforcement learning from human feedback appear to of-\\n51 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c5895949-8137-424b-966a-f8c0e5b10003', embedding=None, metadata={'page_label': '52', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nfer benefts in terms of making the chatbot more resilient against toxic input or prompt \\ninjections [83]. However, adapting chatbots to downstream use cases often involves the \\ncustomization of the pre-trained LLM through further fne-tuning, which introduces new safety risks that may degrade the safety alignment of the LLM [242]. Barrett et al. [17] \\nhave developed detailed risk profles for cutting-edge generative AI systems that map well to the NIST AI RMF [226] and should be used for assessing and mitigating potentially \\ncatastrophic risks to society that may arise from this technology. There are also useful industry resources for managing foundation model risks [32]. \\nThe robust training techniques discussed in Section 2.3 offer different approaches to pro-\\nviding theoretically certifed defenses against data poisoning attacks with the intention of providing much-needed information-theoretic guarantees for security. The results are en-couraging, but more research is needed to extend this methodology to more general as-sumptions about the data distributions, the ability to handle OOD inputs, more complex models, multiple data modalities, and better performance. Another challenge is applying these techniques to very large models like LLMs and generative diffusion models, which are quickly becoming targets of attacks [44, 75]. \\nAnother general problem of AML mitigations for both evasion and poisoning attacks is \\nthe lack of reliable benchmarks which causes results from AML papers to be routinely incomparable, as they do not rely on the same assumptions and methods. While there have been some promising developments into this direction [81, 256], more research and \\nencouragement is needed to foster the creation of standardized benchmarks to allow gaining reliable insights into the actual performance of proposed mitigations. \\nFormal methods verifcation has a long history in other felds where high assurance is re-\\nquired, such as avionics and cryptography. The lessons learned there teach us that although the results from applying this methodology are excellent in terms of security and safety assurances, they come at a very high cost, which has prevented formal methods from being widely adopted. Currently, formal methods in these felds are primarily used in applications mandated by regulations. Applying formal methods to neural networks has signifcant po-tential to provide much-needed security guarantees, especially in high-risk applications. However, the viability of this technology will be determined by a combination of techni-cal and business criteria – namely, the ability to handle today’s complex machine learning models of interest at acceptable costs. More research is needed to extend this technology to all algebraic operations used in machine learning algorithms, to scale it up to the large models used today, and to accommodate rapid changes in the code of AI systems while limiting the costs of applying formal verifcation. \\nThere is an imbalance between the large number of privacy attacks listed in Section 2.4 \\n(i.e., memorization, membership inference, model extraction, and property inference) and \\navailable reliable mitigation techniques. In some sense, this is a normal state of affairs: a rapidly evolving technology gaining widespread adoption – even “hype” – which attracts the attention of adversaries, who try to expose and exploit its weaknesses before the tech-\\n52 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ebddab54-0025-4d27-9c4a-645cbb7ca92d', embedding=None, metadata={'page_label': '53', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nnology has matured enough for society to assess and manage it effectively. To be sure, not \\nall adversaries have malevolent intent. Some simply want to warn the public of potential breakdowns that can cause harm and erode trust in the technology. Additionally, not all attacks are as practical as they need to be to pose real threats to AI system deployments of interest. Yet the race between developers and adversaries has begun, and both sides are making great progress. This poses many diffcult questions for the AI community of stakeholders, such as: \\n• What is the best way to mitigate the potential exploits of memorized data from Sec-tion 3.3.1 as models grow and ingest larger amounts of data? \\n• What is the best way to prevent attackers from inferring membership in the training set or other properties of the training data using the attacks listed in Sections 2.4.2 \\nand 2.4.4? \\n• How can developers protect their ML models with the secret weights and associated intellectual property from the emerging threats in the PredAI and GenAI spaces? Especially, attacks that utilize the public API of the ML model to query and exploit its secret weights or the side-channel leakage attacks from Section 2.4.3? The known mechanisms of preventing large numbers of queries through the API are ineffective in confgurations with anonymous or unauthenticated access to the model. \\nAs answers to these questions become available, it is important for the community of stake-holders to develop specifc guidelines to complement the NIST AI RMF [226] for use cases where privacy is of utmost importance. \\n4.3. The\\n  Open vs. Closed Model Dilemma \\nOpen source \\nhas established itself as an indispensable methodology for developing soft-\\nware today. There are many benefts to open source development that have been widely \\nanalysed [170]. \\nFollowing this model and adding valid arguments related to democratizing access, leveling \\nthe playing feld, enabling reproducibility of scientifc results that in turn enables measuring progress in AI, powerful open access models have become available to the public [209, 293, 294]. In many use cases they help to bridge the performance gaps with closed/proprietary \\nmodels [178, 303]. \\nHowever, there are other use cases where putting powerful AI technology in the hands \\nof people with malicious intent would be very concerning [290]. Researchers have already \\ndemonstrated the ease with which open models can be subverted to perform tasks outside of the original intent of the developers [330]. This brings up the question about open models: should they be allowed? \\nThis question has been approached in other felds of science and engineering. For example, \\nsociety has accepted the risks of cryptography falling in the wrong hands and we have \\n53 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f7fe4e8f-ce90-46b6-a791-483565a83bd0', embedding=None, metadata={'page_label': '54', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nstrong cryptographic algorithms publicly available and widely used. In another example, \\nin bio engineering, society has determined that the risks of uncontrolled genetic engineering are too great to allow open access to the technology. \\nThe open vs. closed model dilemma in AI is being actively debated in the community of \\nstakeholders and should be resolved before models become too powerful and make it moot. \\n4.4. Supply\\n  chain challenges \\nThe li\\nterature on AML shows a trend of designing new attacks with higher power and \\nstealthier behavior. The advent of powerful open models [293] and the reported attacks \\nagainst them [330] can induce a behaviour of a model with TROJANS . This illustrates the \\nchallenges for applications using open models downstream the supply chain. To be clear, \\nbackdoor attacks on models is not limited to open models. \\nDARPA jointly with NIST have created a program TrojAI that is researching the defense of \\nAI systems from intentional, malicious Trojan attacks by developing technology to detect these attacks and by investigating what makes the Trojan detection problem challenging. \\nGoldwasser et al. [118] recently introduced a new class of attacks: information-theoretically \\nundetectable Trojans that can be planted in ML models. If proven practical, such attacks may only be prevented or detected and mitigated by procedures that restrict and control who in the organization has access to the model throughout the life cycle and by thor -\\noughly vetting third-party components coming through the supply chain. The NIST AI Risk Management Framework [226] offers more information on this. \\n4.5. T\\nradeofs Between the Attributes of Trustworthy AI \\nThe trustw\\northiness of an AI system depends on all of the attributes that characterize \\nit [226]. For example, an AI system that is accurate but easily susceptible to adversarial \\nexploits is unlikely to be trusted. Similarly, an AI system that produces harmfully biased \\nor unfair outcomes is unlikely to be trusted even if it is robust. There are also trade-offs between explainability and adversarial robustness [140, 207]. In cases where fairness is \\nimportant and privacy is necessary to maintain, the trade-off between privacy and fairness needs to be considered [142]. Unfortunately, it is not possible to simultaneously maximize \\nthe performance of the AI system with respect to these attributes. For instance, AI sys-tems optimized for accuracy alone tend to underperform in terms of adversarial robustness and fairness [59, 91, 244, 301, 342]. Conversely, an AI system optimized for adversarial \\nrobustness may exhibit lower accuracy and deteriorated fairness outcomes [21, 311, 342]. \\nThe full characterization of the trade-offs between the different attributes of trust-worthy AI is still an open research problem that is gaining increasing importance with the adoption of AI technology in many areas of modern life. \\n54 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='37d1f747-5e94-4f62-acfc-3e91ece29329', embedding=None, metadata={'page_label': '55', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nIn most cases, organizations will need to accept trade-offs between these properties and \\ndecide which of them to prioritize depending on the AI system, the use case, and potentially many other considerations about the economic, environmental, social, cultural, political, and global implications of the AI technology [226]. \\n4.6. Multimo\\n dal Models: Are They More Robust? \\nMULTIMODAL MODELS have shown great potential for achieving high performance on \\nmany machine learning tasks [16, 19, 213, 246, 344]. It is natural to assume that because \\nthere is redundancy of information across the different modalities, the model should be \\nmore robust against adversarial perturbations of a single modality. However, emerging ev-idence from practice shows that this is not necessarily the case. Combining modalities and training the model on clean data alone does not seem to improve adversarial robustness. In addition, one of the most effective defenses against evasion attacks based on adversarial training, which is widely used in single modality applications, is prohibitively expensive in practical applications of multimodal learning. Additional effort is required to beneft from the redundant information in order to improve robustness against single modality attacks [328]. Without such an effort, single modality attacks can be effective and compro-mise multimodal models across a wide range of multimodal tasks despite the information contained in the remaining unperturbed modalities [328, 335]. Moreover, researchers have devised effcient mechanisms for constructing simultaneous attacks on multiple modali-ties, which suggests that multimodal models might not be more robust against adversarial attacks despite improved performance [63, 261, 326]. \\nThe existence of simultaneous attacks on multimodal models suggests that miti-gation techniques that only rely on single modality perturbations are not likely to be robust. Attackers in real life do not constrain themselves to attacks within a given security model but employ any attack that is available to them. \\n4.7. Quantized\\n  models \\nQuantization is \\na technique that allows effciently deploying models to edge platforms such \\nas smart phones and IoT devices [114]. It reduces the computational and memory costs of \\nrunning inference on a given platform by representing the model weights and activations with low-precision data types. For example, quantized models typically use 8-bit integers (int8) instead of the usual 32-bit foating point (foat32) numbers for the original non-quantized model. \\nThis technique has been widely used with PredAI and increasingly with GenAI models. \\nHowever, quantized models do inherit the vulnerabilities of the original models and bring in additional weaknesses making such models vulnerable to adversarial attacks. Error am-plifcation resulting from the reduced computational precision affects adversely the adver -\\nsarial robustness of the quantized models. Some pointers to useful mitigation techniques \\n55 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='79ef43bf-8d4e-4b8e-a77e-f1b4e66ebeb7', embedding=None, metadata={'page_label': '56', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nfor PredAI models exist in the literature [179]. The effects of quantization on GenAI mod-\\nels has been studies lees and organizations deploying such models should be careful to \\ncontinuously monitor their behavior. \\n56 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a01c58f7-2549-4c38-851b-8301d70d2572', embedding=None, metadata={'page_label': '57', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nReferences \\n[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, \\nKunal Talwar, and Li Zhang. Deep learning with differential privacy. In ACM Con-\\nference on Computer and Communications Security, CCS ’16, pages 308–318, 2016. https://arxiv.org/abs/1607.00133. \\n[2] Hojjat Aghakhani, Dongyu Meng, Yu-Xiang Wang, Christopher Kruegel, and Gio-vanni Vigna. Bullseye polytope: A scalable clean-label poisoning attack with im-proved transferability. In IEEE European Symposium on Security and Privacy, 2021, \\nVienna, Austria, September 6-10, 2021, pages 159–178. IEEE, 2021. \\n[3] Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine Stochastic Gradient De-scent. In NeurIPS, 2018. \\n[4] Galen Andrew, Peter Kairouz, Sewoong Oh, Alina Oprea, H. Brendan McMahan, and Vinith Suriyakumar. One-shot empirical privacy estimation for federated learn-ing, 2023. \\n[5] Anthropic. Anthropic acceptable use policy, 2023. \\n[6] Anthropic. Model Card and Evaluations for Claude Models. https://www-fles.ant \\nhropic.com/production/images/Model-Card-Claude-2.pdf, July 2023. Anthropic. \\n[7] Giovanni Apruzzese, Hyrum S Anderson, Savino Dambra, David Freeman, Fabio Pierazzi, and Kevin Roundy. “real attackers don’t compute gradients”: Bridging the gap between adversarial ml research and practice. In 2023 IEEE Conference on \\nSecure and Trustworthy Machine Learning (SaTML), pages 339–364. IEEE, 2023. \\n[8] Arthur. Shield, 2023. \\n[9] Giuseppe Ateniese, Luigi V . Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, and Giovanni Felici. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifers. Int. J. Secur. \\nNetw., 10(3):137–150, September 2015. \\n[10] Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In Jen-nifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Con-\\nference on Machine Learning, ICML 2018, Stockholmsm ¨assan, Stockholm, Sweden, \\nJuly 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages \\n274–283. PMLR, 2018. \\n[11] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples, 2018. \\n[12] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Ar -\\ntifcial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning \\nResearch, pages 2938–2948. PMLR, 26–28 Aug 2020. \\n[13] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In AISTATS. PMLR, 2020. \\n57 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='64c81acc-7d0e-4b89-ba11-cf645dc43b64', embedding=None, metadata={'page_label': '58', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n[14] Marieke Bak, Vince Istvan Madai, Marie-Christine Fritzsche, Michaela Th. \\nMayrhofer, and Stuart McLennan. You can’t have ai both ways: Balancing health data privacy and access fairly. Frontiers in Genetics, 13, 2022. https://www.frontier \\nsin.org/articles/10.3389/fgene.2022.929453. \\n[15] Borja Balle, Giovanni Cherubin, and Jamie Hayes. Reconstructing training data with informed adversaries. In NeurIPS 2021 Workshop on Privacy in Machine Learning \\n(PRIML), 2021. \\n[16] Tadas Baltru ˇsaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal ma-\\nchine learning: A survey and taxonomy, 2017. \\n[17] Anthony M. Barrett, Dan Hendrycks, Jessica Newman, and Brandie Nonnecke. UC \\nBerkeley AI Risk-Management Standards Profle for General-Purpose AI Systems (GPAIS) and Foundation Models. UC Berkeley Center for Long Term Cybersecurity, 2023. https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-st \\nandards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/. \\n[18] Lejla Batina, Shivam Bhasin, Dirmanto Jap, and Stjepan Picek. CSI NN: Reverse engineering of neural network architectures through electromagnetic side channel. In Proceedings of the 28th USENIX Conference on Security Symposium, SEC’19, \\npage 515–532, USA, 2019. USENIX Association. \\n[19] Khaled Bayoudh, Raja Knani, Fayc ¸al Hamdaoui, and Abdellatif Mtibaa. A survey on deep multimodal learning for computer vision: Advances, trends, applications, and datasets. Vis. Comput., 38(8):2939–2970, August 2022. \\n[20] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023. \\n[21] Philipp Benz, Chaoning Zhang, Soomin Ham, Gyusang Karjauv, Adil Cho, and In So Kweon. The triangular trade-off between accuracy, robustness, and fairness. Workshop on Adversarial Machine Learning in Real-World Computer Vision Sys-tems and Online Challenges (AML-CV) at CVPR, 2021. \\n[22] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alch ´e-Buc, E. Fox, and R. Garnett, \\neditors, Advances in Neural Information Processing Systems 32, pages 5050–5060. \\nCurran Associates, Inc., 2019. \\n[23] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Model Poisoning Attacks in Federated Learning. In NeurIPS SECML, 2018. \\n[24] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. An-alyzing federated learning through an adversarial lens. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on \\nMachine Learning, volume 97 of Proceedings of Machine Learning Research, pages \\n634–643. PMLR, 09–15 Jun 2019. \\n[25] Battista Biggio, Igino Corona, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli. Bagging classifers for fghting poisoning attacks in adversarial classifcation tasks. \\n58 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='005ec0e8-955d-4808-a4ec-9c3c8a63739e', embedding=None, metadata={'page_label': '59', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nIn Proceedings of the 10th International Conference on Multiple Classifer Systems, \\nMCS’11, page 350–359, Berlin, Heidelberg, 2011. Springer-Verlag. \\n[26] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim ˇ c, Pavel Srndi ´ \\nLaskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning \\nat test time. In Joint European conference on machine learning and knowledge \\ndiscovery in databases, pages 387–402. Springer, 2013. \\n[27] Battista Biggio, Blaine Nelson, and Pavel Laskov. Support vector machines under adversarial label noise. In Chun-Nan Hsu and Wee Sun Lee, editors, Proceedings of \\nthe Asian Conference on Machine Learning, volume 20 of Proceedings of Machine \\nLearning Research , pages 97–112, South Garden Hotels and Resorts, Taoyuan, Tai-\\nwain, 14–15 Nov 2011. PMLR. \\n[28] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. In Proceedings of the 29th International Coference on Interna-\\ntional Conference on Machine Learning, ICML, 2012. \\n[29] Battista Biggio, Konrad Rieck, Davide Ariu, Christian Wressnegger, Igino Corona, Giorgio Giacinto, and Fabio Roli. Poisoning behavioral malware clustering. In Proceedings of the 2014 Workshop on Artifcial Intelligent and Security Workshop, AISec ’14, page 27–36, New York, NY , USA, 2014. Association for Computing Machinery. \\n[30] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition, 84:317–331, December 2018. \\n[31] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Ma-chine Learning with Adversaries: Byzantine Tolerant Gradient Descent. In NeurIPS, \\n2017. \\n[32] IBM AI Ethics Board. Foundation models: Opportunities, risks and mitigations. https://dataplatform.cloud.ibm.com/docs/content/wsj/ai-risk-atlas/ai-risk-atlas.ht ml?context=wx&audience=wdp, 2023. \\n[33] Nicholas Boucher, Ilia Shumailov, Ross Anderson, and Nicolas Papernot. Bad char -\\nacters: Imperceptible NLP attacks. arXiv preprint arXiv:2106.09898, 2021. \\n[34] Lucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine un-learning. In 42nd IEEE Symposium on Security and Privacy, SP 2021, San Fran-\\ncisco, CA, USA, 24-27 May 2021, pages 141–159. IEEE, 2021. \\n[35] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. In 6th In-\\nternational Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, \\n2018. \\n[36] Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar. When is memorization of irrelevant training data necessary for high-accuracy learning? In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2021, page 123–132, New York, NY , USA, 2021. Association for Computing \\n59 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='eeda46c0-4706-4653-aa00-b99baa0797d3', embedding=None, metadata={'page_label': '60', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nMachinery. \\n[37] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information pro-\\ncessing systems, 33:1877–1901, 2020. \\n[38] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-fulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christo-pher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, \\n2020. \\n[39] Gon Buzaglo, Niv Haim, Gilad Yehudai, Gal Vardi, and Michal Irani. Reconstruct-ing training data from multiclass neural networks, 2023. \\n[40] Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. FLTrust: Byzantine-robust federated learning via trust bootstrapping. In NDSS, 2021. \\n[41] Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine un-learning. In 2015 IEEE Symposium on Security and Privacy, pages 463–480, 2015. \\n[42] Nicholas Carlini. Poisoning the unlabeled dataset of Semi-Supervised learning. In 30th USENIX Security Symposium (USENIX Security 21) , pages 1577–1592. \\nUSENIX Association, August 2021. \\n[43] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Flo-rian Tramer. Membership inference attacks from frst principles. In 2022 IEEE \\nSymposium on Security and Privacy (S&P), pages 1519–1519, Los Alamitos, CA, USA, May 2022. IEEE Computer Society. \\n[44] Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Flo-rian Tram `er, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data \\nfrom diffusion models, 2023. \\n[45] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. https://arxiv.org/abs/2202.07646, 2022. \\n[46] Nicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tramer. ` \\nPoisoning web-scale training datasets is practical. arXiv preprint arXiv:2302.10149, \\n2023. \\n[47] Nicholas Carlini, Matthew Jagielski, and Ilya Mironov. Cryptanalytic extraction of neural network models. In Daniele Micciancio and Thomas Ristenpart, editors, Advances in Cryptology – CRYPTO 2020, pages 189–218, Cham, 2020. Springer International Publishing. \\n´ [48] Nicholas Carlini, Chang Liu, Ulfar Erlingsson, Jernej Kos, and Dawn Song. The Secret Sharer: Evaluating and testing unintended memorization in neural networks. \\n60 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5b5625b3-07ac-4061-8101-015bf490700c', embedding=None, metadata={'page_label': '61', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nIn USENIX Security Symposium, USENIX ’19), pages 267–284, 2019. https://arxiv. \\norg/abs/1802.08232. \\n[49] Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, \\nIrena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, et al. Are aligned neural networks adversarially aligned? arXiv preprint \\narXiv:2306.15447, 2023. \\n[50] Nicholas Carlini, Florian Tramer, Krishnamurthy Dj Dvijotham, Leslie Rice, Mingjie Sun, and J. Zico Kolter. (certifed!!) adversarial robustness for free!, 2023. \\n[51] Nicholas Carlini, Florian Tram `er, Eric Wallace, Matthew Jagielski, Ariel Herbert-\\n´ V oss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language mod-els. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633–2650. \\nUSENIX Association, August 2021. \\n[52] Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on \\nArtifcial Intelligence and Security, AISec ’17, page 3–14, New York, NY , USA, 2017. Association for Computing Machinery. \\n[53] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Proc. IEEE Security and Privacy Symposium, 2017. \\n[54] Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-to-text. In 2018 IEEE Security and Privacy Workshops (SPW), pages 1–7. \\nIEEE, 2018. \\n[55] Defense Use Case. Analysis of the cyber attack on the Ukrainian power grid. Elec-\\ntricity Information Sharing and Analysis Center (E-ISAC), 388:1–29, 2016. \\n[56] Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfeld-Menell. Explore, establish, exploit: Red teaming language models from scratch, 2023. \\n[57] National Cyber Security Center. Introducing our new machine learning security principles, retrieved February 2023 from https://www.ncsc.gov.uk/blog-post/introd ucing-our-new-machine-learning-security-principles. \\n[58] Varun Chandrasekaran, Kamalika Chaudhuri, Irene Giacomelli, Somesh Jha, and Songbai Yan. Exploring connections between active learning and model extraction. In Proceedings of the 29th USENIX Conference on Security Symposium, SEC’20, \\nUSA, 2020. USENIX Association. \\n[59] Hong Chang, Ta Duy Nguyen, Sasi Kumar Murakonda, Ehsan Kazemi, and R. Shokri. On adversarial bias and the robustness of fair machine learning. https: \\n//arxiv.org/abs/2006.08669, 2020. \\n[60] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023. \\n[61] Harsh Chaudhari, John Abascal, Alina Oprea, Matthew Jagielski, Florian Tramer, ` \\nand Jonathan Ullman. SNAP: Effcient extraction of private properties with poison-ing. In 2023 IEEE Symposium on Security and Privacy (S&P), 2023. \\n61 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='60adcfe0-1adc-451a-8982-e60d3a221aae', embedding=None, metadata={'page_label': '62', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n[62] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Ed-\\nwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. https://arxiv.org/abs/1811.03728, \\n2018. \\n[63] Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui Hsieh. Attacking visual language grounding with adversarial examples: A case study on neural image captioning. https://arxiv.org/abs/1712.02051, 2017. \\n[64] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. DeepInspect: A black-box trojan detection and mitigation framework for deep neural networks. In Proceed-\\nings of the Twenty-Eighth International Joint Conference on Artifcial Intelligence, IJCAI-19, pages 4658–4664. International Joint Conferences on Artifcial Intelli-gence Organization, 7 2019. \\n[65] Jianbo Chen, Michael I. Jordan, and Martin J. Wainwright. HopSkipJumpAttack: A query-effcient decision-based attack. In 2020 IEEE Symposium on Security and \\nPrivacy, SP 2020, San Francisco, CA, USA, May 18-21, 2020, pages 1277–1294. IEEE, 2020. \\n[66] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Ze-roth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artif-\\ncial Intelligence and Security, AISec ’17, page 15–26, New York, NY , USA, 2017. Association for Computing Machinery. \\n[67] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Chau. ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detec-tor, page 52–68. Springer International Publishing, 2019. \\n[68] Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, and Yang Zhang. Badnl: Backdoor attacks against nlp models with semantic-preserving improvements. In Annual Computer Security Applications \\nConference, ACSAC ’21, page 554–569, New York, NY , USA, 2021. Association for Computing Machinery. \\n[69] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint \\narXiv:1712.05526, 2017. \\n[70] Heng-Tze Cheng and Romal Thoppilan. LaMDA: Towards Safe, Grounded, and High-Quality Dialog Models for Everything. https://ai.googleblog.com/2022/01/la \\nmda-towards-safe-grounded-and-high.html, 2022. Google Brain. \\n[71] Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Query-effcient hard-label black-box attack: An optimization-based ap-proach. In 7th International Conference on Learning Representations, ICLR 2019, \\nNew Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. \\n[72] Minhao Cheng, Simranjit Singh, Patrick H. Chen, Pin-Yu Chen, Sijia Liu, and Cho-Jui Hsieh. Sign-opt: A query-effcient hard-label adversarial attack. In International \\nConference on Learning Representations, 2020. \\n62 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c8c4c4d8-fc08-4f93-a58c-ae042ff506f8', embedding=None, metadata={'page_label': '63', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n[73] Alesia Chernikova and Alina Oprea. FENCE: Feasible evasion attacks on neural \\nnetworks in constrained environments. ACM Transactions on Privacy and Security \\n(TOPS) Journal, 2022. \\n[74] Christopher A. Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Pa-pernot. Label-only membership inference attacks. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, vol-\\nume 139 of Proceedings of Machine Learning Research, pages 1964–1974. PMLR, \\n18–24 Jul 2021. \\n[75] Sheng-Yen Chou, Pin-Yu Chen, and Tsung-Yi Ho. How to backdoor diffusion mod-els? https://arxiv.org/abs/2212.05400, 2022. \\n[76] Antonio Emanuele Cina, ` Kathrin Grosse, Ambra Demontis, Sebastiano Vascon, \\nWerner Zellinger, Bernhard A. Moser, Alina Oprea, Battista Biggio, Marcello Pelillo, and Fabio Roli. Wild patterns reloaded: A survey of machine learning secu-rity against training data poisoning. ACM Computing Surveys, March 2023. \\n[77] Jack Clark and Raymond Perrault. 2022 AI index report. https://aiindex.stanford.e \\ndu/wp-content/uploads/2022/03/2022-AI-Index-Report Master.pdf, 2022. Human \\nCentered AI, Stanford University. \\n[78] Joseph Clements, Yuzhe Yang, Ankur Sharma, Hongxin Hu, and Yingjie Lao. Ral-lying adversarial techniques against deep learning for network security, 2019. \\n[79] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certifed adversarial robustness via randomized smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 1310–1320. PMLR, 09–15 \\nJun 2019. \\n[80] Committee on National Security Systems. Committee on National Security Systems (CNSS) Glossary. https://rmf.org/wp-content/uploads/2017/10/CNSSI-4009.pdf, \\n2015. \\n[81] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robust-bench: a standardized adversarial robustness benchmark. In Thirty-ffth Conference \\non Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. \\n[82] Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. Ad-versarial classifcation. In Proceedings of the Tenth ACM SIGKDD International \\nConference on Knowledge Discovery and Data Mining, KDD ’04, page 99–108, New York, NY , USA, 2004. Association for Computing Machinery. \\n[83] DeepMind. Building safer dialogue agents. https://www.deepmind.com/blog/buildi \\nng-safer-dialogue-agents, 2022. Online. \\n[84] Luca Demetrio, Battista Biggio, Giovanni Lagorio, Fabio Roli, and Alessandro Ar -\\nmando. Functionality-preserving black-box optimization of adversarial windows malware. IEEE Transactions on Information Forensics and Security, 16:3469–3478, \\n2021. \\n63 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='71061f9c-450f-480d-8ee2-a6b220bc210b', embedding=None, metadata={'page_label': '64', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n[85] Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio, \\nAlina Oprea, Cristina Nita-Rotaru, and Fabio Roli. Why do adversarial attacks trans-fer? Explaining transferability of evasion and poisoning attacks. In 28th USENIX \\nSecurity Symposium (USENIX Security 19), pages 321–338. USENIX Association, 2019. \\n[86] Serguei Denissov, Hugh Brendan McMahan, J Keith Rush, Adam Smith, and Abhradeep Guha Thakurta. Improved differential privacy for SGD via optimal pri-vate linear operators on adaptive streams. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing \\nSystems, 2022. \\n[87] Ben Derico. Chatgpt bug leaked users’ conversation histories, 2023. \\n[88] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. In International Conference on Machine Learning, pages 1596–1606. PMLR, 2019. \\n[89] Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy. In Proceedings of the 22nd ACM Symposium on Principles of Database Systems, PODS ’03, pages 202–210. ACM, 2003. \\n[90] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ArXiv, abs/2010.11929, \\n2021. \\n[91] Sanghamitra Dutta, Dennis Wei, Hazar Yueksel, Pin-Yu Chen, Sijia Liu, and Kush R. Varshney. Is there a trade-off between fairness and accuracy? A perspective using mismatched hypothesis testing. In Proceedings of the 37th International Conference \\non Machine Learning, ICML’20. JMLR.org, 2020. \\n[92] Cynthia Dwork. Differential privacy. In Automata, Languages and Programming, \\n33rd International Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006, Pro-ceedings, Part II, pages 1–12, 2006. \\n[93] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Conference on Theory of Cryptography, \\nTCC ’06, pages 265–284, New York, NY , USA, 2006. \\n[94] Cynthia Dwork, Adam Smith, Thomas Steinke, and Jonathan Ullman. Exposed! A survey of attacks on private data. Annual Review of Statistics and Its Application, \\n4:61–84, 2017. \\n[95] Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ullman, and Salil Vadhan. Robust traceability from trace amounts. In IEEE Symposium on Foundations of \\nComputer Science, FOCS ’15, 2015. \\n[96] Cynthia Dwork and Sergey Yekhanin. New effcient attacks on statistical disclosure control mechanisms. In Annual International Cryptology Conference, pages 469– \\n480. Springer, 2008. \\n[97] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotfip: White-box ad-\\n64 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3572315b-1030-4b5b-b18b-1ad6ca76ab96', embedding=None, metadata={'page_label': '65', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"NIST AI 100-2e2023 \\nJanuary 2024 \\nversarial examples for text classifcation. arXiv preprint arXiv:1712.06751, 2017. \\n[98] ETSI Group Report SAI 005. Securing artifcial intelligence (SAI); mitigation strat-\\negy report, retrieved February 2023 from https://www.etsi.org/deliver/etsi gr/SAI/ 001 099/005/01.01.01 60/gr SAI005v010101p.pdf. \\n[99] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classifcation. In 2018 IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition, pages 1625–1634, 2018. \\n[100] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world at-tacks on deep learning visual classifcation. In 2018 IEEE Conference on Computer \\nVision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 1625–1634. Computer Vision Foundation / IEEE Computer Society, 2018. \\n[101] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. Local Model Poisoning Attacks to Byzantine-Robust Federated Learning. In USENIX Security, \\n2020. \\n[102] Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo Han, and Feng Liu. Is out-of-distribution detection learnable? In Proceedings of the 36th Conference on Neural \\nInformation Processing Systems (NeurIPS 2022). online: https://arxiv.org/abs/2210 \\n.14707, 2022. \\n[103] Vitaly Feldman. Does learning require memorization? A short tale about a long tail. In ACM Symposium on Theory of Computing, STOC ’20, pages 954–959, 2020. \\nhttps://arxiv.org/abs/1906.05271. \\n[104] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via infuence estimation. In Proceedings of the 34th In-\\nternational Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. \\n[105] Ji Feng, Qi-Zhi Cai, and Zhi-Hua Zhou. Learning to confuse: Generating training time adversarial data with auto-encoder. In H. Wallach, H. Larochelle, A. Beygelz-imer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Informa-\\ntion Processing Systems, volume 32. Curran Associates, Inc., 2019. \\n[106] Liam Fowl, Ping-yeh Chiang, Micah Goldblum, Jonas Geiping, Arpit Bansal, Wo-jtek Czaja, and Tom Goldstein. Preventing unauthorized use of proprietary data: Poisoning for secure dataset release, 2021. \\n[107] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confdence information and basic countermeasures. In Proceedings of the \\n22nd ACM SIGSAC Conference on Computer and Communications Security, CCS ’15, page 1322–1333, New York, NY , USA, 2015. Association for Computing Ma-chinery. \\n[108] Aymeric Fromherz, Klas Leino, Matt Fredrikson, Bryan Parno, and Corina Pasare-anu. Fast geometric projections for local robustness certifcation. In International \\n65 \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2e322007-f838-4b2a-83d6-a0b0c4cd840f', embedding=None, metadata={'page_label': '66', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nConference on Learning Representations, 2021. \\n[109] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav \\nKadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson El-hage, Sheer El-Showk, Stanislav Fort, Zac Hatfeld-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Cather -\\nine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022. \\n[110] Karan Ganju, Qi Wang, Wei Yang, Carl A. Gunter, and Nikita Borisov. Property inference attacks on fully connected neural networks using permutation invariant representations. In Proceedings of the 2018 ACM SIGSAC Conference on Computer \\nand Communications Security, CCS ’18, page 619–633, New York, NY , USA, 2018. Association for Computing Machinery. \\n[111] Simson Garfnkel, John Abowd, and Christian Martindale. Understanding database reconstruction attacks on public data. Communications of the ACM, 62:46–53, 02 \\n2019. \\n[112] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. AI2: Safety and robustness certifcation of neu-ral networks with abstract interpretation. In 2018 IEEE Symposium on Security and \\nPrivacy (S&P), pages 3–18, 2018. \\n[113] Jonas Geiping, Liam H Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, and Tom Goldstein. Witches’ brew: Industrial scale data poisoning via gradient matching. In International Conference on Learning Representations, \\n2021. \\n[114] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer. A survey of quantization methods for effcient neural network infer -\\nence, 2021. \\n[115] Antonio A. Ginart, Melody Y . Guan, Gregory Valiant, and James Zou. Making ai forget you: Data deletion in machine learning. In Proceedings of the 33rd Interna-\\ntional Conference on Neural Information Processing Systems, Red Hook, NY , USA, 2019. Curran Associates Inc. \\n[116] David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, and Vardan Papyan. LLM censorship: A machine learning challenge or a computer security problem?, 2023. \\n[117] Micah Goldblum, Avi Schwarzschild, Ankit Patel, and Tom Goldstein. Adversarial attacks on machine learning systems for high-frequency trading. In Proceedings of \\nthe Second ACM International Conference on AI in Finance, ICAIF ’21, New York, NY , USA, 2021. Association for Computing Machinery. \\n[118] Shaf Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, and Or Zamir. Planting undetectable backdoors in machine learning models. https://arxiv.org/abs/2204.069 \\n66 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ebbb2647-063b-4cc2-b2ef-297b923a3d56', embedding=None, metadata={'page_label': '67', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n74, 2022. arXiv. \\n[119] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, \\n2016. http://www.deeplearningbook.org. \\n[120] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing \\nadversarial examples. In International Conference on Learning Representations, \\n2015. \\n[121] Google. Generative ai prohibited use policy, 2023. \\n[122] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what you signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. arXiv preprint \\narXiv:2302.12173, 2023. \\n[123] Shane Griffth, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and An-drea L Thomaz. Policy shaping: Integrating human feedback with reinforcement learning. Advances in neural information processing systems, 26, 2013. \\n[124] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. BadNets: Evalu-ating backdooring attacks on deep neural networks. IEEE Access, 7:47230–47244, \\n2019. \\n[125] Rachid Guerraoui, Arsany Guirguis, J ´er´emy Plassmann, Anton Ragot, and S ´ebastien \\nRouault. Garfeld: System support for byzantine machine learning (regular paper). In DSN. IEEE, 2021. \\n[126] Chuan Guo, Alexandre Sablayrolles, Herve ´ J´ Gradient- egou, and Douwe Kiela. \\nbased adversarial attacks against text transformers. In Proceedings of the 2021 \\nConference on Empirical Methods in Natural Language Processing, pages 5747– 5757, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. \\n[127] Chuan Guo, Alexandre Sablayrolles, e egou, and Douwe Kiela. Herv ´ J´ \\nGradient-based adversarial attacks against text transformers. arXiv preprint \\narXiv:2104.13733, 2021. \\n[128] Niv Haim, Gal Vardi, Gilad Yehudai, michal Irani, and Ohad Shamir. Reconstructing training data from trained neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing \\nSystems, 2022. \\n[129] Jonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. SPECTRE: Defending against backdoor attacks using robust statistics. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine \\nLearning, volume 139 of Proceedings of Machine Learning Research, pages 4129– \\n4139. PMLR, 18–24 Jul 2021. \\n[130] Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John V Pearson, Dietrich A Stephan, Stanley F Nelson, and David W Craig. Resolving individuals contributing trace amounts of DNA to highly com-plex mixtures using high-density SNP genotyping microarrays. PLoS genetics, \\n4(8):e1000167, 2008. \\n67 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d7a8150d-7a7e-4756-ae68-5c56b4e6ada7', embedding=None, metadata={'page_label': '68', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"NIST AI 100-2e2023 \\nJanuary 2024 \\n[131] Xiaoling Hu, Xiao Lin, Michael Cogswell, Yi Yao, Susmit Jha, and Chao Chen. \\nTrigger hunting with a topological prior for trojan detection. In International Con-\\nference on Learning Representations, 2022. \\n[132] Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are large pre-trained lan-guage models leaking your personal information? arXiv preprint arXiv:2205.12628, \\n2022. \\n[133] W. Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein. Metapoison: Practical general-purpose clean-label data poisoning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural In-\\nformation Processing Systems, volume 33, pages 12080–12091. Curran Associates, Inc., 2020. \\n[134] Xijie Huang, Moustafa Alzantot, and Mani Srivastava. NeuronInspect: Detecting backdoors in neural networks via output explanations, 2019. \\n[135] W. Nicholson Price II. Risks and remedies for artifcial intelligence in health care. https://www.brookings.edu/research/risks-and-remedies-for-artifcial-intelligence-i n-health-care/, 2019. Brookings Report. \\n[136] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adver -\\nsarial attacks with limited queries and information. In Jennifer G. Dy and An-dreas Krause, editors, Proceedings of the 35th International Conference on Ma-\\nchine Learning, ICML 2018, Stockholmsm ¨assan, Stockholm, Sweden, July 10-15, \\n2018, volume 80 of Proceedings of Machine Learning Research, pages 2142–2151. \\nPMLR, 2018. \\n[137] Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial attacks with bandits and priors. In International Conference on \\nLearning Representations, 2019. \\n[138] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett, \\neditors, Advances in Neural Information Processing Systems, volume 32. Curran \\nAssociates, Inc., 2019. \\n[139] Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. Approxi-mate data deletion from machine learning models. In Arindam Banerjee and Kenji Fukumizu, editors, The 24th International Conference on Artifcial Intelligence and \\nStatistics, AISTATS 2021, April 13-15, 2021, Virtual Event , volume 130 of Proceed-\\nings of Machine Learning Research, pages 2008–2016. PMLR, 2021. \\n[140] Shahin Jabbari, Han-Ching Ou, Himabindu Lakkaraju, and Milind Tambe. An em-pirical study of the trade-offs between interpretability and fairness. In ICML Work-\\nshop on Human Interpretability in Machine Learning, International Conference on Machine Learning (ICML), 2020. \\n[141] Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas Papernot. High accuracy and high fdelity extraction of neural networks. In Pro-\\nceedings of the 29th USENIX Conference on Security Symposium, SEC’20, USA, \\n68 \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8222cf9e-8ad9-4ad1-b0e7-bc6540078a2c', embedding=None, metadata={'page_label': '69', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n2020. USENIX Association. \\n[142] Matthew Jagielski, Michael Kearns, Jieming Mao, Alina Oprea, Aaron Roth, \\nSaeed Sharif Malvajerdi, and Jonathan Ullman. Differentially private fair learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th \\nInternational Conference on Machine Learning, Proceedings of Machine Learning Research, pages 3000–3008. PMLR, 2019. \\n[143] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In IEEE Symposium on Security and Privacy (S&P), pages \\n19–35, 2018. \\n[144] Matthew Jagielski, Giorgio Severi, Niklas Pousette Harger, and Alina Oprea. Sub-population data poisoning attacks. In Proceedings of the ACM Conference on Com-\\nputer and Communications Security, CCS, 2021. \\n[145] Matthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially pri-vate machine learning: How private is private SGD? In Advances in Neural Infor -\\nmation Processing Systems, volume 33, pages 22205–22216, 2020. \\n[146] Bargav Jayaraman and David Evans. Evaluating differentially private machine learn-ing in practice. In Proceedings of the 28th USENIX Conference on Security Sympo-\\nsium, SEC’19, page 1895–1912, USA, 2019. USENIX Association. \\n[147] Bargav Jayaraman and David Evans. Are attribute inference attacks just imputation? In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communi-\\ncations Security, CCS ’22, page 1569–1582, New York, NY , USA, 2022. Associa-tion for Computing Machinery. \\n[148] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852, 2023. \\n[149] Robin Jia and Percy Liang. Adversarial examples for evaluating reading compre-hension systems. In Proceedings of the 2017 Conference on Empirical Methods in \\nNatural Language Processing, pages 2021–2031, Copenhagen, Denmark, Septem-ber 2017. Association for Computational Linguistics. \\n[150] Pengfei Jing, Qiyi Tang, Yuefeng Du, Lei Xue, Xiapu Luo, Ting Wang, Sen Nie, and Shi Wu. Too good to be safe: Tricking lane detection in autonomous driving with crafted perturbations. In 30th USENIX Security Symposium (USENIX Security 21) , \\npages 3237–3254. USENIX Association, August 2021. \\n[151] Peter Kairouz, Brendan Mcmahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, and Zheng Xu. Practical and private (deep) learning without sampling or shuffing. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th Inter -\\nnational Conference on Machine Learning , volume 139 of Proceedings of Machine \\nLearning Research, pages 5213–5225. PMLR, 18–24 Jul 2021. \\n[152] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur ´elien Bellet, Mehdi Ben-\\nnis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Hubert Eichner, Salim El Rouayheb, \\n69 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='579063a4-eb6b-42fa-865d-615e2b494392', embedding=None, metadata={'page_label': '70', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nDavid Evans, Josh Gardner, Zachary Garrett, Adri ` on, Badih Ghazi, Phillip B. a Gasc ´ \\nGibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, \\nBen Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Kone ˇcn´y, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancr `ede \\n¨ Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozg ¨ur, \\nRasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Flo-rian Tram `er, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, \\nFelix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning, 2019. \\n[153] Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. Exploiting programmatic behavior of llms: Dual-use through standard security attacks. arXiv preprint arXiv:2302.05733, 2023. \\n[154] Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex: An effcient SMT solver for verifying deep neural networks. In Rupak Majumdar and Viktor Kun ˇcak, editors, Computer Aided Verifcation, pages 97–117, \\nCham, 2017. Springer International Publishing. \\n[155] Michael Kearns and Ming Li. Learning in the presence of malicious errors. In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, STOC ’88, page 267–280, New York, NY , USA, 1988. Association for Computing Machinery. \\n[156] Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, and Aleksander Madry. Rethinking backdoor attacks. In An-dreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference \\non Machine Learning, volume 202 of Proceedings of Machine Learning Research, \\npages 16216–16236. PMLR, 23–29 Jul 2023. \\n[157] Megan Kinniment, Lucas Jun Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan, Luke Harold Miles, Tao R. Lin, Hjalmar Wijk, Joel Burget, Aaron Ho, Elizabeth Barnes, and Paul Christiano. Evaluating Language-Model Agents on Realistic Autonomous Tasks. https://evals.alignment.org/Evaluating LMAs Realist \\nic Tasks.pdf, 2023. \\n[158] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models, 2023. \\n[159] Marius Kloft and Pavel Laskov. Security analysis of online centroid anomaly detec-tion. Journal of Machine Learning Research, 13(118):3681–3724, 2012. \\n[160] Pang Wei Koh and Percy Liang. Understanding black-box predictions via infu-ence functions. In Proceedings of the 34th International Conference on Machine \\nLearning-Volume 70, pages 1885–1894. JMLR. org, 2017. \\n[161] Moshe Kravchik, Battista Biggio, and Asaf Shabtai. Poisoning attacks on cyber attack detectors for industrial control systems. In Proceedings of the 36th Annual \\nACM Symposium on Applied Computing, SAC ’21, page 116–125, New York, NY , \\n70 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='24f88c5a-efb8-4445-b066-f70e14d49cf3', embedding=None, metadata={'page_label': '71', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nUSA, 2021. Association for Computing Machinery. \\n[162] Ram Shankar Siva Kumar, Magnus Nystr ¨om, John Lambert, Andrew Marshall, \\nMario Goertzel, Andi Comissoneru, Matt Swann, and Sharon Xia. Adversarial ma-\\nchine learning – industry perspectives. https://arxiv.org/abs/2002.05646, 2020. \\n[163] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. https://arxiv.org/abs/1607.02533, 2016. \\n[164] E. La Malfa and M. Kwiatkowska. The king is naked: On the notion of robustness for natural language processing. In Proceedings of the Thirty-Sixth AAAI Conference on \\nArtifcial Intelligence, volume 10, page 11047–57. Association for the Advancement of Artifcial Intelligence, 2022. \\n[165] Ricky Laishram and Vir Virander Phoha. Curie: A method for protecting SVM classifer from poisoning attack. CoRR, abs/1606.01584, 2016. \\n[166] Lakera. Guard, 2023. \\n[167] Ralph Langner. Stuxnet: Dissecting a cyberwarfare weapon. IEEE Security & Pri-\\nvacy, 9(3):49–51, 2011. \\n[168] Learn Prompting. Defensive measures, 2023. \\n[169] Mathias L ´ecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman \\nJana. Certifed robustness to adversarial examples with differential privacy. In 2019 \\nIEEE Symposium on Security and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019, pages 656–672. IEEE, 2019. \\n[170] Lee Congdon. 8 advantages of using open source in the enterprise. https://enterpri \\nsersproject.com/article/2015/1/top-advantages-open-source-offers-over-proprietar y-solutions, February 2015. The Enterprisers Project. \\n[171] Klas Leino and Matt Fredrikson. Stolen memories: Leveraging model memorization for calibrated white-box membership inference. In Proceedings of the 29th USENIX \\nConference on Security Symposium, SEC’20, USA, 2020. USENIX Association. \\n[172] Alexander Levine and Soheil Feizi. Deep partition aggregation: Provable defenses against general poisoning attacks. In 9th International Conference on Learning Rep-\\nresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, \\n2021. \\n[173] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K ¨ aschel, Se- uttler, Mike Lewis, Wen tau Yih, Tim Rockt ¨ \\nbastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. \\n[174] Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, and Yangqiu Song. Privacy in large language models: Attacks, defenses and future directions. arXiv preprint arXiv:2310.10383, 2023. \\n[175] Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, and Jialiang Lu. Hidden backdoors in human-centric language models. In Yong-dae Kim, Jong Kim, Giovanni Vigna, and Elaine Shi, editors, CCS ’21: 2021 ACM \\nSIGSAC Conference on Computer and Communications Security, Virtual Event, Re-public of Korea, November 15 - 19, 2021, pages 3123–3140. ACM, 2021. \\n71 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c32cdacf-c790-4a24-a993-0e804c73ad7d', embedding=None, metadata={'page_label': '72', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n[176] Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. \\nInvisible backdoor attacks on deep neural networks via steganography and regular -\\nization. IEEE Transactions on Dependable and Secure Computing, 18:2088–2105, \\n2021. \\n[177] Shasha Li, Ajaya Neupane, Sujoy Paul, Chengyu Song, Srikanth V . Krishnamurthy, Amit K. Roy-Chowdhury, and Ananthram Swami. Adversarial perturbations against real-time video classifcation systems. CoRR, abs/1807.00458, 2018. \\n[178] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. \\n[179] Ji Lin, Chuang Gan, and Song Han. Defensive quantization: When effciency meets robustness. ArXiv, abs/1904.08444, 2019. \\n[180] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In Michael Bailey, Sotiris Ioannidis, Manolis Stamatogiannakis, and Thorsten Holz, editors, Research in At-\\ntacks, Intrusions, and Defenses - 21st International Symposium, RAID 2018, Pro-ceedings, Lecture Notes in Computer Science, pages 273–294. Springer Verlag, 2018. \\n[181] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable ad-versarial examples and black-box attacks. In International Conference on Learning \\nRepresentations, 2017. \\n[182] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. Prompt injection attack against llm-integrated applications. arXiv preprint arXiv:2306.05499, 2023. \\n[183] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint arXiv:2305.13860, 2023. \\n[184] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xi-angyu Zhang. ABS: Scanning neural networks for back-doors by artifcial brain stimulation. In Proceedings of the 2019 ACM SIGSAC Conference on Computer \\nand Communications Security, CCS ’19, page 1265–1282, New York, NY , USA, 2019. Association for Computing Machinery. \\n[185] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. In NDSS. The Internet \\nSociety, 2018. \\n[186] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Refection backdoor: A natural backdoor attack on deep neural networks. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision – ECCV 2020, pages 182– \\n199, Cham, 2020. Springer International Publishing. \\n[187] Martin Bertran Lopez, Shuai Tang, Michael Kearns, Jamie Morgenstern, Aaron Roth, and Zhiwei Steven Wu. Scalable membership inference attacks via quantile regression. In NeurIPS 2023, 2023. \\n72 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b610cb45-5040-44db-9789-d40bb6ef3030', embedding=None, metadata={'page_label': '73', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n[188] Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of the \\nEleventh ACM SIGKDD International Conference on Knowledge Discovery in Data \\nMining, KDD ’05, page 641–647, New York, NY , USA, 2005. Association for Com-puting Machinery. \\n[189] Jiajun Lu, Hussein Sibai, Evan Fabry, and David Forsyth. No need to worry about adversarial examples in object detection in autonomous vehicles, 2017. \\n[190] Yiwei Lu, Gautam Kamath, and Yaoliang Yu. Indiscriminate data poisoning attacks on neural networks. https://arxiv.org/abs/2204.09092, 2022. \\n[191] Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and Santi-ago Zanella-B ´eguelin. Analyzing leakage of personally identifable information in \\nlanguage models. In 2023 IEEE Symposium on Security and Privacy (SP), pages \\n346–363. IEEE Computer Society, 2023. \\n[192] Yuzhe Ma, Xiaojin Zhu, and Justin Hsu. Data poisoning against differentially-private learners: Attacks and defenses. In In Proceedings of the 28th International Joint \\nConference on Artifcial Intelligence (IJCAI), 2019. \\n[193] Pooria Madani and Natalija Vlajic. Robustness of deep autoencoder in intrusion detection under adversarial contamination. In HoTSoS ’18: Proceedings of the 5th \\nAnnual Symposium and Bootcamp on Hot Topics in the Science of Security, pages 1–8, 04 2018. \\n[194] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th \\nInternational Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, \\n2018. \\n[195] Saeed Mahloujifar, Esha Ghosh, and Melissa Chase. Property inference from poi-soning. In 2022 IEEE Symposium on Security and Privacy (S&P), pages 1120–1137, \\n2022. \\n[196] James Manyika and Sissie Hsiao. An overview of Bard: an early experiment with generative AI. https://ai.google/static/documents/google-about-bard.pdf, February \\n2023. Google. \\n[197] Shiona McCallum. Chatgpt banned in italy over privacy concerns, 2023. \\n[198] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In IEEE Symposium on Foundations of Computer Science, FOCS ’07, pages 94–103, Las Vegas, NV , USA, 2007. \\n[199] Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum An-derson, Yaron Singer, and Amin Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. arXiv preprint arXiv:2312.02119, 2023. \\n[200] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Ex-ploiting unintended feature leakage in collaborative learning. In 2019 IEEE Sympo-sium on Security and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019, pages 691–706. IEEE, 2019. \\n[201] Melissa Heikkila. ¨ What does GPT-3 “know” about me? https://www.technologyre \\n73 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='22201ae1-5d83-4446-ba5f-1c9b7e8eb87b', embedding=None, metadata={'page_label': '74', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nview.com/2022/08/31/1058800/what-does-gpt-3-know-about-me/, August 2022. \\nMIT Technology Review. \\n[202] Melissa Heikkila. ¨ This new data poisoning tool lets artists fght back against gener -\\native AI. https://www.technologyreview.com/2023/10/23/1082189/data-poisoning \\n-artists-fight-generative-ai/, October 2023. MIT Technology Review. \\n[203] El Mahdi El Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Arsany Guirguis, Lˆe-Nguy ˆen Hoang, and S ´ebastien Rouault. Collaborative learning in the jungle (de-\\ncentralized, byzantine, heterogeneous, asynchronous and nonconvex learning). In NeurIPS, 2021. \\n[204] El Mahdi El Mhamdi, Rachid Guerraoui, and S ´ebastien Rouault. The Hidden Vul-\\nnerability of Distributed Learning in Byzantium. In ICML, 2018. \\n[205] El Mahdi El Mhamdi, Rachid Guerraoui, and S ´ebastien Rouault. Distributed mo-\\nmentum for byzantine-resilient stochastic gradient descent. In ICLR, 2021. \\n[206] Microsoft. Power virtual agents. https://powervirtualagents.microsoft.com/en-us/a \\ni-chatbot/, 2022. Online. \\n[207] Dang Minh, H. Xiang Wang, Y . Fen Li, and Tan N. Nguyen. You can’t have AI both ways: Balancing health data privacy and access fairly. Artifcial Intelligence Review \\nvolume, 55:3503–3568, 2022. https://doi.org/10.1007/s10462-021-10088-y. \\n[208] Ilya Mironov, Kunal Talwar, and Li Zhang. R\\\\’enyi differential privacy of the sam-pled gaussian mechanism. arXiv preprint arXiv:1908.10530, 2019. \\n[209] Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Ger-chick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, So-maieh Nikpoor, Carlos Mu ˜noz Ferrandis, Stas Bekman, Christopher Akiki, Danish \\nContractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilic, ´ \\nG´erard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi \\nBaylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, and Niklas Muennighoff. BigScience Large Open-science Open-access Multilingual Language Model. https: \\n//huggingface.co/bigscience/bloom, 2022. Hugging Face. \\n[210] Seungyong Moon, Gaon An, and Hyun Oh Song. Parsimonious black-box adversar -\\nial attacks via effcient combinatorial optimization. In International Conference on \\nMachine Learning (ICML), 2019. \\n[211] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition (CVPR), July 2017. \\n[212] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deep-Fool: A simple and accurate method to fool deep neural networks. https://arxiv.org/ abs/1511.04599, 2015. \\n[213] Ghulam Muhammad, Fatima Alshehri, Fakhri Karray, Abdulmotaleb El Saddik, Mansour Alsulaiman, and Tiago H. Falk. A comprehensive survey on multimodal medical signals fusion for smart healthcare systems. Information Fusion, 76:355– \\n375, 2021. \\n[214] Sasi Kumar Murakonda and Reza Shokri. ML Privacy Meter: Aiding regulatory \\n74 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='02991446-fcf2-474b-bcaa-96bfdf5661c5', embedding=None, metadata={'page_label': '75', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\ncompliance by quantifying the privacy risks of machine learning, 2020. \\n[215] Luis Mu ˜noz-Gonz ´alez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin \\nWongrassamee, Emil C. Lupu, and Fabio Roli. Towards poisoning of deep learn-\\ning algorithms with back-gradient optimization. In Proceedings of the 10th ACM \\nWorkshop on Artifcial Intelligence and Security, AISec ’17, 2017. \\n[216] Nina Narodytska and Shiva Kasiviswanathan. Simple black-box adversarial attacks on deep neural networks. In 2017 IEEE Conference on Computer Vision and Pattern \\nRecognition Workshops (CVPRW), pages 1310–1318, 2017. \\n[217] Milad Nasr, Jamie Hayes, Thomas Steinke, Borja Balle, Florian Tram `er, Matthew \\nJagielski, Nicholas Carlini, and Andreas Terzis. Tight auditing of differentially pri-vate machine learning. In Joseph A. Calandrino and Carmela Troncoso, editors, 32nd USENIX Security Symposium, USENIX Security 2023, Anaheim, CA, USA, August 9-11, 2023, pages 1631–1648. USENIX Association, 2023. \\n[218] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In IEEE Symposium on Security and Privacy, pages 739– \\n753. IEEE, 2019. \\n[219] Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, and Nicholas Carlini. Adversary instantiation: Lower bounds for differentially private machine learning. In IEEE Symposium on Security & Privacy, IEEE S&P ’21, 2021. https: \\n//arxiv.org/abs/2101.04535. \\n[220] National Security Commission on Artifcial Intelligence. Final report. https://www. \\nnscai.gov/2021-final-report/, 2021. \\n[221] Seth Neel, Aaron Roth, and Saeed Sharif-Malvajerdi. Descent-to-delete: Gradient-based methods for machine unlearning. In Vitaly Feldman, Katrina Ligett, and Sivan Sabato, editors, Proceedings of the 32nd International Conference on Algorithmic \\nLearning Theory, volume 132 of Proceedings of Machine Learning Research, pages \\n931–962. PMLR, 16–19 Mar 2021. \\n[222] Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D. Joseph, Benjamin I.P. Rubinstein, Udam Saini, Charles Sutton, and Kai Xia. Exploiting machine learning to subvert your spam flter. In First USENIX Workshop on Large-Scale Exploits and \\nEmergent Threats (LEET 08), San Francisco, CA, April 2008. USENIX Association. \\n[223] Jessica Newman. A Taxonomy of Trustworthiness for Artifcial Intelligence: Con-necting Properties of Trustworthiness with Risk Management and the AI Lifecy-cle. Technical report, Center for Long Term Cybersecurity, University of California, Berkeley, 2023. Online: https://cltc.berkeley.edu/wp-content/uploads/2023/01/Tax \\nonomy of AI Trustworthiness.pdf. \\n[224] J. Newsome, B. Karp, and D. Song. Polygraph: Automatically generating signatures for polymorphic worms. In 2005 IEEE Symposium on Security and Privacy (S&P), \\npages 226–241, 2005. \\n[225] Thien Duc Nguyen, Phillip Rieger, Huili Chen, Hossein Yalame, Helen M ¨ollering, \\nHossein Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Shaza \\n75 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='48b82657-6b02-4271-b7de-5b796022c4a0', embedding=None, metadata={'page_label': '76', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nZeitouni, Farinaz Koushanfar, Ahmad-Reza Sadeghi, and Thomas Schneider. \\nFLAME: Taming backdoors in federated learning. In 31st USENIX Security Sympo-\\nsium (USENIX Security 22), pages 1415–1432, Boston, MA, August 2022. USENIX Association. \\n[226] National Institute of Standards and Technology. Artifcial Intelligence Risk Man-agement Framework (AI RMF 1.0). https://doi.org/10.6028/NIST.AI.100-1, 2023. \\nOnline. \\n[227] OpenAI. ChatGPT: Optimizing Language Models for Dialogue. https://openai.com \\n/blog/chatgpt/, 2022. Online. \\n[228] OpenAI. Gpt-4 technical report, 2023. \\n[229] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. \\n[230] OpenAI. Openai moderation platform documentation, 2023. \\n[231] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of black-box models. In 2019 IEEE/CVF Conference on Computer \\nVision and Pattern Recognition (CVPR), pages 4949–4958, 2019. \\n[232] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. https: \\n//arxiv.org/abs/1605.07277, 2016. \\n[233] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, \\nand Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communica-tions Security, ASIA CCS ’17, page 506–519, New York, NY , USA, 2017. Associa-tion for Computing Machinery. \\n[234] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy (S&P), pages 582–597, 2016. \\n[235] Andrea Paudice, Luis Mu ˜ alez, and Emil C. Lupu. noz-Gonz ´ Label sanitiza-\\ntion against label fipping poisoning attacks. In Carlos Alzate, Anna Mon-reale, Haytham Assem, Albert Bifet, Teodora Sandra Buda, Bora Caglayan, Brett Drury, Eva Garc ´ıa-Mart ´ın, Ricard Gavald `a, Stefan Kramer, Niklas Lavesson, \\nMichael Madden, Ian Molloy, Maria-Irina Nicolae, and Mathieu Sinn, editors, Nemesis/UrbReas/SoGood/IWAISe/GDM@PKDD/ECML, volume 11329 of Lecture \\nNotes in Computer Science, pages 5–15. Springer, 2018. \\n[236] R. Perdisci, D. Dagon, Wenke Lee, P. Fogla, and M. Sharif. Misleading worm sig-nature generators using deliberate noise injection. In 2006 IEEE Symposium on \\nSecurity and Privacy (S&P’06), Berkeley/Oakland, CA, 2006. IEEE. \\n[237] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming lan-guage models with language models. arXiv preprint arXiv:2202.03286, 2022. \\n[238] Neehar Peri, Neal Gupta, W. Ronny Huang, Liam Fowl, Chen Zhu, Soheil Feizi, Tom Goldstein, and John P. Dickerson. Deep k-nn defense against clean-label data poisoning attacks. In Adrien Bartoli and Andrea Fusiello, editors, Computer Vision \\n76 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='137cc842-90a9-497e-ae00-4a0a3c483791', embedding=None, metadata={'page_label': '77', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n– ECCV 2020 Workshops, pages 55–70, Cham, 2020. Springer International Pub-\\nlishing. \\n[239] Fabio Pierazzi, Feargus Pendlebury, Jacopo Cortellazzi, and Lorenzo Cavallaro. In-\\ntriguing properties of adversarial ML attacks in the problem space. In 2020 IEEE \\nSymposium on Security and Privacy (S&P), pages 1308–1325. IEEE Computer So-ciety, 2020. \\n[240] Krishna Pillutla, Galen Andrew, Peter Kairouz, H. Brendan McMahan, Alina Oprea, and Sewoong Oh. Unleashing the power of randomization in auditing differentially private ml. In Advances in Neural Information Processing Systems, 2023. \\n[241] Jonathan Protzenko, Bryan Parno, Aymeric Fromherz, Chris Hawblitzel, Marina Polubelova, Karthikeyan Bhargavan, Benjamin Beurdouche, Joonwon Choi, An-toine Delignat-Lavaud, C ´edric Fournet, Natalia Kulatova, Tahina Ramananandro, \\nAseem Rastogi, Nikhil Swamy, Christoph Wintersteiger, and Santiago Zanella-Beguelin. EverCrypt: A fast, verifed, cross-platform cryptographic provider. In Proceedings of the IEEE Symposium on Security and Privacy (Oakland), May 2020. \\n[242] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! https://arxiv.org/abs/2310.03693, 2023. \\n[243] Gauthama Raman M. R., Chuadhry Mujeeb Ahmed, and Aditya Mathur. Machine learning for intrusion detection in industrial control systems: Challenges and lessons from experimental evaluation. Cybersecurity, 4(27), 2021. \\n[244] Aida Rahmattalabi, Shahin Jabbari, Himabindu Lakkaraju, Phebe Vayanos, Max Izenberg, Ryan Brown, Eric Rice, and Milind Tambe. Fair infuence maximization: A welfare optimization approach. In Proceedings of the AAAI Conference on Artif-\\ncial Intelligence 35th, 2021. \\n[245] Adnan Siraj Rakin, Md Hafzul Islam Chowdhuryy, Fan Yao, and Deliang Fan. DeepSteal: Advanced model extractions leveraging effcient weight stealing in mem-ories. In 2022 IEEE Symposium on Security and Privacy (S&P), pages 1157–1174, \\n2022. \\n[246] Dhanesh Ramachandram and Graham W. Taylor. Deep multimodal learning: A survey on recent advances and trends. IEEE Signal Processing Magazine, 34(6):96– \\n108, 2017. \\n[247] Robust Intelligence. AI Firewall, 2023. \\n[248] Elan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and Zico Kolter. Certifed ro-bustness to label-fipping attacks via randomized smoothing. In International Con-\\nference on Machine Learning, pages 8230–8241. PMLR, 2020. \\n[249] Benjamin IP Rubinstein, Blaine Nelson, Ling Huang, Anthony D Joseph, Shing-hon Lau, Satish Rao, Nina Taft, and J Doug Tygar. Antidote: understanding and defending against poisoning of anomaly detectors. In Proceedings of the 9th ACM \\nSIGCOMM conference on Internet measurement, pages 1–14, 2009. \\n[250] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and Herv ´e \\nJ´egou. White-box vs black-box: Bayes optimal strategies for membership inference. \\n77 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e480d598-29f1-4b1b-b25e-1f6f668e5acb', embedding=None, metadata={'page_label': '78', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nIn ICML, volume 97 of Proceedings of Machine Learning Research, pages 5558– \\n5567. PMLR, 2019. \\n[251] Carl Sabottke, Octavian Suciu, and Tudor Dumitras. Vulnerability disclosure in the \\nage of social media: Exploiting Twitter for predicting real-world exploits. In 24th \\nUSENIX Security Symposium (USENIX Security 15), pages 1041–1056, Washing-ton, D.C., August 2015. USENIX Association. \\n[252] Ahmed Salem, Giovanni Cherubin, David Evans, Boris K ¨opf, Andrew Paverd, An-\\nshuman Suri, Shruti Tople, and Santiago Zanella-B ´eguelin. SoK: Let the privacy \\ngames begin! A unifed treatment of data inference privacy in machine learning. https://arxiv.org/abs/2212.10986, 2022. \\n[253] Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang. Dynamic backdoor attacks against machine learning models. https://arxiv.org/abs/2003.036 \\n75, 2020. \\n[254] Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, and Aleksander Madry. Raising the cost of malicious ai-powered image editing. In Proceedings \\nof the 40th International Conference on Machine Learning, ICML’23. JMLR.org, \\n2023. \\n[255] Roman Samoilenko. New prompt injection attack on chatgpt web version. mark-down images can steal your chat data, 2023. \\n[256] Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. Just how toxic is data poisoning? A unifed benchmark for backdoor and data poisoning attacks. https://arxiv.org/abs/2006.12557, 2020. arXiv. \\n[257] Giorgio Severi, Jim Meyer, Scott Coull, and Alina Oprea. Explanation-guided back-door poisoning attacks against malware classifers. In 30th USENIX Security Sym-\\nposium (USENIX Security 2021), 2021. \\n[258] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! Targeted clean-label poisoning attacks on neural networks. In Advances in Neural Information Processing Systems, \\npages 6103–6113, 2018. \\n[259] Shawn Shan, Arjun Nitin Bhagoji, Haitao Zheng, and Ben Y . Zhao. Poison forensics: Traceback of data poisoning attacks in neural networks. In 31st USENIX Security \\nSymposium (USENIX Security 22), pages 3575–3592, Boston, MA, August 2022. USENIX Association. \\n[260] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. Acces-\\nsorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 23rd ACM SIGSAC Conference on Computer and Communica-tions Security, October 2016. \\n[261] Vasu Sharma, Ankita Kalra, Vaibhav, Simral Chaudhary, Labhesh Patel, and LP Morency. Attend and attack: Attention guided adversarial attacks on visual ques-tion answering models. https://nips2018vigil.github.io/static/papers/accepted/33.pd \\nf, 2018. \\n[262] Ryan Sheatsley, Blaine Hoak, Eric Pauley, Yohan Beugin, Michael J. Weisman, and \\n78 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b09407a7-9d6c-4dd1-87b1-15f2144e76c6', embedding=None, metadata={'page_label': '79', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nPatrick McDaniel. On the robustness of domain constraints. In Proceedings of \\nthe 2021 ACM SIGSAC Conference on Computer and Communications Security, \\nCCS ’21, page 495–515, New York, NY , USA, 2021. Association for Computing Machinery. \\n[263] Virat Shejwalkar and Amir Houmansadr. Manipulating the byzantine: Optimizing model poisoning attacks and defenses for federated learning. In NDSS, 2021. \\n[264] Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, and Daniel Ramage. Back to the drawing board: A critical evaluation of poisoning attacks on production federated learning. In 43rd IEEE Symposium on Security and Privacy, SP 2022, San Francisco, \\nCA, USA, May 22-26, 2022, pages 1354–1371. IEEE, 2022. \\n[265] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. ” do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825, 2023. \\n[266] Xinyue Shen, Yiting Qu, Michael Backes, and Yang Zhang. Prompt stealing attacks against text-to-image generation models. arXiv preprint arXiv:2302.09923, 2023. \\n[267] Toby Shevlane, Sebastian Farquhar, Ben Garfnkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth, Shahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark, Yoshua Bengio, Paul Christiano, and Allan Dafoe. Model evaluation for extreme risks, 2023. \\n[268] Cong Shi, Tianfang Zhang, Zhuohang Li, Huy Phan, Tianming Zhao, Yan Wang, Jian Liu, Bo Yuan, and Yingying Chen. Audio-domain position-independent back-door attack via unnoticeable triggers. In Proceedings of the 28th Annual Inter -\\nnational Conference on Mobile Computing And Networking, MobiCom ’22, page 583–595, New York, NY , USA, 2022. Association for Computing Machinery. \\n[269] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on \\nSecurity and Privacy (SP), pages 3–18. IEEE, 2017. \\n[270] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In IEEE Symposium on Security \\nand Privacy (S&P), Oakland, 2017. \\n[271] Satya Narayan Shukla, Anit Kumar Sahu, Devin Willmott, and Zico Kolter. Simple and effcient hard label black-box adversarial attacks in low query budget regimes. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & \\nData Mining, KDD ’21, page 1461–1469, New York, NY , USA, 2021. Association for Computing Machinery. \\n[272] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget, 2023. \\n[273] Ilia Shumailov, Yiren Zhao, Daniel Bates, Nicolas Papernot, Robert Mullins, and Ross Anderson. Sponge examples: Energy-latency attacks on neural networks. http \\ns://arxiv.org/abs/2006.03463, 2020. \\n79 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='910500c7-5a9d-4e5d-be1b-71e7792a8a70', embedding=None, metadata={'page_label': '80', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n[274] Gagandeep Singh, Timon Gehr, Markus P ¨uschel, and Martin Vechev. An abstract \\ndomain for certifying neural networks. Proc. ACM Program. Lang., 3, January 2019. \\n[275] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, \\nEkin D. Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. FixMatch: Simplifying semi-supervised learning with consistency and confdence. In Proceedings of the \\n34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. \\n[276] Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, and Prem Natarajan. AlexaTM 20B: Few-shot learning using a large-scale multilingual seq2seq model. https://www.amazon.science/publications/ \\nalexatm-20b-few-shot-learning-using-a-large-scale-multilingual-seq2seq-model, 2022. Amazon. \\n[277] Dawn Song, Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rah-mati, Florian Tramer, ` Atul Prakash, and Tadayoshi Kohno. Physical adversarial ex-\\namples for object detectors. In 12th USENIX Workshop on Offensive Technologies \\n(WOOT 18), Baltimore, MD, August 2018. USENIX Association. \\n[278] Shuang Song and David Marn. Introducing a new privacy testing library in Tensor-Flow, 2020. \\n[279] N. Srndic and P. Laskov. Practical evasion of a learning-based classifer: A case study. In Proc. IEEE Security and Privacy Symposium, 2014. \\n[280] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certifed defenses for data poisoning attacks. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Process-\\ning Systems, volume 30. Curran Associates, Inc., 2017. \\n[281] Thomas Steinke, Milad Nasr, and Matthew Jagielski. Privacy auditing with one (1) training run. In Advances in Neural Information Processing Systems, 2023. \\n[282] Octavian Suciu, Scott E Coull, and Jeffrey Johns. Exploring adversarial examples in malware detection. In 2019 IEEE Security and Privacy Workshops (SPW), pages \\n8–14. IEEE, 2019. \\n[283] Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor Du-mitras. When does machine learning FAIL? generalized transferability for evasion and poisoning attacks. In 27th USENIX Security Symposium (USENIX Security 18), \\npages 1299–1316, 2018. \\n[284] Jingwei Sun, Ang Li, Louis DiValentin, Amin Hassanzadeh, Yiran Chen, and Hai Li. FL-WBC: Enhancing robustness against model poisoning attacks in federated learning from a client perspective. In NeurIPS, 2021. \\n[285] Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really backdoor federated learning? arXiv:1911.07963, 2019. \\n[286] Anshuman Suri and David Evans. Formalizing and estimating distribution inference risks. Proceedings on Privacy Enhancing Technologies, 2022. \\n80 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='de978ca6-f4be-4d73-b7dd-7b8bb28c272f', embedding=None, metadata={'page_label': '81', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n[287] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, \\nIan Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In Inter -\\nnational Conference on Learning Representations, 2014. \\n[288] Rahim Taheri, Reza Javidan, Mohammad Shojafar, Zahra Pooranian, Ali Miri, and Mauro Conti. On defending against label fipping attacks on malware detection systems. CoRR, abs/1908.04473, 2019. \\n[289] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Ka-reem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana ¨ıs White, Anders An-\\ndreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Lau-rent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shan-tanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Ruibo Liu, Yunxuan Li, Sarah Co-gan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Martin Chadwick, Gaurav Singh Tomar, Xavier Garcia, Evan Senter, Emanuel Taropa, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adri `a Puig-\\ndom`enech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey \\nBrin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Win-kler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Yujing Zhang, Ravi Addanki, Antoine Miech, Annie Louis, Laurent El Shafey, Denis Teplyashin, Geoff Brown, Elliot Catt, Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ash-wood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, S ´ebastien M. R. Arnold, Vijay Vasudevan, Shub-\\nham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srini-vasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Za-heer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Di-\\n81 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a2eabf61-5134-42ee-9ced-9cbc34a53be1', embedding=None, metadata={'page_label': '82', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\npanjan Das, Dominika Rogozi ´nska, Vitaly Nikolaev, Pablo Sprechmann, Zachary \\nNado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, \\nChris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufm, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yun-han Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Lan-don, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Gim ´enez, Legg Yeung, Hanzhao Lin, James Keeling, Petko Georgiev, \\nDiana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran V odrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIlroy, Mario Luci ˇ ´c, Guodong Zhang, Wael Farhan, Michael Sharman, Paul \\nNatsev, Paul Michel, Yong Cheng, Yamini Bansal, Siyuan Qiao, Kris Cao, Sia-mak Shakeri, Christina Butterfeld, Justin Chung, Paul Kishan Rubenstein, Shiv-ani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Rapha ¨el Lopez Kaufman, Simon Tokumine, Hexiang \\nHu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sj ¨osund, \\nS´ebastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, \\nVittorio Selo, Rhys May, Konstantinos Aisopos, L ´eonard Hussenot, Livio Baldini \\nSoares, Kate Baumli, Michael B. Chang, Adri `a Recasens, Ben Caine, Alexander \\nPritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, V ´ıctor Cam-\\npos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, \\n¨ Phoebe Thacker, C ¸ glar u, Zhishuai Zhang, Mohammad Saleh, James Svensson, a˘ Unl¨ \\n82 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7a2d861a-f31c-49f1-bdac-9d101b537e2e', embedding=None, metadata={'page_label': '83', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nMax Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi \\nVezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, YaGuang Li, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Raki ´cevi´c, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Jun-\\nhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Gamaleldin Elsayed, Ed Chi, Mahdis Mahdieh, Ian Tenney, Nan Hua, Ivan Petrychenko, Patrick Kane, Dylan Scandinaro, Rishub Jain, Jonathan Uesato, Romina Datta, Adam Sadovsky, Oskar Bunyan, Dominik Rabiej, Shimu Wu, John Zhang, Gautam Vasudevan, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Betty Chan, Pam G Rabinovitch, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Sahitya Potluri, Jane Park, Elnaz Davoodi, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Chris Gorgolewski, Pe-ter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Paul Suganthan, Evan Palmer, Geoffrey Irving, Edward Loper, Man-aal Faruqui, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Michael Fink, Alfonso Casta ˜ nski, Ashwin Sreevatsa, no, Irene Giannoumis, Wooyeol Kim, Mikołaj Rybi ´ \\nJennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marin Georgiev, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie Lui, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Lam Nguyen Thiet, Daniel Andor, Pedro Valenzuela, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Sarmishta Velury, Sebastian Krause, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Tejasi Latkar, Mingyang Zhang, Quoc Le, Elena Allica Abellan, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Or-gad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Pe-ter Hawkins, Robert Dadashi, Colin Gaffney, Sid Lall, Ken Franko, Egor Filonov, Anna Bulanova, R ´emi Leblond, Vikas Yadav, Shirley Chung, Harry Askham, Luis C. \\n83 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='489876de-d659-468c-9676-96f2f4b7e028', embedding=None, metadata={'page_label': '84', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nCobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-\\nCheng Lin, Colin Evans, Hao Zhou, Alek Dimitriev, Hannah Forbes, Dylan Ba-narse, Zora Tung, Jeremiah Liu, Mark Omernick, Colton Bishop, Chintu Kumar, Rachel Sterneck, Ryan Foley, Rohan Jain, Swaroop Mishra, Jiawei Xia, Taylor Bos, Geoffrey Cideron, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Petru Gurita, Hila Noga, Premal Shah, Daniel J. Mankowitz, Alex Polo-zov, Nate Kushman, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Anhad Mohananey, Matthieu Geist, Sidharth Mudgal, Sertan Girgin, Hui Li, Jiayu Ye, Ofr Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Quan Yuan, Sumit Bagri, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Aliaksei Sev-eryn, Jonathan Lai, Kathy Wu, Heng-Tze Cheng, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Mark Geller, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Andrei Sozanschi, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafe, Tanya Grunina, Rishika Sinha, Alice Talbert, Abhimanyu Goyal, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Sabaer Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Tao Zhu, Yeongil Ko, Laura Knight, Am ´ eliou, Ning Niu, Shane Gu, Chenxi Pang, Dustin Tran, Yeqing Li, elie H ´ \\nNir Levine, Ariel Stolovich, Norbert Kalb, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Balaji Lakshminarayanan, Charlie Deck, Shyam Upadhyay, Hyo Lee, Mike Dusenberry, Zonglin Li, Xuezhi Wang, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sum-mer Yue, Sho Arora, Eric Malmi, Daniil Mirylenka, Qijun Tan, Christy Koh, So-heil Hassas Yeganeh, Siim P ˜oder, Steven Zheng, Francesco Pongetti, Mukarram \\nTariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Ragha Kotikalapudi, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Chenkai Kuang, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Pei Sun, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Ishita Dasgupta, Guillaume Des-jardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivi `ere, \\nAlanna Walton, Cl ´ement Crepy, Alicia Parrish, Yuan Liu, Zongwei Zhou, Clement \\nFarabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fid-jeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Pluci ´nska, David \\nBridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Ivo Penchev, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anas-\\n84 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1eb2a38b-66f9-4b96-ac10-3fe9150e3ea4', embedding=None, metadata={'page_label': '85', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\ntasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, \\nAmir Globerson, Adam Kurzrok, Lynette Webb, Sahil Dua, Dong Li, Preethi La-hoti, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mi-hir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Taylan Bilal, Evgenii Eltyshev, Daniel Balle, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Adams Yu, Christof Angermueller, Xiaowei Li, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Guru-murthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Ur -\\nbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Kevin Brooks, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, Gin-ger Perng, Blake Hechtman, Parker Schuh, Milad Nasr, Mia Chen, Kieran Milan, Vladimir Mikulik, Trevor Strohman, Juliana Franco, Tim Green, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. Gemini: A family of highly capable multimodal models. https://arxiv.org/abs/2312.11805, 2023. \\n[290] The White House. Executive Order on the Safe, Secure, and Trustworthy Develop-ment and Use of Artifcial Intelligence. https://www.whitehouse.gov/briefing-room \\n/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustw orthy-development-and-use-of-artificial-intelligence/, October 2023. The White \\nHouse. \\n[291] Anvith Thudi, Ilia Shumailov, Franziska Boenisch, and Nicolas Papernot. Bounding membership inference. https://arxiv.org/abs/2202.12232, 2022. \\n[292] Lionel Nganyewou Tidjon and Foutse Khomh. Threat assessment in machine learn-ing based systems. arXiv preprint arXiv:2207.00091, 2022. \\n[293] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal \\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and effcient foundation language models, 2023. \\n[294] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucu-rull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya \\n85 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='dfad61be-bc4f-48b7-89da-ab3cd1b55a8c', embedding=None, metadata={'page_label': '86', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nLee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, \\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ran-jan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fne-tuned chat mod-els, 2023. \\n[295] Florian Tramer. Detecting adversarial examples is (Nearly) as hard as classifying them. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference \\non Machine Learning, volume 162 of Proceedings of Machine Learning Research, \\npages 21692–21702. PMLR, 17–23 Jul 2022. \\n[296] Florian Tramer, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, and Joern-Henrik Jacobsen. Fundamental tradeoffs between invariance and sensitivity to adver -\\nsarial perturbations. In Hal Daume ´ III and Aarti Singh, editors, Proceedings of the \\n37th International Conference on Machine Learning, volume 119 of Proceedings of \\nMachine Learning Research, pages 9561–9571. PMLR, 13–18 Jul 2020. \\n[297] Florian Tramer, ` Nicholas Carlini, Wieland Brendel, and Aleksander Ma ¸dry. On \\nadaptive attacks to adversarial example defenses. In Proceedings of the 34th In-\\nternational Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY , USA, 2020. Curran Associates Inc. \\n[298] Florian Tram `er, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. \\nStealing machine learning models via prediction APIs. In USENIX Security, 2016. \\n[299] Florian Tram `er, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-\\nDaniel. The space of transferable adversarial examples. https://arxiv.org/abs/1704.0 3453, 2017. \\n[300] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. \\nCurran Associates, Inc., 2018. \\n[301] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Alek-sander Madry. Robustness may be at odds with accuracy. In International Confer -\\nence on Learning Representations, 2019. \\n[302] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks. In ICLR, 2019. \\n[303] Apostol Vassilev, Honglan Jin, and Munawar Hasan. Meta learning with language models: Challenges and opportunities in the classifcation of imbalanced text, 2023. \\n[304] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017. \\n[305] Sridhar Venkatesan, Harshvardhan Sikka, Rauf Izmailov, Ritu Chadha, Alina Oprea, \\n86 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a6e60e27-9a12-4765-a5ad-7cf86fba413a', embedding=None, metadata={'page_label': '87', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nand Michael J. De Lucia. Poisoning attacks and data sanitization mitigations for ma-\\nchine learning models in network intrusion detection systems. In MILCOM, pages \\n874–879. IEEE, 2021. \\n[306] Brandon Vigliarolo. GPT-3 ’prompt injection’ attack causes bad bot manners. https: \\n//www.theregister.com/2022/09/19/in brief security/, 2022. The Register, Online. \\n[307] Vincent, James. Google and Microsoft’s chatbots are already citing one another in a misinformation shitshow, 2023. \\n[308] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Uni-versal adversarial triggers for attacking and analyzing NLP. arXiv preprint \\narXiv:1908.07125, 2019. \\n[309] Eric Wallace, Tony Z. Zhao, Shi Feng, and Sameer Singh. Concealed data poisoning attacks on NLP models. In NAACL, 2021. \\n[310] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y . Zhao. Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks. In 2019 IEEE Symposium on Security and Privacy \\n(SP), pages 707–723, San Francisco, CA, USA, May 2019. IEEE. \\n[311] Haotao Wang, Tianlong Chen, Shupeng Gui, Ting-Kuei Hu, Ji Liu, and Zhangyang Wang. Once-for-All Adversarial Training: In-Situ Tradeoff between Robustness and Accuracy for Free. In Proceedings of the 34th Conference on Neural Information \\nProcessing Systems (NeurIPS 2020), Vancouver, Canada, 2020. \\n[312] Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the Tails: Yes, You Really Can Backdoor Federated Learning. In NeurIPS, 2020. \\n[313] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Formal security analysis of neural networks using symbolic intervals. In 27th USENIX Se-\\ncurity Symposium (USENIX Security 18), pages 1599–1614, Baltimore, MD, August 2018. USENIX Association. \\n[314] Wenxiao Wang, Alexander Levine, and Soheil Feizi. Improved certifed defenses against data poisoning with (deterministic) fnite aggregation. In Kamalika Chaud-huri, Stefanie Jegelka, Le Song, Csaba Szepesv ´ari, Gang Niu, and Sivan Sabato, \\neditors, International Conference on Machine Learning, ICML 2022, 17-23 July \\n2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning \\nResearch, pages 22769–22783. PMLR, 2022. \\n[315] Xiaosen Wang and Kun He. Enhancing the transferability of adversarial attacks through variance tuning. In IEEE Conference on Computer Vision and Pattern \\nRecognition, CVPR 2021, virtual, June 19-25, 2021, pages 1924–1933. Computer Vision Foundation / IEEE, 2021. \\n[316] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023. \\n[317] Xingxing Wei, Jun Zhu, Sha Yuan, and Hang Su. Sparse adversarial perturbations for videos. In Proceedings of the Thirty-Third AAAI Conference on Artifcial In-\\ntelligence and Thirty-First Innovative Applications of Artifcial Intelligence Confer -\\n87 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a797bbd7-d715-437b-ba90-451122b86c6b', embedding=None, metadata={'page_label': '88', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nence and Ninth AAAI Symposium on Educational Advances in Artifcial Intelligence, \\nAAAI’19/IAAI’19/EAAI’19. AAAI Press, 2019. \\n[318] Zhipeng Wei, Jingjing Chen, Xingxing Wei, Linxi Jiang, Tat-Seng Chua, Fengfeng Zhou, and Yu-Gang Jiang. Heuristic black-box adversarial attacks on video recog-nition models. In Proceedings of the AAAI Conference on Artifcial Intelligence, \\nvolume 34, pages 12338–12345, 2020. \\n[319] Lilian Weng. Adversarial attacks on latent language models, 2023. \\n[320] Emily Wenger, Josephine Passananti, Arjun Nitin Bhagoji, Yuanshun Yao, Haitao Zheng, and Ben Y . Zhao. Backdoor attacks against deep learning systems in the physical world. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recog-\\nnition (CVPR), pages 6202–6211, 2020. \\n[321] Dongxian Wu and Yisen Wang. Adversarial neuron pruning purifes backdoored deep models. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wort-man Vaughan, editors, Advances in Neural Information Processing Systems, vol-\\nume 34, pages 16913–16925. Curran Associates, Inc., 2021. \\n[322] Zhen Xiang, David J. Miller, and George Kesidis. Post-training detection of back-door attacks for two-class and multi-attack scenarios. In The Tenth International \\nConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. \\n[323] Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and Fabio Roli. Is feature selection secure against training data poisoning? In Interna-\\ntional Conference on Machine Learning, pages 1689–1698, 2015. \\n[324] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. In H. Larochelle, M. Ranzato, R. Had-sell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing \\nSystems, volume 33, pages 6256–6268. Curran Associates, Inc., 2020. \\n[325] Weilin Xu, Yanjun Qi, and David Evans. Automatically evading classifers. In Proceedings of the 2016 Network and Distributed Systems Symposium, pages 21– 24, 2016. \\n[326] Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrell, and Dawn Song. Fooling vision and language models despite localization and attention mech-anism. https://arxiv.org/abs/1709.08693, 2017. \\n[327] Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A. Gunter, and Bo Li. De-tecting AI trojans using meta neural analysis. In IEEE Symposium on Security and \\nPrivacy, S&P 2021, pages 103–120, United States, May 2021. \\n[328] Karren Yang, Wan-Yi Lin, Manash Barman, Filipe Condessa, and Zico Kolter. Defending multimodal fusion models against single-source adversaries. In 2021 \\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE \\nXplore, 2022. \\n[329] Limin Yang, Zhi Chen, Jacopo Cortellazzi, Feargus Pendlebury, Kevin Tu, Fabio Pierazzi, Lorenzo Cavallaro, and Gang Wang. Jigsaw puzzle: Selective backdoor attack to subvert malware classifers. CoRR, abs/2202.05470, 2022. \\n88 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9ac8f0ac-9a2e-4c5b-8fc2-b89ab65a9bba', embedding=None, metadata={'page_label': '89', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n[330] Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun \\nZhao, and Dahua Lin. Shadow alignment: The ease of subverting safely-aligned language models, 2023. \\n[331] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y . Zhao. Latent backdoor attacks on deep neural networks. In Proceedings of the 2019 ACM SIGSAC Conference on \\nComputer and Communications Security, CCS ’19, page 2041–2055, New York, NY , USA, 2019. Association for Computing Machinery. \\n[332] Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda, Vincent Bindschaedler, and Reza Shokri. Enhanced membership inference attacks against machine learning models. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and \\nCommunications Security, CCS ’22, page 3093–3106, New York, NY , USA, 2022. Association for Computing Machinery. \\n[333] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overftting. In IEEE Computer \\nSecurity Foundations Symposium, CSF ’18, pages 268–282, 2018. https://arxiv.org/ \\nabs/1709.01604. \\n[334] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates. In ICML, 2018. \\n[335] Youngjoon Yu, Hong Joo Lee, Byeong Cheon Kim, Jung Uk Kim, and Yong Man Ro. Investigating vulnerability to adversarial examples on multimodal data fusion in deep learning. https://arxiv.org/abs/2005.10987, 2020. Online. \\n[336] Andrew Yuan, Alina Oprea, and Cheng Tan. Dropout attacks. In IEEE Symposium \\non Security and Privacy (S&P), 2024. \\n[337] Santiago Zanella-B ´ uhle, Andrew eguelin, Lukas Wutschitz, Shruti Tople, Victor R ¨ \\nPaverd, Olga Ohrimenko, Boris K ¨opf, and Marc Brockschmidt. Analyzing informa-\\ntion leakage of updates to natural language models. In Proceedings of the 2020 ACM \\nSIGSAC Conference on Computer and Communications Security, page 363–375, New York, NY , USA, 2020. Association for Computing Machinery. \\n[338] Santiago Zanella-Beguelin, Lukas Wutschitz, Shruti Tople, Ahmed Salem, Victor R¨uhle, Andrew Paverd, Mohammad Naseri, Boris K ¨opf, and Daniel Jones. Bayesian \\nestimation of differential privacy. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings \\nof the 40th International Conference on Machine Learning, volume 202 of Proceed-\\nings of Machine Learning Research, pages 40624–40636. PMLR, 23–29 Jul 2023. \\n[339] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models, 2021. \\n[340] Yi Zeng, Si Chen, Won Park, Zhuoqing Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning of backdoors via implicit hypergradient. In International Conference on \\nLearning Representations, 2022. \\n[341] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Commun. \\nACM, 64(3):107–115, feb 2021. \\n89 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ae4f45a9-2d2b-4e34-a9f9-ef13b1bcfddd', embedding=None, metadata={'page_label': '90', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\n[342] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and \\nMichael Jordan. Theoretically principled trade-off between robustness and accu-racy. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the \\n36th International Conference on Machine Learning, volume 97 of Proceedings of \\nMachine Learning Research, pages 7472–7482. PMLR, 09–15 Jun 2019. \\n[343] Ruisi Zhang, Seira Hidano, and Farinaz Koushanfar. Text revealer: Private text reconstruction via model inversion attacks against transformers. arXiv preprint \\narXiv:2209.10505, 2022. \\n[344] Su-Fang Zhang, Jun-Hai Zhai, Bo-Jun Xie, Yan Zhan, and Xin Wang. Multi-modal representation learning: Advances, trends and challenges. In 2019 Inter -\\nnational Conference on Machine Learning and Cybernetics (ICMLC), pages 1–6. IEEE, 2019. \\n[345] Susan Zhang, Mona Diab, and Luke Zettlemoyer. Democratizing access to large-scale language models with OPT-175B. https://ai.facebook.com/blog/democratizi \\nng-access-to-large-scale-language-models-with-opt-175b/, 2022. Meta AI. \\n[346] Wanrong Zhang, Shruti Tople, and Olga Ohrimenko. Leakage of dataset properties in Multi-Party machine learning. In 30th USENIX Security Symposium (USENIX \\nSecurity 21), pages 2687–2704. USENIX Association, August 2021. \\n[347] Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. Adversarial attacks on deep-learning models in natural language processing: A survey. ACM \\nTrans. Intell. Syst. Technol., 11(3), apr 2020. \\n[348] Yiming Zhang and Daphne Ippolito. Prompts should not be seen as se-crets: Systematically measuring prompt extraction attack success. arXiv preprint \\narXiv:2307.06865, 2023. \\n[349] Zhengming Zhang, Ashwinee Panda, Linyue Song, Yaoqing Yang, Michael Ma-honey, Prateek Mittal, Ramchandran Kannan, and Joseph Gonzalez. Neurotoxin: Durable backdoors in federated learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the \\n39th International Conference on Machine Learning, volume 162 of Proceedings of \\nMachine Learning Research, pages 26429–26446. PMLR, 17–23 Jul 2022. \\n[350] Zhikun Zhang, Min Chen, Michael Backes, Yun Shen, and Yang Zhang. Infer -\\nence attacks against graph neural networks. In 31st USENIX Security Symposium \\n(USENIX Security 22), 2022. \\n[351] Junhao Zhou, Yufei Chen, Chao Shen, and Yang Zhang. Property inference attacks against GANs. In Proceedings of Network and Distributed System Security, NDSS, \\n2022. \\n[352] Chen Zhu, W. Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom Goldstein. Transferable clean-label poisoning attacks on deep neural nets. In Ka-malika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th Inter -\\nnational Conference on Machine Learning, volume 97 of Proceedings of Machine \\nLearning Research, pages 7614–7623. PMLR, 09–15 Jun 2019. \\n[353] Giulio Zizzo, Chris Hankin, Sergio Maffeis, and Kevin Jones. Adversarial machine \\n90 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='395ee9c8-6b22-42a1-bfb3-6882371a8108', embedding=None, metadata={'page_label': '91', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nlearning beyond the image domain. In Proceedings of the 56th Annual Design Au-\\ntomation Conference 2019, DAC ’19, New York, NY , USA, 2019. Association for \\nComputing Machinery\\n. \\n[354] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and \\ntransferable adversarial attacks on aligned language models. arXiv preprint \\narXiv:2307.15043, 2023. \\n91 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fee31a91-22a7-41c8-aa5e-4c6fc2021f76', embedding=None, metadata={'page_label': '92', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C NIST AI 100-2e2023 \\nJanuary 2024 \\nAppendix: Glossary \\nNote:\\n one may click on the page number shown at the end of the defnition of each \\nglossary entry to go to the page where the term is used. \\nA \\nadv\\nersarial examples (adversarial examples) Modifed testing samples which induce mis-\\nclassifcation of a machine learning model at deployment time v, 9 \\nadversarial success (adversarial success) Indicates reaching an availability breakdown, \\nintegrity violations, privacy compromise, or abuse trigger (for GenAI models only) \\nin response to attempted adversarial attacks on the model 9 \\nArea Under the Curve (Area Under the Curve) In ML the Area Under the Curve (AUC) \\nis a measure of the ability of a classifer to distinguish between classes. The higher the AUC, the better the performance of the model at distinguishing between the two classes. AUC measures the entire two-dimensional area underneath the R\\nECEIVER \\nOPERATING CHARACTERISTICS (ROC) curve 31 \\navailability attack (availability attack) Adversarial attacks against machine learning which \\ndegrade the overall model performance 9 \\nB \\nbackdoor \\npattern (backdoor pattern) A trigger pattern inserted into a data sample to induce \\nmis-classifcation of a poisoned model. For example, in computer vision it may be constructed from a set of neighboring pixels, e.g., a white square, and added to a specifc target label. To mount a backdoor attack, the adversary frst poisons the data by adding the trigger to a subset of the clean data and changing their corresponding labels to the target label 9 \\nbackdoor poisoning attacks (backdoor poisoning attacks) Poisoning attacks against ma-\\nchine learning which change the prediction on samples including a backdoor pat-tern 9, 40 \\nclassifcation (classifcation) Type of supervised learning in which data labels are discrete \\n8 \\nconvolutional neural networks (convolutional neural networks) A Convolutional Neural \\nNetwork (CNN) is a class of artifcial neural networks whose architecture connects neurons from one layer to the next layer and includes at least one layer performing convolution operations. CNNs are typically applied to image analysis and classif-cation. See [119] for further details 8, 32 \\nD \\ndata \\npoisoning (data poisoning) Poisoning attacks in which a part of the training data is \\nunder the control of the adversary 4, 8 \\n92 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='66055b81-8ece-4ac0-9c2c-83a10683dc56', embedding=None, metadata={'page_label': '93', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\ndata privacy (data privacy) Attacks against machine learning models to extract sensitive \\ninformation about training data 10 \\ndata reconstruction (data reconstruction) Data privacy attacks which reconstruct sensitive \\ninformation about training data records 10, 29 \\ndeployment stage (deployment stage) Stage of ML pipeline in which the model is deployed \\non new data 8, 37 \\nDiffusion Model (Diffusion Model) A class of latent variable generative models con-\\nsisting of three major components: a forward process, a reverse process, and a \\nsampling procedure. The goal of the diffusion model is to learn a diffusion pro-cess that generates the probability distribution of a given dataset. It is widely used in computer vision on a variety of tasks, including image denoising, inpainting, super-resolution, and image generation 35 \\ndiscriminative (discriminative) Type of machine learning methods which learn to discrim-\\ninate between classes 8 \\nE \\nener\\ngy-latency attacks (energy-latency attacks) Attacks that exploit the performance de-\\npendency on hardware and model optimizations to negate the effects of hardware optimizations, increase computation latency, increase hardware temperature and massively increase the amount of energy consumed 9, 10 \\nensemble learning (ensemble learning) Type of a meta machine learning approach that \\ncombines the predictions of several models to improve the performance of the combination 8 \\nExpectation Over Transformation (Expectation Over Transformation) Expectation Over \\nTransformation (EOT) helps to strengthen adversarial examples to remain adver -\\nsarial under image transformations that occur in the real world, such as angle and viewpoint changes. EOT models such perturbations within the optimization pro-cedure. Rather than optimizing the log-likelihood of a single example, EOT uses a chosen distribution of transformation functions taking an input controlled by the adversary to the “true” input perceived by the classifer 18 \\nextraction (extraction) The ability of an attacker to extract training data of a generative \\nmodel by prompting the model on specifc inputs 10 \\nF \\nfederated \\nlearning (federated learning) Type of collaborative machine learning, in which \\nmultiple users train jointly a machine learning model 8 \\nfederated learning models (federated learning models) Federated learning is a method-\\nology to train a decentralized machine learning model (e.g., deep neural networks or a pre-trained large language model) across multiple end-devices without shar -\\ning the data residing on each device. Thus, the end-devices collaboratively train a global model by exchanging model updates with a server that aggregates the updates. Compared to traditional centralized learning where the data are pooled, \\n93 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='74487514-8dee-45ed-860c-c42fbee0620d', embedding=None, metadata={'page_label': '94', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nfederated learning has advantages in terms of data privacy and security but these \\nmay come as tradeoffs to the capabilities of the models learned through federated data. Other potential problems one needs to contend with here concern the trust-worthiness of the end-devices and the impact of malicious actors on the learned model 32 \\nfeed-forward neural networks (feed-forward neural networks) A Feed Forward Neural \\nNetwork is an artifcial neural network in which the connections between nodes is from one layer to the next and do not form a cycle. See [119] for further details 32 \\nfne-tuning (fne-tuning) Refers to the process of adapting a pre-trained model to perform \\nspecifc tasks or to specialize in a particular domain. This phase follows the initial pre-training phase and involves training the model further on task-specifc data. This is often a supervised learning task 37 \\nformal methods (formal methods) Formal methods are mathematically rigorous techniques \\nfor the specifcation, development, and verifcation of software systems 20 \\nFunctional Attacks (Functional Attacks) Adversarial attacks that are optimized for a set \\nof data in a domain rather than per data point 16, 25 \\nG \\ngenerati\\nve (generative) Type of machine learning methods which learn the data distribution \\nand can generate new examples from distribution 8 \\ngenerative adversarial networks (generative adversarial networks) A generative adver -\\nsarial network (GAN) is a class of machine learning frameworks in which two neural networks contest with each other in the form of a zero-sum game, where one agent’s gain is another agent’s loss. GAN’s learn to generate new data with the same statistics as the training set. See [119] for further details 32, 35 \\nGenerative Pre-Trained Transformer (Generative Pre-Trained Transformer) An artifcial \\nneural network based on the transformer architecture [304], pre-trained on large \\ndata sets of unlabelled text, and able to generate novel human-like content. Today, this is the predominant architecture for natural language processing tasks 35 \\ngraph neural networks (graph neural networks) A Graph Neural Network (GNN) is an \\noptimizable transformation on all attributes of the graph (nodes, edges, global-context) that preserves the graph symmetries (permutation invariances). GNNs utilize a “graph-in, graph-out” architecture that takes an input graph with informa-tion loaded into its nodes, edges and global-context, and progressively transform these embeddings into an output graph with the same connectivity as that of the input graph 32 \\nH \\nhidden \\nMarkov models (hidden Markov models) A hidden Markov model (HMM) is a \\nstatistical Markov model in which the system being modeled is assumed to be a Markov process with unobservable states. In addition, the model provides an ob-servable process whose outcomes are ”infuenced” by the outcomes of Markov \\n94 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8685e9b4-cf56-4a6c-8682-be9b477b5f89', embedding=None, metadata={'page_label': '95', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nmodel in a known way. HMM can be used to describe the evolution of observable \\nevents that depend on internal factors, which are not directly observable. In ma-chine learning it is assumed that the internal state of a model is hidden but not the hyperparameters 32 \\nI \\nindir\\nect prompt injection (indirect prompt injection) Attacker technique in which a hacker \\nrelies on an LLM ingesting a prompt injection attack indirectly, e.g., by visiting a web page or document. Unlike its direct prompt injection sibling, the attacker in \\nthis scenario does not directly supply a prompt, but attempts to inject instructions indirectly by having the text ingested by some other mechanism, e.g., a plugin 38, \\n39, 44 \\nintegrity attack (integrity attack) Adversarial attacks against machine learning which change \\nthe output prediction of the machine learning model 9 \\nJ \\njailbr\\neak (jailbreak) An attack that employs prompt injection to specifcally circumvent \\nthe safety and moderation features placed on LLMs by their creators. 37, 40 \\nL \\nlabel \\nfipping (label fipping) a type of data poisoning attack where the adversary is re-\\nstricted to changing the training labels 22 \\nlabel limit (label limit) Capability in which the attacker in some scenarios does not control \\nthe labels of training samples in supervised learning 10 \\nlogistic regression (logistic regression) Type of linear classifer that predicts the probability \\nof an observation to be part of a class 8 \\nM \\nmachine \\nunlearning (machine unlearning) Technique that enables a user to request re-\\nmoval of their records from a trained ML model. Effcient approximate unlearning techniques do not require retraining the ML model from scratch 34 \\nmembership-inference attacks (membership-inference attacks) Data privacy attacks to \\ndetermine if a data sample was part of the training set of a machine learning model 10, 29 \\nmodel control (model control) Capability in which the attacker has control over machine \\nlearning model parameters 10 \\nmodel extraction (model extraction) Type of privacy attack to extract model architecture \\nand parameters 10 \\nmodel poisoning (model poisoning) Poisoning attacks in which the model parameters are \\nunder the control of the adversary 8, 9, 40 \\n95 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6a460824-0a8c-401e-918a-c595ffd36fa6', embedding=None, metadata={'page_label': '96', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nmodel privacy (model privacy) Attacks against machine learning models to extract sensi-\\ntive information about the model 10 \\nmultimodal models (multimodal models) Modality is associated with the sensory modal-\\nities which represent primary human channels of communication and sensation, \\nsuch as vision or touch. Multimodal models process and relate information from multiple modalities 55 \\nO \\nout-of-distrib\\nution (out-of-distribution) This term refers to data that was collected at a dif-\\nferent time, and possibly under different conditions or in a different environment, than the data collected to train the model 51 \\nP \\npoisoning \\nattacks (poisoning attacks) Adversarial attacks against machine learning at \\ntraining time 8, 37 \\npre-training (pre-training) Refers to the initial phase of model training where the model \\nlearns general patterns, features, and relationships from vast amounts of unlabeled data. This is typically unsupervised or self-supervised learning, and aims to equip the model with commonly occurring patterns prior to a fne-tuning stage that spe-cializes the model for a specifc downstream task. Foundation models (text or images) are pre-trained models 37 \\nprompt extraction (prompt extraction) An attack in which the objective is to divulge the \\nsystem prompt or other information in an LLMs context that would nominally be hidden from a user 37, 39 \\nprompt injection (prompt injection) Attacker technique in which a hacker enters a text \\nprompt into an LLM or chatbot designed to enable the user to perform unintended or unauthorized actions 39 \\nprompt injections (prompt injections) Malicious plain text instructions to a generative AI \\nsystem that uses textual instructions (a “prompt”) to accomplish a task causing the AI system to generate text on a topic prohibited by the designers of the system 51 \\nproperty inference (property inference) Data privacy attacks which infer global property \\nabout the training data of a machine learning model 10 \\nQ \\nquery \\naccess (query access) Capability in which the attacker can issue queries to a trained \\nmachine learning model and obtain predictions 10, 39 \\nR \\nRecei\\nver Operating Characteristics (ROC) (Receiver Operating Characteristics (ROC)) \\nIn ML the Receiver Operating Characteristics (ROC) curve plots true positive rate versus false positive rate for a classifer 92 \\n96 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='0b12c4b6-7877-4a0e-b6b3-0eb88e217f8d', embedding=None, metadata={'page_label': '97', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nRed Teaming (Red Teaming) NIST defnes cybersecurity red-teaming as “A group of peo-\\nple authorized and organized to emulate a potential adversary’s attack or exploita-\\ntion capabilities against an enterprise’s security posture. The Red Team’s objective is to improve enterprise cybersecurity by demonstrating the impacts of successful attacks and by demonstrating what works for the defenders (i.e., the Blue Team) in an operational environment.” (CNSS 2015 [80]) Traditional red-teaming might \\ncombine physical and cyber attack elements, attack multiple systems, and aims to evaluate the overall security posture of an organization. Penetration testing (pen testing), in contrast, tests the security of a specifc application or system. In AI discourse, red-teaming has come to mean something closer to pen testing, where the model may be rapidly or continuously tested by a set of evaluators and under conditions other than normal operation 51 \\nregression (regression) Type of supervised ML model that is trained on data including \\nnumerical labels (called response variables). Types of regression algorithms in-clude linear regression, polynomial regression, and various non-linear regression methods 8 \\nreinforcement learning (reinforcement learning) Type of machine learning in which an \\nagent interacts with the environment and learns to take actions which optimize a reward function 8 \\nresource control (resource control) Capability in which the attacker has control over the \\nresources consumed by a ML model, particularly for LLMs and RAG applications 39, 44 \\nRetrieval Augmented Generation (Retrieval Augmented Generation) This term refers \\nto retrieving data from outside a foundation model and augmenting prompts by adding the relevant retrieved data in context. RAG allows fne-tuning and modif-cation of the internal knowledge of the model in an effcient manner and without needing to retrain the entire model. First, the documents and user prompts are converted into a compatible format to perform relevancy search. Typically this is accomplished by converting the document collection and user prompts into numer -\\nical representations using embedding language models. RAG model architectures compare the embeddings of user prompts within the vector of the knowledge li-brary. The original user prompt is then appended with relevant context from sim-ilar documents within the knowledge library. This augmented prompt is then sent to the foundation model. For RAG to work well, the augmented prompt must ft into the context window of the model 3, 36, 37, 39, 44 \\nrowhammer attacks (rowhammer attacks) Rowhammer is a software-based fault-injection \\nattack that exploits DRAM disturbance errors via user-space applications and al-lows the attacker to infer information about certain victim secrets stored in memory cells. Mounting this attack requires attacker’s control of a user-space unprivileged process that runs on the same machine as the victim’s ML model 32 \\nS \\n97 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f7127855-1261-4270-b9d0-00ebeaeb90ff', embedding=None, metadata={'page_label': '98', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nsemi-supervised learning (semi-supervised learning) Type of machine learning in which \\na small number of training samples are labeled, while the majority are unlabeled 8 \\nshadow models (shadow models) Shadow models imitate the behavior of the target model. \\nThe training datasets and thus the ground truth about membership in these datasets \\nare known for these models. Typically, the attack model is trained on the labeled inputs and outputs of the shadow models 27 \\nside channel (side channel) side channels allow an attacker to infer information about a \\nsecret by observing nonfunctional characteristics of a program, such as execution time or memory or by measuring or exploiting indirect coincidental effects of the system or its hardware, like power consumption variation, electromagnetic em-anations, while the program is executing. Most commonly, such attacks aim to exfltrate sensitive information, including cryptographic keys 32 \\nsource code control (source code control) Capability in which the attacker has control over \\nthe source code of the machine learning algorithm 10, 39 \\nsupervised learning (supervised learning) Type of machine learning methods based on \\nlabeled data 8 \\nSupport Vector Machines (Support Vector Machines) A Support Vector Machine imple-\\nments a decision function in the form of a hyperplane that serves to separate (i.e., classify) observations belonging to one class from another based on patterns of information about those observations (i.e., features). 8, 9, 22, 32 \\nT \\ntar\\ngeted poisoning attacks (targeted poisoning attacks) Poisoning attacks against machine \\nlearning which change the prediction on a small number of targeted samples 9, 40 \\ntesting data control (testing data control) Capability in which the attacker has control over \\nthe testing data input to the machine learning model 10 \\ntraining data control (training data control) Capability in which the attacker has control \\nover a part of the training data of a machine learning model 10, 39 \\ntraining stage (training stage) Stage of machine learning pipeline in which the model is \\ntrained using training data 8, 37 \\ntrojans (trojans) A malicious code/logic inserted into the code of a software or hard-\\nware system, typically without the knowledge and consent of the organization that owns/develops the system, that is diffcult to detect and may appear harmless, but can alter the intended function of the system upon a signal from an attacker to cause a malicious behavior desired by the attacker. For Trojan attacks to be effective, the trigger must be rare in the normal operating environment so that it does not affect the normal effectiveness of the AI and raise the suspicions of human users 4, 54 \\nU \\nunsuper\\nvised learning (unsupervised learning) Type of machine learning methods based \\non unlabeled data 8 \\n98 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8f5d9ccb-a7b1-43e0-9a8c-7e0283e6893d', embedding=None, metadata={'page_label': '99', 'file_name': 'NIST.AI.100-2e2023.pdf', 'file_path': '/content/data/NIST.AI.100-2e2023.pdf', 'file_type': 'application/pdf', 'file_size': 1037794, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NIST AI 100-2e2023 \\nJanuary 2024 \\nW \\nwatermarking (w\\natermarking) a technique which embeds a hidden signal in a piece of \\ntext, image, or video to identify it as AI-generated. The signal can later be retrieved \\neven if the content has been modifed. Generally, the watermark must be accessible and reliable. The watermark is accessible if it is possible to test whether the content is AI-generated. There are two types of accessibility: public and private. Public access means everyone can access and verify it. Private access means that only the parties authorized by the organization controlling the generating model have the ability to access and verify the watermark. The watermark must be reliable in the sense that malicious actors should not be able to remove the watermark easily. However, in cases of public access to the watermark the attacker may be allowed to test for the presence of watermarks, which increases the technical challenges with watermarking 50 \\n99 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ac02d0fe-0134-4c59-9430-c47cf2c387ad', embedding=None, metadata={'page_label': '1', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded: A Survey of Machine Learning Security against\\nTraining Data Poisoning\\nANTONIO EMANUELE CINÀ∗,DAIS, Ca’ Foscari University of Venice, Italy\\nKATHRIN GROSSE∗,VITA Lab, École Polytechnique Fédérale de Lausanne, Switzerland\\nAMBRA DEMONTIS†,DIEE, University of Cagliari, Italy\\nSEBASTIANO VASCON, DAIS, Ca’ Foscari University of Venice and European Center for Living Technology, Italy\\nWERNER ZELLINGER, Software Competence Center Hagenberg GmbH (SCCH), Austria\\nBERNHARD A. MOSER, Software Competence Center Hagenberg GmbH (SCCH), Austria\\nALINA OPREA, Khoury College of Computer Sciences, Northeastern University, MA, USA\\nBATTISTA BIGGIO, DIEE, University of Cagliari, CINI, and Pluribus One, Italy\\nMARCELLO PELILLO, DAIS, Ca’ Foscari University of Venice, Italy\\nFABIO ROLI, DIBRIS, University of Genoa, CINI, and Pluribus One, Italy\\nThe success of machine learning is fueled by the increasing availability of computing power and large training datasets. The training\\ndata is used to learn new models or update existing ones, assuming that it is sufficiently representative of the data that will be\\nencountered at test time. This assumption is challenged by the threat of poisoning, an attack that manipulates the training data\\nto compromise the model’s performance at test time. Although poisoning has been acknowledged as a relevant threat in industry\\napplications, and a variety of different attacks and defenses have been proposed so far, a complete systematization and critical review\\nof the field is still missing. In this survey, we provide a comprehensive systematization of poisoning attacks and defenses in machine\\nlearning, reviewing more than 100 papers published in the field in the last 15 years. We start by categorizing the current threat models\\nand attacks, and then organize existing defenses accordingly. While we focus mostly on computer-vision applications, we argue that\\nour systematization also encompasses state-of-the-art attacks and defenses for other data modalities. Finally, we discuss existing\\nresources for research in poisoning, and shed light on the current limitations and open research questions in this research field.\\nCCS Concepts: •General and reference →Surveys and overviews ;•Computing methodologies →Neural networks ;Ma-\\nchine learning ;Adversarial learning .\\nAdditional Key Words and Phrases: Poisoning Attacks, Backdoor Attacks, Machine Learning, Computer Vision, Computer Security\\n∗Equal contribution\\n†Corresponding Author\\nAuthors’ addresses: Antonio Emanuele Cinà, antonioemanuele.cina@unive.it, DAIS, Ca’ Foscari University of Venice, Via Torino, 155, Venice, Italy,\\n30170; Kathrin Grosse, kathrin.grosse@unica.it, VITA Lab, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland, 1015; Ambra Demontis,\\nambra.demontis@unica.it, DIEE, University of Cagliari, Italy; Sebastiano Vascon, sebastiano.vascon@unive.it, DAIS, Ca’ Foscari University of Venice and\\nEuropean Center for Living Technology, Italy; Werner Zellinger, werner.zellinger@scch.at, Software Competence Center Hagenberg GmbH (SCCH),\\nSoftwarepark, 21, Hagenberg, Austria, 4232; Bernhard A. Moser, bernhard.moser@scch.at, Software Competence Center Hagenberg GmbH (SCCH),\\nAustria; Alina Oprea, a.oprea@northeastern.edu, Khoury College of Computer Sciences, Northeastern University, 360 Huntington Ave, Boston, MA, USA,\\n09123; Battista Biggio, battista.biggio@unica.it, DIEE, University of Cagliari, CINI, and Pluribus One, Italy; Marcello Pelillo, pelillo@unive.it, DAIS, Ca’\\nFoscari University of Venice, Italy; Fabio Roli, fabio.roli@unige.it, DIBRIS, University of Genoa, CINI, and Pluribus One, Via All’Opera Pia, 13, Genoa,\\nItaly, 09124.\\n1arXiv:2205.01992v3  [cs.LG]  9 Mar 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='33fd4e14-b9e4-4035-9180-1ed1ea280ed5', embedding=None, metadata={'page_label': '2', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2 Cinà, Grosse, et al.\\n1 INTRODUCTION\\nThe unprecedented success of machine learning (ML) in many diverse applications has been inherently dependent\\non the increasing availability of computing power and large training datasets, under the implicit assumption that\\nsuch datasets are well representative of the data that will be encountered at test time. However, this assumption\\nmay be violated in the presence of data poisoning attacks, i.e., if attackers can either compromise the training data,\\nor gain some control over the learning process (e.g., when model training is outsourced to an untrusted third-party\\nservice) [ 43,70,126,134]. Poisoning attacks are staged at training time, and consist of manipulating the training data to\\ndegrade the model’s performance at test time. Three main categories of data poisoning attacks have been investigated\\nso far [ 39]. These include indiscriminate, targeted, and backdoor poisoning attacks. In indiscriminate poisoning attacks,\\nthe attacker manipulates a fraction of the training data to maximize the classification error of the model on the (clean)\\ntest samples. In targeted poisoning attacks, the attacker manipulates again a subset of the training data, but this time\\nto cause misclassification of a specific set of (clean) test samples. In backdoor poisoning attacks, the training data is\\nmanipulated by adding poisoning samples containing a specific pattern, referred to as the backdoor trigger, and labeled\\nwith an attacker-chosen class label. This typically induces the model to learn a strong correlation between the backdoor\\ntrigger and the attacker-chosen class label. Accordingly, at test time, the input samples that embed the trigger are\\nmisclassified as samples of the attacker-chosen class.\\nAlthough many different attacks can be staged against ML models, a recent survey shows that poisoning is the\\nlargest concern for ML deployment in industry [ 68,95]. Furthermore, several sources confirm that poisoning is already\\ncarried out in practice [ 68,119]. For example, Microsoft’s chatbot Tay1was designed to learn language by interacting\\nwith users, but instead learned offensive statements. Chatbots in other languages have shared its fate, including a\\nChinese2and a Korean3version. Another attack showed how to poison the auto-complete feature in search engines.4\\nFinally, a group of extremists submitted wrongly-labeled images of portable ovens with wheels tagging them as Jewish\\nbaby strollers to poison Google’s image search.5Due to their practical relevance, various scientific articles have been\\npublished on training-time attacks against ML models. While the vast majority of the poisoning literature focuses on\\nsupervised classification models in the computer vision domain, we would like to remark here that data poisoning\\nhas been investigated earlier in cybersecurity [ 126,134], and more recently also in other application domains, like\\naudio [ 1,91] and natural language processing [ 34,206], and against different learning methods, such as federated\\nlearning [4, 191], unsupervised learning [17, 41], and reinforcement learning [10, 205].\\nWithin this survey paper, we provide a comprehensive framework for threat modeling of poisoning attacks and\\ncategorization of defenses. We identify the main practical scenarios that enable staging such attacks on ML models,\\nand use our framework to properly categorize attacks and defenses. We then review their historical development, also\\nhighlighting the main current limitations and the corresponding future challenges. We do believe that our work can\\nserve as a guideline to better understand how and when these attacks can be staged, and how we can defend effectively\\nagainst them, while also giving a perspective on the future development of trustworthy ML models limiting the impact\\nof malicious users. With respect to existing surveys in the literature on ML security, which either consider a high-level\\noverview of the whole spectrum of attacks on ML [ 19,26], or are specific to an application domain [ 159,187], our work\\nfocuses solely on poisoning attacks and defenses, providing a greater level of detail and a more specific taxonomy. Other\\n1https://www.theguardian.com/technology/2016/mar/26/microsoft-deeply-sorry-for-offensive-tweets-by-ai-chatbot\\n2https://www.khaleejtimes.com/technology/ai-getting-out-of-hand-chinese-chatbots-re-educated-after-rogue-rants\\n3https://www.vice.com/en/article/akd4g5/ai-chatbot-shut-down-after-learning-to-talk-like-a-racist-asshole\\n4http://www.nickdiakopoulos.com/2013/08/06/algorithmic-defamation-the-case-of-the-shameless-autocomplete/\\n5https://www.timebulletin.com/jewish-baby-stroller-image-algorithm/', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='35f878b3-bdfc-490d-9629-9c67ea2d6f7b', embedding=None, metadata={'page_label': '3', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 3\\nsurvey papers on poisoning attacks do only consider backdoor attacks [ 59,88,103], except for the work by Goldblum\\net al. [64] and Tian et al . [164] . Our survey is complementary to recent work in [ 64,164]; in particular, while in [ 64,164]\\nthe authors give an overview of poisoning attacks and countermeasures in centralized and federated learning settings,\\nour survey: (i) categorizes poisoning attacks and defenses in the centralized learning setting, based on a more systematic\\nthreat modeling; (ii) introduces a unified optimization framework for poisoning attacks, matches the defenses with\\nthe corresponding attacks they prevent, and (iii) discusses the historical timeline of poisoning attacks since the early\\ndevelopments in cybersecurity applications of ML, dating back to more than 15 years ago.\\nWe start our review in Sect. 2, with a detailed discussion on threat modeling for poisoning attacks, and on the\\nunderlying assumptions needed to defend against them. This includes defining the learning settings where data\\npoisoning attacks (and defenses) are possible. We further highlight the different attack strategies that give us a scaffold\\nfor a detailed overview of data poisoning attacks in Sect. 3. Subsequently, in Sect. 4, we give an overview of the main\\ndefense mechanisms proposed to date against poisoning attacks, including training-time and test-time defense strategies.\\nWhile our survey is mostly focused on poisoning classification models for computer vision, which encompasses most\\nof the work related to poisoning attacks and defenses, in Sect. 5 we discuss related work that has been developed in\\ndifferent contexts. In Sect. 6, we discuss poisoning research resources such as libraries and dataset containing poisoned\\nmodels. Finally, in Sect. 7 we review the historic development of poisoning attacks and defenses. This overview serves\\nas a basis for discussing ongoing challenges in the field, such as limitations of current threat models, the design of more\\nscalable attacks, and the arms race towards designing more comprehensive and effective defenses. For each of these\\npoints, we discuss open questions and related future work.\\nTo summarize, this work provides the following contributions: (i) we propose a unifying framework for threat\\nmodeling of poisoning attacks and systematization of defenses; (ii) we categorize around 45attack approaches in\\ncomputer vision according to their assumptions and strategies; (iii) we provide a unified formalization for optimizing\\npoisoning attacks via bilevel programming; (iv) we categorize more than 70defense approaches in computer vision,\\ndefining six distinct families of defenses; (v) we take advantage of our framework to match specific attacks with\\nappropriate defenses according to their strategies; (vi) we discuss state-of-the-art libraries and datasets as resources for\\npoisoning research; and (vii) we show the historical development of poisoning research and derive open questions,\\npressing issues, and challenges within the field of poisoning research. Finally, we also derive a unified formalization for\\noptimizing poisoning attacks via bilevel programming, and investigate in the supplementary material in which other\\ndomains poisoning attacks and defenses have been developed.\\n2 MODELING POISONING ATTACKS AND DEFENSES\\nWe discuss here how to categorize poisoning attacks against learning-based systems. We start by introducing the\\nnotation and symbols used throughout this paper in Table 1. In the remainder of this section, we define the learning\\nsettings under which poisoning attacks have been investigated. We then revisit the framework by Muñoz-González et al .\\n[124] to systematize poisoning attacks according to the attacker’s goal, knowledge of the target system, and capability\\nof manipulating the input data. We conclude by characterizing the defender’s goal, knowledge, and capability.\\n2.1 Learning Settings\\nWe define here the three main scenarios under which ML models can be trained, and which can pose serious concerns in\\nrelationship to data poisoning attacks. We refer to them below respectively as (i) training-from-scratch , (ii) fine-tuning ,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ecbfcc8d-98e2-4db2-bf19-42aae9fcd0b4', embedding=None, metadata={'page_label': '4', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4 Cinà, Grosse, et al.\\nTable 1. Notation and symbols used in this survey.\\nData Model Noise\\nDClean samples in training set 𝜽Model’s parameters 𝒕Test data perturbation\\nD𝑝Poisoning samples in training set 𝜙Model’s feature extractor 𝜹Training data perturbation\\nD′Poisoned training set ( D′=D∪D𝑝)𝑓Model’s classifier ΔSet of admissible manipulations for 𝜹\\nTraining Attack Strategy\\nVClean samples in validation dataset MMachine learning model BL Bilevel\\nV𝑡Attacker target samples in validation dataset WLearning algorithm FC Feature Collision\\n𝐿Loss function T𝑃Patch Trigger\\nTTest samples LTraining loss (regularized) T𝑆Semantical Trigger\\n𝑝Percentage of poisoned data T𝐹Functional Trigger\\nand (iii) model-training . In Fig. 1, we conceptually represent these settings, along with the entry points of the attack\\nsurface which enable staging a poisoning attack.\\nTraining from Scratch (TS) and Fine Tuning (FT). In the training-from-scratch andfine-tuning scenarios, the user\\ncontrols the training process, but collects the training data from external repositories, potentially compromised by\\nattackers. In practice, these are the cases where data gathering and labeling represent time-consuming and expensive\\ntasks that not all organizations and individuals can afford, forcing them to collect data from untrusted external sources.\\nThe distinction between the two scenarios hinges on how the collected data are employed during training. In the\\ntraining-from-scratch scenario, the collected data is used to train the model from a random initialization of its weights.\\nIn the fine-tuning setting, instead, a pretrained model is typically downloaded from an untrusted source, and used to\\nmap the input samples on a given representation space induced by a feature mapping function 𝜙. Then, a classification\\nfunction𝑓is fine tuned on top of the given representation 𝜙.\\nModel Training (MT). In the model-training (outsourcing) scenario, the user is supposed to have limited computational\\ncapacities and outsources the whole training procedure to an untrusted third party, while providing the training dataset.\\nThe resulting model can then be provided either as an online service which the user can access via queries, or given\\ndirectly to the user. In this case, both the feature mapping 𝜙and the classification function 𝑓are trained by the attacker\\n(i.e., the untrusted party). The user, however, can validate the model’s accuracy on a separate validation dataset to\\nensure that the model meets the desired performance requirements.\\n2.2 Attack Framework\\n2.2.1 Attacker’s Goal. The goal of a poisoning attack can be defined in terms of the intended security violation, and\\nthe attack and error specificity, as detailed below.\\nSecurity Violation. It defines the security violation caused by the attack, which can be: (i) an integrity violation, if\\nmalicious activities evade detection without compromising normal system operation; (ii) an availability violation, if\\nnormal system functionality is compromised, causing a denial of service for legitimate users; or (iii) a privacy violation,\\nif the attacker aims to obtain private information about the system itself, its users, or its data.\\nAttack Specificity. It determines which samples are subject to the attack. It can be: (i) sample-specific (targeted), if a\\nspecific set of sample(s) is targeted by the attack, or (ii) sample-generic (indiscriminate), if any sample can be affected.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='86a3d09a-e217-4e7e-afcf-34517d6d4b92', embedding=None, metadata={'page_label': '5', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 5\\nFig. 1. Training (left) and test (right) pipeline. The victim collects a training dataset D′from an untrusted source. The training or\\nfine-tuning algorithm uses these data to train a model M, composed of a feature extractor 𝜙, and a classification layer 𝑓. In the\\ncase of fine-tuning, only 𝑓is modified, while the feature representation 𝜙is left untouched. At test time, some test samples may be\\nmanipulated by the attacker to exploit the poisoned model and induce misclassification errors.\\nError Specificity. It determines how the attack influences the model’s predictions. It can be: (i) error-specific , if the\\nattacker aims to have a sample misclassified as a specific class; or (ii) error-generic , if the attacker attempts to have a\\nsample misclassified as any class different from the true class.\\n2.2.2 Attacker’s Knowledge. The attacker may get to know some details about the target system, including information\\nabout: (i) the (clean) training data D, (ii) the ML model Mbeing used, and (iii) the test data T. The first component\\nconsiders how much knowledge the attacker has on the training data. The second component refers to the ability of\\nthe attacker to access the target model, including its internal (trained) parameters 𝜽, but also additional information\\nlike hyperparameters, initialization, and the training algorithm. The third component specifies if the attacker knows\\nin advance (or has access to) the samples that should be misclassified at test time. Although not explicitly mentioned\\nin previous work, we have found that the knowledge of test samples is crucial for some attacks to work as expected.\\nClearly, attacks that are designed to work on specific test instances are not expected to generalize to different test\\nsamples (e.g., to other samples belonging to the same class). Depending on the combination of the previously-defined\\nproperties, we can define two main attack settings, as detailed below.\\nWhite-Box Attacks. The attacker has complete knowledge about the targeted system. Although not always repre-\\nsentative of practical cases, this setting allows us to perform a worst-case analysis, and it is particularly helpful for\\nevaluating defenses.\\nBlack-Box Attacks. Black-box attacks can be subdivided into two main categories: black-box transfer attacks, and\\nblack-box query attacks. Although generally referred to as a black-box attack, black-box transfer attacks assume that the\\nattacker has partial knowledge of the training data and/or the target model. In particular, the attacker is assumed to be\\nable to collect a surrogate dataset and use it to train a surrogate model approximating the target. Then, white-box attacks\\ncan be computed against the surrogate model, and subsequently transferred against the target model. Under some mild\\nconditions, such attacks have been shown to transfer successfully to the target model with high probability [ 45]. It is\\nalso worth remarking that black-box query attacks can also be staged against a target model, by only sending input\\nqueries to the model and observing its predictions to iteratively refine the attack, without exploiting any additional\\nknowledge [ 31,132,167]. However, to date, most of the poisoning attacks staged against learning algorithms in black-box\\nsettings exploit surrogate models and attack transferability.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='81098420-6660-4c6d-9ab6-0cfe1cd792e5', embedding=None, metadata={'page_label': '6', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6 Cinà, Grosse, et al.\\nPatchFeature CollisionSemantical\\nBilevel\\nFunctional\\nSignalBlendingWarping\\nclean sampletrigger δbackdoor sample\\nclean sampletrigger δbackdoor sample\\nh(,)\\nFig. 2. Visual examples of data perturbation noise ( 𝜹) categories. The first five figures show some examples of patch, functional,\\nand semantical triggers. For functional triggers we consider signal [ 8], blending [ 33], and warping [ 129] transformations. The\\nremaining two depict poisoning samples crafted with a bilevel attack with visible noise, and a clean-label feature collision attack with\\nimperceptible noise. The second row shows the backdoor image generation process with patch and functional blending triggers. For\\nthe latter, aℎmanipulation function blends the original image and the backdoor trigger according to a certain ratio.\\n2.2.3 Attacker’s Capability. The attacker’s capability is defined in terms of how the attacker can influence the learning\\nsetting , and on the data perturbation that can be applied to training and/or test samples.\\nInfluence on Learning Setting. The three learning settings described in Sect. 2.1 open the door towards different\\ndata poisoning attacks. In both training-from-scratch (TS) and fine-tuning (FT) scenarios, the attacker alters a subset of\\nthe training dataset collected and used by the victim to train or fine-tune the machine learning model. Conversely, in\\nthemodel-training (MT) scenario, as firstly hypothesized by Gu et al . [70] , the attacker acts as a malicious third-party\\ntrainer, or as a man-in-the-middle, controlling the training process. The attacker tampers with the training procedure\\nand returns to the victim user a model that behaves according to their goal. The advantage for the attacker is the victim\\nwill never be aware of the training dataset actually used. However, to keep their attack stealthy, the attacker must\\nensure that the provided model retains high prediction accuracy, making sure to pass the validation phase without\\nsuspicion from the victim user. The attacker’s knowledge, discussed in Sect. 2.2.2, is defined depending on the setting\\nunder consideration. In the model-training andtraining-from-scratch settings,D′andMrefer to the training data and\\nalgorithm used for training the model from random initialization of its weights. Conversely, in the fine-tuning setting,\\nD′andMrefer to the fine-tuning dataset and learning algorithm, respectively.\\nData Perturbation. Staging a poisoning attack requires the attacker to manipulate a given fraction ( 𝑝) of the training\\ndata. In some cases, i.e., in backdoor attacks, the attacker is also required to manipulate the test samples that are\\nunder their control, by adding an appropriate trigger to activate the previously-implanted backdoor at test time. More\\nspecifically, poisoning attacks can alter a fraction of the training labels and/or apply a (different) perturbation to each\\nof the training (poisoning) samples. If the attack only modifies the training labels, but it does not perturb any training\\nsample, it is often referred to as a label-flip poisoning attack. Conversely, if the training labels are not modified (e.g., if\\nthey are validated or assigned by human experts or automated labeling procedures), the attacker can stage a so-called\\nclean-label poisoning attack. Such attacks only slightly modify the poisoning samples, using imperceptible perturbations\\nthat preserve the original semantics of the input samples along with their class labels [ 148]. We define the strategies\\nused to manipulate training and test data in poisoning attacks in the next section.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fa084f18-0cd0-4be8-9cb8-8f5327d1c3f5', embedding=None, metadata={'page_label': '7', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 7\\n2.2.4 Attack Strategy. The attack strategy defines how the attacker manipulates data to stage the desired poisoning\\nattack. Both indiscriminate and targeted poisoning attacks only alter the training data, while backdoor attacks also\\nrequire embedding the trigger within the test samples to be misclassified. We revise the corresponding data manipulation\\nstrategies in the following.\\nTraining Data Perturbation ( 𝜹).Two main categories of perturbation have been used to mount poisoning attacks. The\\nformer includes perturbations which are found by solving an optimization problem, either formalized as a bilevel (BL)\\nprogramming problem, or as a feature-collision (FC) problem. The latter involves the manipulation of training samples\\nin targeted and backdoor poisoning attacks such that they collide with the target samples in the given representation\\nspace, to induce misclassification of such target samples in an attacker-chosen class. When it comes to backdoor attacks,\\nthree main types of triggers exist, which can be applied to training samples to implant the backdoor during learning:\\npatch triggers (T𝑃), which consist of replacing a small subset of contiguous input features with a patch pattern in\\nthe input sample; functional triggers (T𝐹), which are embedded into the input sample via a blending function; and\\nsemantical triggers (T𝑆), which perturb the given input while preserving its semantics (e.g., modifying face images\\nby adding sunglasses, or altering the face expression, but preserving the user identity). The choice of this strategy\\nplays a fundamental role since it influences the computational effort, effectiveness, and stealthiness of the attack.\\nMore concretely, the trigger strategies are less computationally demanding, as they do not require optimizing the\\nperturbation, but the attack may be less effective and easier to detect. Conversely, an optimized approach can enhance\\nthe effectiveness and stealthiness of the attack, at the cost of being more computationally demanding. In Fig. 2 we give\\nsome examples of patch, functional, and semantical triggers, one example of a poisoning attack optimized with bilevel\\nprogramming, and one example of a clean-label feature-collision attack.\\nTest Data Perturbation ( 𝒕).During operation, i.e., at test time, the attacker can submit malicious samples to exploit\\npotential vulnerabilities that were previously implanted during model training, via a backdoor attack. In particular, as\\nwe will see in Sect. 3.3, backdoor attacks are activated when a specific trigger 𝒕is present in the test samples. Normally,\\nthe test-time trigger is required to exactly match the trigger implanted during training, thus including patch, functional,\\nand semantical triggers.\\n2.3 Defense Framework\\nIn this section, we introduce the main strategies that can be used to counter poisoning attacks, based on different\\nassumptions made on the defender’s goal, knowledge and capability.\\n2.3.1 Defender’s Goal. The defender’s goal is to preserve the integrity, availability, and privacy of their ML model, i.e.,\\nto mitigate any kind of security violation that might be caused by an attack. The defender thus adopts appropriate\\ncountermeasures to alleviate the effect of possible attacks, without significantly affecting the behavior of the model for\\nlegitimate users.\\n2.3.2 Defender’s Knowledge and Capability. The defender’s knowledge and capability determine in which learning\\nsetting a defense can be applied. We identify four aspects that influence how the defender can operate to protect the\\nmodel: (i) having access to the (poisoned) training data D′, and to (ii) a separate, clean validation set V, and (iii) having\\ncontrol on the training procedure W, and on (iv) the model’s parameters 𝜽. We will see in more detail how these\\nassumptions are matched to each defense in Sect. 4.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6af37298-03b9-4905-83be-c530380598e7', embedding=None, metadata={'page_label': '8', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8 Cinà, Grosse, et al.\\n2.3.3 Defense Strategy. The defense strategy defines how the defender operates to protect the system from malicious\\nattacks before deployment (i.e., at training time), and after the model’s deployment (i.e., at test time). We identify six\\ndistinct categories of defenses:\\n(1)training data sanitization , which aims to remove potentially-harmful training points before training the model;\\n(2)robust training , which alters the training procedure to limit the influence of malicious points;\\n(3)model inspection , which returns for a given model whether it has been compromised (e.g., by a backdoor attack);\\n(4)model sanitization , which cleans the model to remove potential backdoors or targeted poisoning attempts;\\n(5)trigger reconstruction , which recovers the trigger embedded in a backdoored network; and\\n(6)test data sanitization , which filters potentially-triggered samples presented at test time.\\nThese defenses essentially work by either (i) cleaning the data or (ii) modifying the model. In the former case, the\\ndefender aims to sanitize training or test data. Training data sanitization andtest data sanitization as thus two strategies\\nadopted respectively at training and at test time to mitigate the influence of data poisoning attacks. Alternatively,\\nthe defender can act directly on the model, by (i) identifying possible internal vulnerabilities and removing/fixing\\ncomponents that lead to anomalous behavior/classifications, or by (ii) changing the training procedure to make the\\nmodel less susceptible to training data manipulations. The first approach is employed in model inspection ,trigger\\nreconstruction andmodel sanitization defensive mechanisms. The second approach, instead, includes algorithms that\\noperate at the training level to implement robust training mechanisms.\\n2.4 Poisoning Attacks and Defenses\\nWe provide in Fig. 3 a preliminary, high-level categorization of attacks and defenses according to our framework (while\\nleaving a more complete categorization of each work to Tables 2-3, respectively for attacks and defenses). This simplified\\ntaxonomy categorizes attacks and defenses based on whether they are applied at training time (and in which learning\\nsetting) or at test time; whether the attack aims to violate integrity or availability;6and whether the defense aims to\\nsanitize data or modify the learning algorithm/model. As one may note, indiscriminate and targeted poisoning only\\nmanipulate data at training time to violate availability and integrity, respectively, and they are typically staged in the\\ntraining-from-scratch (TS) or fine-tuning (FT) learning settings. Backdoor attacks, in addition, require manipulating the\\ntest data to embed the trigger and cause the desired misclassifications, with the goal of violating integrity. Such attacks\\ncan be ideally staged in any of the considered learning settings. For defenses, data sanitization strategies can be applied\\neither at training time or at test time, while defenses that modify the learning algorithm or aim to sanitize the model\\ncan be applied clearly only at training time (i.e., before model deployment). To conclude, while being simplified, we do\\nbelieve that this conceptual overview of attacks and defenses provides a comprehensive understanding of the main\\nassumptions behind each poisoning attack and defense strategy. Accordingly, we are now ready to delve into a more\\ndetailed description of attacks and defenses in Sects. 3 and 4.\\n3 ATTACKS\\nWe now take advantage of the previous framework to give an overview of the existing attacks according to the\\ncorresponding violation and strategy. A compact summary of all attacks from the vision domain is given in Table 2.\\n6To our knowledge, no poisoning attack violating a model’s privacy has been considered so far, so we omit the privacy dimension from this representation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='510e426c-8db3-4c8d-b182-4d62f29d3d9f', embedding=None, metadata={'page_label': '9', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 9\\nFig. 3. Conceptual overview of poisoning attacks and defenses according to our framework. Attacks are categorized based on whether\\nthey compromise system integrity or availability. Defenses are categorized based on whether they sanitize data or modify the learning\\nalgorithm/model. Training-time (test-time) defenses are applied before (after) model deployment. Training-time interventions are also\\ndivided according to whether model-training (MT) is outsourced, or training-from-scratch (TS) / fine-tuning (FT) is performed.\\n(a) Indiscriminate attack.\\nclean target stop sign \\nclassiﬁed as speedlimit \\n (b) Targeted attack.\\nbackdoored stop \\nsigns classiﬁed as \\nspeedlimit \\n (c) Backdoor attack.\\nFig. 4. Conceptual representation of the impact of indiscriminate, targeted, and backdoor poisoning on the learned decision function.\\nWe depict the feature representations of the speed limit sign (red dots) and stop signs (blue dots). The poisoning samples (solid black\\nborder) change the original decision boundary (dashed gray) to a poisoned variant (dashed black).\\n3.1 Indiscriminate (Availability) Poisoning Attacks\\nIndiscriminate poisoning attacks represent the first class of poisoning attacks against ML algorithms. The attacker aims\\nto subvert the system functionalities, compromising its availability for legitimate users by poisoning the training data.\\nMore concretely, the attacker’s goal is to cause misclassification on clean validation samples by injecting new malicious\\nsamples or perturbing existing ones in the training dataset. In Fig. 4a we consider the case where an attacker poisons a\\nlinear street-sign classifier to have stop signs misclassified as speed limits. The adversary injects poisoning samples\\nto rotate the classifier’s decision boundary, thus compromising the victim’s model performance. In the following, we\\npresent the strategies developed in existing works, and we categorize them in Table 2. Although they could also operate\\non the fine-tuning (FT) scenario, existing works have been proposed only in the training-from-scratch (TS) setting.\\nBy contrast, their application in the model-training (MT) scenario would not be feasible, as the model, with reduced\\naccuracy due to the attack, would not pass the user validation phase. Indiscriminate attacks, to be adaptable in the\\nlatter scenario, must compromise the availability of the system but not in terms of increasing the classification error.\\nThis has been recently done by Cinà et al . [38] , who proposed a so-called sponge poisoning attack aimed to increase the\\nmodel’s prediction latency.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='56d1428b-4523-4199-962b-d15c5c4b4fbd', embedding=None, metadata={'page_label': '10', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10 Cinà, Grosse, et al.\\nTable 2. Taxonomy of existing poisoning attacks, according to the attack framework defined in Sect. 2. The presence of the ✓indicates\\nthat the corresponding properties is satisfied by the attack. For the attacker’s knowledge we use: #when the attacker has knowledge\\nof the corresponding component; G #if the attacker uses a surrogate to mount the attack;  if the attacker does not require that\\nknowledge. In the attacker’s capabilities we use MT, TS and FT as acronyms for model-training ,training-from-scratch , and fine-tuning\\nlearning settings.\\n ,\\n,\\nrepresent the amount of poisoning: small ( ≤10%), medium (≤30%), or high percentage of the training\\nset. The columns 𝜹and𝒕define the training and test strategies: optimized bilevel – BL, feature collision – FC and trigger – T.SectionAttacksGoal Knowledge Capability Strategy Model\\nSample\\nSpecificError\\nSpecificDM T SettingClean\\nLabelp 𝜹 𝒕 DNNIndiscriminate\\n3.1.1Biggio et al. [15], Xiao et al. [190], # # TS\\nLF -Xiao et al. [189], Paudice et al. [133] # # TS\\n3.1.2Biggio et al. [16], Xiao et al. [188]# # TS\\nBL -Frederickson et al. [58]\\nBetaPoison [42], Ma et al. [116] ✓# G # TS\\nDemontis et al. [45], Solans et al. [153] G # G # TS\\nMuñoz-González et al. [124], Yang et al. [194] G # G # TS\\n ✓3.1.3Mei and Zhu [121] ✓# # TS ✓\\nBL - Feng et al. [53] ✓G # G # TS ✓\\n ✓\\nFowl et al. [55] ✓G # G # TS ✓\\n ✓Targeted\\n3.2.1Koh and Liang [92] ✓ ✓# # ✓ FT\\nBL - Muñoz-González et al. [124] ✓ ✓G # G # TS\\n ✓\\nJagielski et al. [84] ✓G # G # TS/FT\\n ✓3.2.2PoisonFrog [148] ✓ ✓# # ✓ FT ✓\\nFC -✓\\nGuo and Liu [72], StingRay[158]✓ ✓G # G # ✓ FT ✓\\n ✓ConvexPolytope [212], BullseyePolytope [2] 3.2.3Geiping et al. [62], MetaPoison [80] ✓ ✓G # G # ✓ TS ✓\\n BL - ✓Backdoor\\n3.3.1BadNet [70], LatentBackdoor [196] ✓ ✓# # MT\\nT𝑃T𝑃✓\\nBaN [143] ✓ ✓# # MT\\n ✓\\nTrojanNN [110] ✓ ✓ # MT\\n ✓3.3.1WaNET [129], Li et al. [99], DFST [36] ✓ ✓# # MT\\nT𝐹T𝐹✓\\nRefool [111] ✓ ✓# # TS ✓\\n ✓\\nSIG [8] ✓ ✓  TS ✓\\n ✓\\nChen et al. [33], Zhong et al. [211] ✓ ✓  TS/FT\\n ✓3.3.1FaceHack [145] ✓ ✓# # MT\\nT𝑆T𝑆✓\\nChen et al. [33] ✓ ✓  TS/FT\\n ✓\\nWenger et al. [179] ✓ ✓#  FT\\n ✓3.3.2Nguyen and Tran [128], LIRA [48] ✓ ✓# # MT\\nBLT𝐹✓\\nLi et al. [99] ✓ ✓ # MT\\n ✓\\nLi et al. [101] ✓ ✓# G # TS\\n ✓\\nZhong et al. [211] ✓ ✓G # G # TS/FT\\n ✓3.3.3HiddenTrigger [142] ✓ ✓# # FT ✓\\nFCT𝑃✓\\nTurner et al. [169] ✓ ✓G # G # TS ✓\\n ✓3.3.4Souri et al. [156] ✓ ✓G # G # TS ✓\\n BLT𝑃✓', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='599324c9-4dc9-4a44-8bab-be5c2fb1184f', embedding=None, metadata={'page_label': '11', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 11\\n3.1.1 Label-Flip Poisoning. The most straightforward strategy to stage poisoning attacks against ML is label-flip,\\noriginally proposed by Biggio et al . [15] . The adversary does not perturb the feature values, but they mislabel a subset\\nof samples in the training dataset, compromising the performance accuracy of ML models such as Support Vector\\nMachines (SVMs). Beyond that, Xiao et al . [190] showed that random flips could have far-from-optimal performance,\\nwhich nevertheless would require solving an NP-hard optimization problem. Due to its intractability, heuristic strategies\\nhave been proposed by Xiao et al . [190] , and later by Xiao et al . [189] , to efficiently approximate the optimal formulation.\\n3.1.2 Bilevel Poisoning. In this case, the attacker manipulates both the training samples and their labels. The pioneering\\nwork in this direction was proposed by Biggio et al . [16] , where a gradient-based indiscriminate poisoning attack is\\nexploited against SVMs. They exploited implicit differentiation to derive the gradient required to optimize the poisoning\\nsamples by their iterative algorithm. Until convergence, the poisoning samples are iteratively updated following the\\nimplicit gradient, directing towards maximization of the model’s validation error. Mathematically speaking, this idea\\ncorresponds to treating the poisoning task as a bilevel optimization problem:\\nmax\\n𝜹∈Δ𝐿(V,M,𝜽★), (1)\\ns.t. 𝜽★∈arg min\\n𝜽L(D∪D𝜹\\n𝑝,M,𝜽). (2)\\nwithΔbeing the set of admissible manipulation of the training samples that preserve the constraints imposed by the\\nattackers (e.g., ℓ𝑝, or box-constraints)7. We define withD𝑝={(𝒙𝑖,𝑦𝑖)}𝑛\\n𝑖=1the training data controlled by the attacker,\\nbefore any perturbation is applied, being 𝑦𝑖the pristine label of sample 𝒙𝑖and𝑛the number of samples in D𝑝. We\\nthen denote withD𝜹𝑝the corresponding poisoning dataset manipulated according to the perturbation parameter 𝜹.\\nThe attacker optimizes the perturbation 𝜹(applied to the poisoning samples D𝑝) to increase the error/loss 𝐿of the\\ntarget modelMon the clean validation samples V. Our formulation in Eqs. (1)-(2)encompass both dirty or clean-label\\nattacks according to the nature of D𝜹𝑝. For example, we can define D𝜹𝑝={(𝒙𝑖+𝜹𝑖,𝑦′\\n𝑖)}𝑛\\n𝑖=18, being𝑦′\\n𝑖the poisoning\\nlabel chosen by the attacker, with 𝑦′\\n𝑖=𝑦𝑖for a clean-label attack and 𝑦′\\n𝑖≠𝑦𝑖for a dirty-label attack. Solving this\\nbilevel optimization is challenging, since the inner and the outer problems in Eqs. (1)-(2)have conflicting objectives.\\nMore concretely, the inner objective is a regularized empirical risk minimization, while the outer one is empirical\\nrisk maximization, both considering data from the same distribution. A similar approach was later generalized in\\nXiao et al . [188] and Frederickson et al . [58] to target feature selection algorithms (i.e., LASSO, ridge regression, and\\nelastic net). Subsequent work tried to analyze the robustness of ML models when the attacker has limited knowledge\\nabout the training dataset or the victim’s classifier. In this scenario, the most investigated methodology is given by the\\ntransferability of the attack [ 45,116,153]. The attacker crafts the poisoning samples using surrogate datasets and/or\\nmodels, and then transfers the attack to another target model. This approach has proven effective for corrupting logistic\\nclassifiers [ 45], algorithmic fairness [ 153], and differentially-private learners [ 116]. More details about the transferability\\nof poisoning attacks are reported in Sect. 3.5.\\nDifferently from previous work, Cinà et al . [42] observed that a simple heuristic strategy, together with a variable\\nreduction technique, can reach noticeable results against linear classifiers, with increased computational efficiency.\\nMore concretely, the authors showed how previous gradient-based approaches can be affected by several factors (e.g.,\\nloss landscape) that degrade their performance in terms of computation time and attack efficiency.\\n7For example, the attacker can constraint the perturbation magnitude of 𝜹imposing∥𝜹∥𝑝≤𝜖withΔ={𝜹∈R𝑛×𝑑|∥𝜹∥𝑝≤𝜖}.\\n8In this example we used 𝜹as additive noise. To be more generic we can define a manipulation function ℎparametrized by 𝜹and the sample 𝒙to perturb.\\nSee example in Fig. 2 for functional blending trigger.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3be6281e-63c5-4e61-9e03-af7fd97566b5', embedding=None, metadata={'page_label': '12', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12 Cinà, Grosse, et al.\\nAlthough effective, the aforementioned poisoning attacks have been designed to fool models with a relatively small\\nnumber of parameters. More recently, Muñoz-González et al . [124] showed that devising poisoning attacks against\\nlarger models, such as convolutional neural networks, can be computationally and memory demanding. To this end,\\nMuñoz-González et al . [124] pioneered the idea to adapt hyperparameter optimization methods, which aims to solve\\nbilevel programming problems more efficiently, in the context of poisoning attacks. The authors indeed proposed\\na back-gradient descent technique to optimize poisoning samples, drastically reducing the attack complexity. The\\nunderlying idea is to back-propagate the gradient of the objective function to the poisoning samples while learning the\\npoisoned model. However, they assume the objective function is sufficiently smooth to trace the gradient backward\\ncorrectly. Accordingly with the results in [ 124], Yang et al . [194] showed that computing the analytical or estimated\\ngradient of the validation loss in Eq. (1)with respect to the poisoning samples can be as well computational and query\\nexpensive. Another way explored in Yang et al . [194] was to train a generative model from which the poisoning samples\\nare generated, thus increasing the generation rate.\\n3.1.3 Bilevel Poisoning (Clean-Label). Previous work examined in Sect. 3.1.2 assumes that the attacker has access\\nto a small percentage of the training data and can alter both features and labels. Similar attacks have been staged by\\nassuming that the attacker can control a much larger fraction of the training set, while only slightly manipulating\\neach poisoning sample to preserve its class label, i.e., performing a clean-label attack. This idea was introduced by Mei\\nand Zhu [121] , who considered manipulating the whole training set to arbitrarily define the importance of individual\\nfeatures on the predictions of convex learners. More recently, DeepConfuse [ 53] and Fowl et al . [55] proposed novel\\ntechniques to mount clean-label poisoning attacks against DNNs. In [ 53], the attacker trains a generative model,\\nsimilarly to [ 194], to craft clean-label poisoning samples which can compromise the victim’s model. Inspired by recent\\ndevelopments proposed in [ 62], Fowl et al . [55] used a gradient alignment optimization technique to alter the training\\ndata imperceptibly, but diminishing the model’s performance. Even though Feng et al . [53] and Fowl et al . [55] can\\ntarget DNNs, the attacker is assumed to perturb a high fraction of samples in the training set. We do believe that this is\\na very demanding setting for poisoning attacks. In fact, such attacks are often possible because ML is trained on data\\ncollected in the wild (e.g., labeled through tools such as a mechanical Turk) or crowdsourced from multiple users; thus,\\nit would be challenging for attackers in many applications to realistically control a substantial fraction of these training\\ndata. In conclusion, the quest for scalable, effective, and practical indiscriminate poisoning attacks on DNNs is still open.\\nAccordingly, it remains also unclear whether DNNs can be significantly subverted by such attacks in practical settings.\\n3.2 Targeted (Integrity) Poisoning Attacks\\nIn contrast to indiscriminate poisoning, targeted poisoning attacks preserve the availability, functionality and behavior of\\nthe system for legitimate users, while causing misclassification of some specific target samples. Similarly to indiscriminate\\npoisoning, targeted poisoning attacks manipulate the training data but they do not require modifying the test data.\\nAn example of a targeted attack is given in Fig. 4b, where the classifier’s decision function for clean samples is not\\nsignificantly changed after poisoning, preserving the model’s accuracy. However, the model isolated the target stop\\nsign (grey) to be misclassified as a speed-limit sign. The system can still correctly classify the majority of clean samples,\\nbut outputs wrong predictions for the target stop sign.\\nIn the following sections, we describe the targeted poisoning attacks categorized in Table 2. Notably, such attacks\\nhave been investigated both in the training-from-scratch (TS) and fine-tuning (FT) settings, defined in Sect. 2.1.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='0a0f3b09-72e2-4d19-81be-a4777a2f70b5', embedding=None, metadata={'page_label': '13', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 13\\n3.2.1 Bilevel Poisoning. In Sect. 3.1.2, we reviewed the work in Muñoz-González et al . [124] . In addition to indiscriminate\\npoisoning, the authors also formulated targeted poisoning attacks as:\\nmin\\n𝜹∈Δ𝐿(V,M,𝜽★)+𝐿(V𝑡,M,𝜽★), (3)\\ns.t. 𝜽★∈arg min\\n𝜽L(D∪D𝜹\\n𝑝,M,𝜽). (4)\\nWithin this formulation, the attacker optimizes the perturbation 𝜹on the poisoning samples D𝑝to have a set of target\\n(validation) samples V𝑡misclassified, while preserving the accuracy on the clean (validation) samples in V. It is worth\\nremarking here that the attack is optimized on a set of validation samples, and then evaluated on a separate set of test\\nsamples. The underlying rationale is that the attacker can not typically control the specific realization of the target\\ninstances at test time (e.g., if images are acquired from a camera sensor, the environmental and acquisition conditions\\ncan not be controlled), and the attack is thus expected to generalize correctly to that case.\\nA similar attack was introduced by Koh and Liang [92], to show the equivalence between gradient-based (bilevel)\\npoisoning attacks and influence functions, i.e., functions defined in the area of robust statistics that identify the most\\nrelevant training points influencing specific predictions. Notably, these authors were the first to consider the fine-tuning\\n(FT) scenario in their experiments, training the classification function 𝑓(i.e., an SVM with the RBF kernel) on top of a\\nfeature representation 𝜙extracted from an internal layer of a DNN. Although these two bilevel optimization strategies\\nhave been proven effective, they remain too computationally demanding to be applied to DNNs.\\nJagielski et al . [84] showed how to generalize targeted poisoning attacks to an entire subpopulation in the data\\ndistribution, while reducing the computational cost. To create subpopulations, the attacker selects data samples by\\nmatching their features or clustering them in feature space. The poisoning attack can be performed either by label\\nflipping, or linearizing the influence function to approximate the poisoning gradients, thus reducing the computational\\ncost of the attack. Muñoz-González et al . [124] and Jagielski et al . [84] define a more ambitious goal for the attack\\ncompared to Koh and Liang [92], as their attacks aim to generalize to all samples coming from the target distribution or\\nthe given subpopulation. Specifically, the attack by Koh and Liang [92]is tailored for misleading the model only for\\nsome specific test samples, which means considering the test set Trather than a validation set V𝑡in Eq. (3). However,\\nthe cost of the attack by Muñoz-González et al . [124] is quite high, due to need of solving a bilevel problem, while the\\nattack by Jagielski et al. [84] is faster, but it does not achieve the same success rate on all subpopulations.\\n3.2.2 Feature Collision (Clean-Label). This category of attacks is based on a heuristic strategy named feature collision ,\\nsuited to the so-called fine-tuning (FT) scenario, which avoids the need of solving a complex bilevel problem to optimize\\npoisoning attacks. In particular, PoisonFrog [148] was the first work proposing this idea, which can be formalized as:\\nmin\\n𝜹∈Δ∥𝜙(𝒙+𝜹)−𝜙(𝒛)∥2\\n2. (5)\\nThis attack amounts to creating a poisoning sample 𝒙+𝜹that collides with the target test sample 𝒛∈T in the\\nfeature space, so that the fine-tuned model predicts 𝒛according to the poisoning label associated with 𝒙. To this end,\\nthe adversary leverages the feature extractor 𝜙to minimize the distance of the poisoning sample with the target\\nin the feature space. Moreover, the authors observed that, due to the complexity and nonlinear behavior of 𝜙, even\\npoisoning samples coming from different distributions can be slightly perturbed in the input space to match the feature\\nrepresentation of the target sample 𝒛. To make the poisoning sample look realistic in input space and implement a\\nclean-label attack, the adversarial perturbation 𝜹∈Δis bounded by the attacker in its ℓ𝑝norm [ 148] (e.g.,∥𝜹∥2≤𝜖).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e4f7551a-5bc2-4db3-af29-361b9fb3057b', embedding=None, metadata={'page_label': '14', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14 Cinà, Grosse, et al.\\nSuch box constraint can also be implemented as a soft constraint, as originally done by Shafahi et al . [148] .9Similarly,\\nGuo and Liu [72] adopted feature collision to stage the attack, but they extended the attack’s objective function to\\nfurther increase the poisoning effectiveness. Nevertheless, although this strategy turns out to be effective, it assumes\\nthat the feature extractor is fixed and that it is not updated during the fine-tuning process. Moreover, StringRay [ 158],\\nConvexPolytope [ 212], and BullseyePolytope [ 2] observed that when reducing the attacker’s knowledge the poisoning\\neffectiveness decreases. These works showed that feature collision is not practical if the attacker does not know exactly\\nthe details of the feature extractor, as the embedding of poisoning samples may not be consistent across different\\nfeature extractors. To mitigate these difficulties, ConvexPolytope [ 212] and BullseyePolytope [ 2] optimize the poisoning\\nsamples against ensemble models , constructing a convex polytope around the target samples to enhance the effectiveness\\nof the attack. The underlying idea is that constructing poisoning samples against ensemble models may improve the\\nattack transferability. The authors further optimize the poisoning samples by establishing a strong connection among all\\nthe layers and the embeddings of the poisoning samples, partially overcoming the assumption that the feature extractor\\n𝜙remains fixed.\\nAll these approaches have the property of creating clean-label samples, as first proposed in Shafahi et al . [148] , to\\nstay undetected even when the class labels of training points are validated by humans. This is possible as these attacks\\nare staged against deep models, since for these models, small (adversarial) perturbations of samples in the input space\\ncorrespond to large changes in their feature representations.\\n3.2.3 Bilevel Poisoning (Clean-Label). Although feature collision attacks are effective, they may not result in the\\noptimal accuracy, and they do not minimize the number of poisoned points to change the model’s prediction on a\\nsingle test point. Moreover, they assume that the training process is not significantly changing the feature embedding.\\nIndeed, when the whole model is trained from scratch, these strategies may not work properly as poisoning samples\\ncan be embedded differently. Recent developments, including MetaPoison [ 80] and the work by Geiping et al . [62] ,\\ntackle the targeted poisoning attack in the training-from-scratch (TS) scenario, while ensuring the clean-label property.\\nThese approaches are derived from the bilevel formulation in Eqs. (3)-(4), but they exploit distinct and more scalable\\napproaches to target DNNs, and optimize the attack directly against the test samples Tas done in [ 92]. More concretely,\\nMetaPoison [ 80] uses a meta-learning algorithm, as done by Muñoz-González et al . [124] , to decrease the computational\\ncomplexity of the attack. They further enhance the transferability of their attack by optimizing the poisoning samples\\nagainst an ensemble of neural networks, trained with different hyperparameter configurations and algorithms (e.g.,\\nweight initialization, number of epochs). Geiping et al . [62] craft poisoning samples to maximize the alignment between\\nthe inner loss and the outer loss in Eqs. (3)-(4). The authors observed that matching the gradient direction of malicious\\nexamples is an effective strategy for attacking DNNs trained from scratch, even on large training datasets. Although\\nmodern feature collision or optimized strategies are emerging with notable results for targeted attacks, their performance,\\nespecially in black-box settings, still demands further investigation.\\n3.3 Backdoor (Integrity) Poisoning Attacks\\nBackdoor poisoning attacks aim to cause an integrity violation. In particular, for any test sample containing a specific\\npattern, i.e., the so-called backdoor trigger , they aim to induce a misclassification, without affecting the classification of\\nclean test samples. The backdoor trigger is clearly known only to the attacker, making it challenging for the defender to\\nevaluate whether a given model provided to them has been backdoored during training or not. In Fig. 4c we consider\\n9The original formulation of feature collision in [ 148] adopts theℓ𝑝constraint as soft constraint up-weighted by a Lagrangian penalty term 𝛽, which is\\nbasically equivalent to our hard-constraint formulation for appropriate choices of 𝛽and𝜖.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='74ff1eee-bbac-43de-8745-cfe14ebb17c2', embedding=None, metadata={'page_label': '15', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 15\\nthe case where the attacker provides a backdoored street-sign detector that has good accuracy for classifying street\\nsigns in most circumstances. However, the classifier has successfully learned the backdoor data distribution, and will\\noutput speed-limit predictions for any stop-sign containing the backdoor trigger. In the following sections, we describe\\nbackdoor attacks following the categorization given in Table 2. Notably, such attacks have been initially staged in the\\nmodel-training (MT) setting, assuming that the user outsources the training process to an untrusted third-party service,\\nbut they have been then extended also to the training-from-scratch (TS) and fine-tuning (FT) scenarios.\\n3.3.1 Trigger Poisoning. Earlier work in backdoor attacks considered three main families of backdoor triggers, i.e.,\\npatch ,functional , and semantical triggers, as discussed below.\\nPatch. The first threat vector of attack for backdoor poisoning has been investigated in BadNets [ 70]. The authors\\nconsidered the case where the user outsources the training process of a DNN to a third-party service, which maliciously\\nalters the training dataset to implant a backdoor in the model. To this end, the attacker picks a random subset of\\nthe training data, blends the backdoor trigger into them, and changes their corresponding labels according to an\\nattacker-chosen class. A similar idea has been investigated further in LatentBackdoor [ 196] and TrojanNN [ 110], where\\nthe backdoor trigger is designed to maximize the response of selected internal neurons, thus reducing the training\\ndata needed to plant the trigger. Additionally, LatentBackdoor [ 196] designed the trigger to survive even if the last\\nlayers are fine-tuned with novel clean data, while TrojanNN [ 110] does not need access to the training data as a\\nreverse-engineering procedure is applied to create a surrogate dataset. All these attacks assume that the trigger is always\\nplaced in the same position, limiting their application against specific defense strategies [ 5,28,155]. To overcome this\\nissue, BaN [ 143] introduced different backdoor attacks where the trigger can be attached in various locations of the\\ninput image. The underlying idea was to force the model to learn the backdoor trigger and make it location invariant.\\nFunctional. The patch strategy is based on the idea that poisoning samples repeatedly present a fixed pattern as a\\ntrigger, which may however be detected upon human validation of training samples (in the TS and FT scenarios, at least).\\nIn contrast, a functional trigger represents a stealthier strategy as the corresponding trigger perturbation is slightly\\nspaced throughout the image or changes according to the input. Some works assume to slightly perturb the entire image\\nso that those small variations are not detectable by humans, but evident enough to mislead the model. In WaNET [ 129]\\nwarping functions are used to generate invisible backdoor triggers (see Fig. 2). Moreover, the authors enforced the model\\nto distinguish the backdoor warping functions among other pristine ones. In Li et al . [99] steganography algorithms are\\nused to hide the trigger into the training data. Specifically, the attacker replaces the least significant bits to contain\\nthe binary string representing the trigger. In DFST [ 36] style transfer generative models are exploited to generate and\\nblend the trigger. However, the aforementioned poisoning approaches assume that the attacker can change the labeling\\nprocess and that no human inspection is done on the training data. This assumption is then relaxed by Barni et al . [8]\\nand Liu et al . [111] , where clean-label backdoor poisoning attacks are considered; in particular, Liu et al . [111] used\\nnatural reflection effects as trigger to backdoor the system, while Barni et al . [8] used an invisible sinusoidal signal\\nas backdoor trigger (see Fig. 2). More practical scenarios, where the attacker is assumed to have limited knowledge,\\nhave been investigated by Chen et al . [33] and Zhong et al . [211] . In these two works, the authors used the idea of\\nblending fixed patterns to backdoor the model. In the former approach, Chen et al . [33] assume that the attacker blends\\nimage patterns into the training data and tunes the blend ratio to create almost invisible triggers, while impacting the\\nbackdoor’s effectiveness. In the latter, Zhong et al . [211] assume that an invisible grid pattern is generated to increase\\nthe pixel’s intensity, and its effectiveness is tested in the TS and FT settings.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='831fc0db-b87a-4104-9a5e-80b21095dc33', embedding=None, metadata={'page_label': '16', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16 Cinà, Grosse, et al.\\nSemantical. The semantical strategy incorporates the idea that backdoor triggers should be feasible and stealthy. For\\nexample, Sarkar et al . [145] used facial expressions or image filters (e.g., old-age, smile) as backdoor triggers against\\nreal-world facial recognition systems. At training time, the backdoor trigger is injected into the training data to cause\\nthe model to associate a smile filter with the authorization of a user. At test time, the attacker can use the same filter to\\nmislead classification. Similarly, Chen et al . [33] and Wenger et al . [179] tried to poison face-recognition systems by\\nblending physically-implementable objects (e.g., sunglasses, earrings) as triggers.\\n3.3.2 Bilevel Poisoning. Trigger-based strategies assume that the attacker uses a predefined perturbation to mount\\nthe attack. However, an alternative strategy for the attacker is to learn the trigger/perturbation itself to enhance the\\nbackdoor effectiveness. To this end, even backdoor poisoning can be formalized as a bilevel optimization problem:\\nmin\\n𝜹∈Δ𝐿(V,M,𝜽★)+𝐿(V𝒕\\n𝑡,M,𝜽★), (6)\\ns.t. 𝜽★∈arg min\\n𝜽L(D∪D𝜹\\n𝑝,M,𝜽). (7)\\nHere, the attacker optimizes the training perturbation 𝜹for poisoning samples in D𝑝to mislead the model’s prediction\\nfor validation samples V𝑡containing the backdoor trigger 𝒕. In contrast to indiscriminate and targeted attacks (in\\nSect. 3.1 and Sect. 3.2), the attacker injects the backdoor trigger in the validation samples 𝒕to cause misclassifications.\\nAdditionally, as for targeted poisoning, the error on Vis minimized to preserve the system’s functionality.\\nOne way to address this bilevel formulation is to craft optimal poisoning samples using generative models [ 48,\\n101,128], as also done in [ 194] for indiscriminate poisoning. Nguyen and Tran [128] trained the generative model\\nwith a loss that enforces the diversity andnoninterchangeable of the trigger, while LIRA [ 48]’s generator is trained\\nto enforce effectiveness and invisibility of the triggers. Conversely, Li et al . [101] used a generative neural network\\nsteganography technique to embed a backdoor string into poisoning samples. Another way is to perturb training\\nsamples with adversarial noise, as done by Li et al . [99] and Zhong et al . [211] . More concretely, in the former approach,\\nthe trigger maximizes the response of specific internal neurons, and a regularization term is introduced in the objective\\nfunction to make the backdoor trigger invisible. In the latter work, the attacker looks for the minimum universal\\nperturbation that pushes any input towards the decision boundary of a target class. The attacker can use this invisible\\nperturbation trigger on any image, inducing the model to misclassify the target class.\\n3.3.3 Feature Collision (Clean-Label). The backdoor trigger visibility influences the stealthiness of the attack. A\\nbackdoor trigger that is too obvious can be easily spotted when the dataset is inspected [ 142]. However, Hidden\\nTrigger [ 142] introduced the idea of using the feature collision strategy, seen in Sect. 3.2.2 and formulated in Eq. (5),\\nto hide the trigger into natural target samples. Specifically, the attacker first injects a random patch trigger into the\\ntraining set, and then each poisoning sample is masked via feature collision . The resulting poisoning images are visually\\nindistinguishable from the target, and have a consistent label (i.e., they are clean-label), while the test samples with the\\npatch trigger will collide with the poisoning samples in feature space, ensuring that the attack works as expected.\\nAlthough the work in [ 142] implements an effective and stealthy clean-label attack, it is applicable only in the feature\\nextractor𝜙is not updated. Such a limitation is mitigated by Turner et al . [169] who exploit a surrogate latent space,\\nrather than𝜙, to interpolate the backdoor samples, hiding the training-time trigger. Moreover, the attacker can tune the\\ntrigger visibility at test time to enhance the attack’s effectiveness.\\n3.3.4 Bilevel Poisoning (Clean-Label). Inspired by recent success of the gradient-alignment technique in [ 62] for\\ntargeted poisoning, Souri et al . [156] exploited the same bilevel-descending strategy to stage clean-label backdoor', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='caf1caf1-63d8-49ac-a61e-bbcea1dc0e02', embedding=None, metadata={'page_label': '17', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 17\\npoisoning attacks in the training-from-scratch scenario. Similarly to Saha et al . [142] the training and the test data\\nperturbations are different, enhancing the stealthiness of the attack and making it stronger against existing defenses.\\n3.4 Current Limitations\\nAlthough data poisoning has been widely studied in recent years, we argue here that two main challenges are still\\nhindering a thorough development of poisoning attacks.\\n3.4.1 Unrealistic Threat Models. The first challenge we formulate here questions some of the threat models considered\\nin previous work. The reason is that such threat models are not well representative of what may happen in many\\nreal-world scenarios for the attackers. They are valuable because they allow system designers to test the system’s\\nrobustness under worst-case scenarios, but their practicability and effectiveness against realistic production systems\\nare unknown. To give an accurate estimate of how poisoning attacks are effective against ML production systems, we\\nshould consider assumptions that are less favorable to the attacker. For example, Fowl et al . [55] and Feng et al . [53]\\nassume that the attacker controls almost the entire training dataset to effectively mount an indiscriminate poisoning\\nattack against DNNs. While this may happen in certain hypothesized situations, it is also not quite surprising that a\\npoisoning attack works if the attacker controls a large fraction of the training set. We believe that poisoning attacks\\nthat assume that only a small fraction of the training points can be controlled by the attacker are more realistic and,\\ntherefore, viable against real production systems. We refer the reader to a similar discussion in the context of federated\\nlearning poisoning in [150].\\nAnother limitation of threat models considered for poisoning attacks is that, in some cases, exact knowledge of\\nthetestsamples is implicitly assumed. For example, [ 148] and [ 62] optimize a targeted poisoning attack to induce\\nmisclassification of few specific testsamples. In particular, the attack is both optimized and tested using the same test\\nsamples, differently from work which optimizes the poisoning samples using validation data, and then tests the attack\\nimpact on a separate test set [16, 124]. This evaluation setting clearly enables the attack to reach higher success rates,\\nbut at the same time, there is no guarantee that the attack will generalize even to minor variations of the considered\\ntest samples, questioning its applicability outside of settings in which the attacker has exact knowledge of the test\\ninputs. For instance, the attack may not work as expected in physical domains, where images are acquired by a camera\\nunder varying illumination and environmental conditions. In such cases, it is indeed clear that the attacker can not\\nknow beforehand the specific realization of the test sample, as they do not control the acquisition conditions. On a\\nsimilar note, only a few studies on backdoor poisoning have considered real-world scenarios where external factors\\n(such as lighting, camera orientation, etc.) can alter the trigger. Indeed, as done in [ 148] and [ 62], most papers consider\\ndigital applications where the implanted trigger is nearly unaltered.\\nIn conclusion, although some recent works seem to have improved the effectiveness of poisoning attacks, their\\nassumptions are often not representative of the actual production system or the attacker’s settings, limiting their\\napplicability only in the proposed context.\\n3.4.2 Computational Complexity of Poisoning Attacks. The second challenge we discuss here is related to the solution\\nof the bilevel programming problem used to optimize poisoning attacks. The problem, as analyzed by Muñoz-González\\net al. [124] , is that solving the bilevel formulation with a gradient-based approach requires computing and inverting the\\nHessian matrix associated to the equilibrium conditions of the inner learning problem, which scales cubically in time\\nand quadratically in space with respect to the number of model’s parameters. Even if one may exploit rank-one updates\\nto the Hessian matrix, and Hessian-vector products coupled with conjugate descent to speed up the computation of\\nrequired gradients, the approach remains too computationally demanding to attack modern deep models, where the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='640eb198-2e1e-4ff1-ad56-76af28c09375', embedding=None, metadata={'page_label': '18', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18 Cinà, Grosse, et al.\\nnumber of parameters is on the order of millions. Nevertheless, it is also true that that solving the bilevel problem is\\nexpected to improve the effectiveness of the attack and its stealthiness against defenses. For example, the bilevel strategy\\napproach is the only one at the state of the art which allows mounting an effective attack in the training-from-scratch\\n(TS) setting. Other heuristic approaches, e.g., feature collision , have been shown to be totally ineffective if the feature\\nextractor𝜙is updated during training [ 62]. For backdoor poisoning, the recent developments in the literature show\\nthat bilevel-inspired attacks are more effective and can better counter existing defenses [ 48,128,156]. Thus tackling the\\ncomplexity of the bilevel poisoning problem remains a relevant open challenge to ensure a fairer and scalable evaluation\\nof modern deep models against such attacks.\\n3.5 Transferability of Poisoning Attacks\\nTransferability is a characteristic of attacks to be effective even against classifiers the attacker does not have full\\nknowledge about. The term transferability was first investigated for adversarial examples in [ 66,131,132]. In case of\\nlimited knowledge (i.e., black-box attacks), the attacker can use surrogate learners or training data to craft the attack\\nand transfer it to mislead the unknown target model. Nevertheless, the first to introduce the idea of surrogates for\\ndata poisoning attacks were Nelson et al . [126] and Biggio et al . [16] . The authors claimed that if the attacker does\\nnot have exact knowledge about the training data, they could sample a surrogate dataset from the same distribution\\nand transfer the attack to the target learner. In subsequent work, Muñoz-González et al . [124] and Demontis et al . [45]\\nanalyzed the transferability of poisoning attacks using also surrogate learners, showing that matching the complexity\\nof the surrogate and the target model enhances the attack effectiveness. Transferability has also been investigated when\\nconsidering surrogate objective functions. More concretely, optimizing attacks against a smoother objective function\\nmay find effective, or even better, local optima than the ones of the target function [ 45,92,116,131]. For example,\\noptimizing a non-differentiable loss can be harder, thus using a smoothed version may turn out to be more effective [ 92].\\nMore recently, Suciu et al . [158] showed that the attacker can leverage transferability even when the attacker has\\nlimited knowledge about the feature representation, at the cost of reducing the attack effectiveness. However, Zhu et al .\\n[212] and Aghakhani et al . [2] independently hypothesize that the stability of feature collision attacks is compromised\\nwhen the feature representation in the representation space is changed. To mitigate this problem, they craft poisoning\\nsamples to attack an ensemble of models, encouraging their transferability against multiple networks.\\n3.6 Unifying Framework\\nAlthough the three poisoning attacks are detailed in Sects. 3.1-3.3 aim to cause different violations, they can be described\\nby the following generalized bilevel programming problem:\\nmax\\n𝜹∈Δ𝛼𝐿(V,M,𝜽★)−𝛽𝐿(V𝒕\\n𝑡,M,𝜽★), (8)\\ns.t. 𝜽★∈arg min\\n𝜽L(D∪D𝜹\\n𝑝,M,𝜽), (9)\\nThe optimization program in Eqs. (8)-(9)aims to accomplish the attacker’s goal, considering their capacity of tampering\\nwith the training set and knowledge of the victim model, by optimizing the perturbation 𝜹used to poison the training\\nsamples inD𝑝. Additionally, as in Eqs. (1)-(7), the poisoning noise 𝜹belongs to Δwhich encompass possible domain\\nconstraints or feature constraints to improve stealthiness of the attack (e.g., invisibility of the trigger). The test\\ndata perturbation 𝒕is absent (i.e., 𝒕=0), for indiscriminate and target poisoning. For backdoor poisoning, 𝒕is pre-\\ndefined/optimized by the attacker before training, unlike from adversarial examples [ 14,66] where the perturbation 𝒕is', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4dd04f49-988e-4cfd-adeb-0d5ab3cc2c6e', embedding=None, metadata={'page_label': '19', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 19\\noptimized at test time. The coefficients 𝛼and𝛽are calibrated according to the attacker’s desired violation. We can set:\\n(i)𝛼=1(−1)and𝛽=0for error-generic (specific) indiscriminate poisoning; (ii) 𝛼=−1and𝛽=−1(1)for error-specific\\n(generic) targeted poisoning; (iii) 𝛼=−1and𝛽=−1(1)for error-specific (generic) backdoor poisoning.\\nIn conclusion, although backdoor, indiscriminate and targeted attacks are designed to cause distinct security violations,\\nthey can be formulated under a unique bilevel optimization program. Therefore, as we will explore in Sec. 7, solutions\\nfor optimizing bilevel optimization programs fast can pave the way towards developing novel effective and stealthy\\npoisoning attacks capable of mitigating the scalability limit of current strategies.\\n4 DEFENSES\\nMany defenses have been proposed to mitigate poisoning attacks. In this section, we discuss each of the six defense\\nclasses identified in Sect. 2.3. For each group, we review the related learning and defense settings, and the various\\napproaches suggested by prior works. Some defenses can be assigned to several groups. In these cases, we assigned a\\ndefense to the most suitable group in terms of writing flow. A compact summary of all defenses is given in Table 3. We\\nfurther match attack strategies and defenses at training and test time in Table 4. Having reviewed all defense groups,\\nwe conclude the section by discussing current defense evaluation issues, outlining three main open challenges.\\n4.1 Training Data Sanitization\\nThese defenses aim to identify and remove poisoning samples before training , to alleviate the effect of the attack. The\\nunderlying rationale is that, to be effective, poisoning samples have to be different from the rest of the training points.\\nOtherwise, they would have no impact at all on the training process. Accordingly, poisoning samples typically exhibit\\nan outlying behavior with respect to the training data distribution, which enables their detection. The defenses that\\nfall into this category require access to the training data D′, and in a few cases also access to clean validation data\\nV, i.e., to an untainted dataset that can be used to facilitate detection of outlying poisoning samples in the training\\nset. No capabilities are required to alter the learning algorithm Wor to train the model parameters 𝜽. Theoretically,\\nthese defenses can be applied in all learning settings. We can however not exclude the possibility in the model-training\\nsetting that the attacker tampers with the data provided, which is beyond the defender’s control. We first discuss\\ndefenses against indiscriminate poisoning. Paudice et al . [133] target label-flip attacks by using label propagation.\\nAs Steinhardt et al . [157] show, the difference between poisons and benign data allows to use outlier detection as a\\ndefense. Detection can also be eased by taking into account both features and labels, using clustering techniques for\\nindiscriminate [ 96,162] and backdoor/targeted attacks [ 149]. Backdoor and targeted poisoning attacks can also be\\ndetected using outlier detection, where the outlier is determined in the networks’ latent features on the potentially\\ntampered data [ 75,135,168]. An orthogonal line of work, by Xiang et al . [183 ,186], reconstructs the backdoor trigger\\nand removes samples containing it. As shown in Table 4, training data sanitization has been applied against various\\nattack strategies. Attack strategies that have not been mitigated yet are only indiscriminate clean-label bilevel attacks,\\nsemantical trigger backdoors and bilevel backdoors.\\n4.2 Robust Training\\nAnother possibility to mitigate poisoning attacks is during training . The underlying idea is to design a training algorithm\\nthat limits the influence of malicious samples and thereby alleviates the influence of the poisoning attack. Intuitively, as\\nreported in Table 3, all of these defenses require access to the training data D′and none to clean validation data V.\\nNonetheless, they require altering the learning algorithm Wand access to the model’s parameters 𝜽. Hence, robust', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='671c0114-ece7-4ca7-9979-900133be19b0', embedding=None, metadata={'page_label': '20', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20 Cinà, Grosse, et al.\\nTable 3. Overview of defenses in the area of classification. When several approaches are named, we order them according to publication\\nyear and alphabetical order of the authors. For each paper we report the defender’s knowledge and capability required, consisting\\nin access to the training data D′, clean validation data Vand access to the training procedure Wand the model’s parameters\\n𝜃.#indicates that the corresponding knowledge or capability is present,  that it is not. In 𝜃,G #refers to the ability to fine-tune\\nthe model. We further denote whether the approach provides a certification (Cert.), or it is suited to deep neural networks (DNN).\\n∗intended as forensic tool to determine which points were poisoned in retrospect, not as a defense.\\nDefense strategy Defense D′V W 𝜽Cert. DNNIndiscriminate4.1 Training Data Sanitization\\uf8f1\\uf8f4\\uf8f4 \\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3Taheri et al. [162] # #   \\nCurie [96], Paudice et al. [133], Frederickson et al. [58] #    \\nSphere / Slab Defense [157] #    ✓\\n4.2 Robust Training\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4 \\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3RONI [125], Biggio et al. [15], Demontis et al. [44] #  # #\\nSever [46], Jia et al. [86], Rosenfeld et al. [140] #  # # ✓\\nHong et al. [77] #  # # ✓\\n(SS-)DPA [97] #   # ✓ ✓\\nWeighted Bagging [13] #   #\\nDiff. Private Learners [116], Chen et al. [32] Wang et al. [175] #   # ✓Backdoor / Targeted4.1 Training Data Sanitization\\uf8f1\\uf8f4\\uf8f4 \\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3Shan et al. [149]∗#  # # ✓\\nSpectral Signatures [168], CI [183], Peri et al. [135]#    ✓RE [186], SPECTRE [75]\\n4.2 Robust Training\\uf8f1\\uf8f4\\uf8f4 \\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3Du et al. [51], Hong et al. [77], Borgnia et al. [21], Geiping et al. [61]#  # # ✓ABL [102], Huang et al. [79], Sun et al. [160], Yang et al. [195]\\nDP-InstaHide [22], RAB [176] #  # # ✓ ✓\\n4.3 Model Inspection\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4 \\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3AEGIS [155]  # # # ✓\\nNeuroninspect [81], Bajcsy and Majurski [5], L-RED [185]  #  # ✓\\nActivation Clustering [28], Tang et al. [163] #    ✓\\nMNTD [193], Litmus patterns [94], AEVA [71]  #   ✓\\nDeepInspect [30]     ✓\\n4.4 Model Sanitization\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4 \\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3I-BAU [199]  # # # ✓\\nYoshida et al. [197] #  # G # ✓\\nCheng et al. [35], Zhao et al. [208], ANP [180], CLEAR [214]  #  # ✓\\nRe-training [112] #   # ✓\\nLiu et al. [107], Neural Attention Distillation [100]  #  G # ✓\\nDeepSweep [200] #   G # ✓Backdoor4.5 Trigger Reconstruction\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4 \\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3ABS [109], Neural Cleanse [173], Shen et al. [151] #  #✓\\nNEO [170], Hu et al. [78]\\nMESA [138], Gangsweep [213], Xiang et al. [184] #   ✓TAD [204], AEVA [71], Xiang et al. [182]\\nTabor [73], B3D-SS [50]     ✓\\n4.6 Test Data Sanitization\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4 \\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3NNoculation [171]  # # G # ✓\\nAnomaly Detection [112] #   # ✓\\nInput Preprocessing [112], SentiNet [37], ConFoc [172], #  # ✓CleaNN [85], Februus [47]\\nSTRIP [60]  #   ✓\\nLi et al. [104], Sarkar et al. [144]     ✓\\ntraining can only be implemented when the defender trains the model, e.g., in the training-from-scratch orfine-tuning\\nsetting. To alleviate the effect of indiscriminate poisoning attacks, the training data can be split into small subsets.\\nThe high-level idea is that a larger number of poisoning samples is needed to alter all small classifiers. The defender\\ncan build such ensembles using bagging [ 13,97,175] or voting mechanisms [ 86] or a combination thereof [ 32,97]. An\\nalternative approach by Nelson et al . [125] is to exclude a sample from training if it leads to a significant decrease in\\naccuracy when used in training. In addition, Diakonikolas et al . [46] apply techniques from robust optimization and\\nrobust statistics, thereby limiting the impact of individual, poisonous points. Alternatively, the influence of poisons can\\nbe limited by increasing the level of regularization [ 15,44]. The alleviating effect of regularization against backdoors\\nhas been described by Carnerero-Cano et al . [25] , with a more detailed analysis by Cinà et al . [40] . The latter work', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c725c7c1-2da7-4c58-a56b-3e79f7e92c88', embedding=None, metadata={'page_label': '21', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 21\\nTable 4. Matching poisoning attack strategies and defenses. For each defense, we depict on which attack strategy (as defined in\\nSection 3) the defense was evaluated. We mark cells with\\n❙if the corresponding defense category have not been investigated so far\\nfor the corresponding attack. Conversely, we mark cells with ✗if corresponding defense has no sense and cannot be applied.\\nAttackDefenses\\nTraining Time Test Time\\n𝜹 𝒕Clean\\nLabelTraining Data\\nSanitizationRobust\\nTrainingModel\\nInspectionModel\\nSanitizationTrigger\\nReconstructionTest Data\\nSanitizationIndiscr.LF- [96, 133, 162][15, 32, 44, 77, 97,\\n140, 175]\\n❙\\n❙ ✗ ✗\\nBL- [58, 157] [13, 86, 116, 125]\\n❙\\n❙ ✗ ✗\\nBL-✓\\n❙\\n❙\\n❙\\n❙ ✗ ✗TargetedBL- [58]\\n❙\\n❙\\n❙ ✗\\n❙\\nFC-✓ [135,149,195][22, 61, 77, 102] [163, 214] [214] ✗\\n❙\\nBL-✓ [149, 195] [21, 22, 61]\\n❙\\n❙ ✗\\n❙BackdoorT𝑃T𝑃 [75, 149, 168,\\n172, 183, 186][21, 51, 61, 79, 86,\\n102, 160, 176, 197][5, 28, 30, 71,\\n78, 94, 151,\\n155, 163, 182,\\n185, 193, 204][30, 100, 107,\\n138, 180, 197,\\n199, 200, 208,\\n213][35, 50, 71, 73,\\n78, 109, 138,\\n170, 173, 182,\\n186][37, 47, 60,\\n104, 137, 144,\\n171–\\n173, 200]\\nT𝑆T𝑆\\n❙ [79, 149] [71, 151][107, 112, 199,\\n200][71] [112, 171]\\nT𝐹T𝐹[75, 186] [79, 102, 176][78, 81, 151,\\n155, 163, 185,\\n193][100, 180, 199,\\n200, 213][78, 109, 184–\\n186, 213][60, 200]\\nFC T𝑃✓ [75][22, 61, 79, 102,\\n195][71, 214][100, 180, 213,\\n214][71] [104]\\nBL T𝐹\\n❙\\n❙ [71, 151] [180] [213] [200]\\nBL T𝑃✓\\n❙ [195]\\n❙\\n❙\\n❙\\n❙\\nshows that hyperparameters related to regularization affect backdoor performance. Backdoor and targeted poisoning\\nattacks can also be mitigated using data augmentations like mix-up [ 21,22], or based on the model’s gradients wrt. the\\ninput [ 61]. Analogously, the data can be augmented using noise to mitigate indiscriminate [ 140] and backdoor [ 176]\\nattacks. Furthermore, differences in the loss between backdoored/targeted and clean data allows to unlearn [ 102] or\\nidentify [ 195] poisons later in training. Alternatively, a trained preprocessor can alleviate the threat of backdoors [ 160].\\nFurthermore, Huang et al . [79] show that pre-training the network unsupervisedly (e.g., without wrong labels) can\\nalleviate backdoors. Finally, in both indiscriminate [ 77,116] and backdoor/targeted [ 22,51,77] attacks, the framework of\\ndifferential privacy can be used to alleviate the effect of poisoning. The intuition behind this approach is that differential\\nprivacy limits the impact individual data points have, thereby limiting the overall impact of outlying poisoning samples\\ntoo [ 77]. However, further investigation is still required to defend against some bilevel strategies, as visible in Table 4.\\n4.3 Model Inspection\\nStarting with model inspection, we discuss groups of defenses operating before the model is deployed. The approaches\\nin these groups mitigate only backdoor and targeted attacks. In model inspection, we determine for a given model\\nwhether a backdoor is implanted or not. The defense settings in this group are diverse, and encompass all combinations.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9586c333-5cbc-4bf8-93ed-365c0a0be7e8', embedding=None, metadata={'page_label': '22', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='22 Cinà, Grosse, et al.\\nIn principle, model inspection can be used in all learning settings, where exceptions for specific defenses might apply.\\nTo inspect a model can be formulated as classifications tasks. For example, Kolouri et al . [94] and Xu et al . [193] show\\nthat crafting specific input patterns and training a meta-classifier on the outputs of a given model computed on such\\ninputs can reveal whether the model is backdoored. Bajcsy and Majurski [5]follow a similar approach, using clean data\\nand a pruned model. A different observation is that when relying on the backdoor trigger to output a class, the network\\nbehaves somehow unusual : it will rely on normally irrelevant features. Thus, outlier detection can be used. For example,\\nZhu et al . [214] alternatively search for a set of points that are reliably misclassified to detect feature-collision attacks.\\nTo detect backdoors and backdoored models, outlier detection can be used on top of interpretability techniques [ 81],\\nor latent representations [ 28,155,163]. Alternatively, Xiang et al . [185] show that finding a trigger that is reliably\\nmisclassified indicates the model is backdoored. As reported in Table 4, model inspection has primarily been evaluated\\non backdoor attacks with a predefined trigger strategy.\\n4.4 Model Sanitization\\nOnce a backdoored model is detected, the question becomes how to sanitize it. Sanitization requires diverse defense\\nsettings encompassing all possibilities. Model sanitization often involves (re-)training or fine-tuning. Depending on\\nthe exact model-training setting, sanitizing the model might be impossible (e.g., if the model is provided as a service\\naccessible only via queries). To sanitize a model, pruning [ 35,180], retraining [ 199], or fine-tuning [ 107,112] can be\\nused. Given knowledge of the trigger, Zhu et al . [214] propose to relabel the identified poisoned samples after the trigger\\nis removed. Alternatively, approaches such as data augmentation [ 200] or distillation [ 100,197] can augment small,\\nclean datasets. Finally, Zhao et al . [208] show that path connection between two backdoored models, using a small\\namount of clean data, also reduces the success of the attack. As shown in Table 4, model sanitization has been evaluated\\nmainly against backdoor attacks. Extensions to other kinds of triggers and targeted attacks might however be possible.\\n4.5 Trigger Reconstruction\\nAs an alternative to model sanitization, this category of defenses aim to reconstruct the implanted trigger. The\\nassumptions on the defender’s knowledge and capabilities are diverse, and encompass many possibilities, although the\\nlearning algorithm Wis never altered. As for model inspection, trigger reconstruction can in theory be used in all\\nlearning settings, where exceptions for specific defenses might apply. While a trigger can be randomly generated [ 170,\\n204], the question remains on how to verify that the reconstructed pattern is a trigger. Many techniques leverage\\nthe fact that a trigger changes the classifier’s output reliably. This finding has been in detail investigated by Grosse\\net al. [69] , who show that backdoor patterns lead to a very stable or smooth output of the target class. In other words,\\nthe classifier ignores other features and only relies on the backdoor trigger. Such a stable output also enables to\\nreformulate trigger reconstruction as an optimization problem [ 173]. In the first approach of its kind, Wang et al.’s\\nNeural Cleanse [ 173] optimizes a pattern that leads to reliable misclassification of a batch of input points. The idea\\nis that if there is such a pattern, and it is small, it must be similar to the backdoor trigger. Wang et al.’s approach has\\nbeen improved in terms of how to determine whether a pattern is indeed a trigger [ 73,184], how to decrease runtime\\nfor many classes [ 78,151,182], how many triggers can be recovered at once [ 78], or how to reverse-engineer without\\ncomputing gradients [ 50,71]. Zhu et al . [213] establish that not an optimization, but also a GAN can be used to generate\\ntriggers. In general, a reconstruction can be based on the intuition that triggers themselves form distributions that can\\nbe learned [ 138,213]. Finally, Liu et al . [109] successfully use stimulation analysis of individual neurons to retrieve', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4c06b463-7194-4c82-bf4d-5005739c7171', embedding=None, metadata={'page_label': '23', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 23\\nimplanted trigger patterns. Trigger reconstruction has been evaluated on almost all trigger-based backdoor attacks (see\\nTable 4), as their applicability is naturally limited to the existence of a trigger.\\n4.6 Test Data Sanitization\\nAs the name suggests, this is the only group of defenses operating during test time , where the defender attempts to\\nsanitize malicious test inputs. The assumptions on the defender’s knowledge and capabilities, as in other cases, are\\ndiverse and encompass all possible settings. Test data sanitization can be used in all learning settings, where exceptions\\nfor specific cases might apply. This group can, in principle, be applied in all learning scenarios, but is the only sanitization\\napplicable if the model is only available as an online service, and accessible via queries. There are three strategies\\noverall when sanitizing test data. The first one boils down to removing the trigger [ 37,47,104]. For example Chou et al .\\n[37] use interpretability techniques to identify crucial parts of the input and then mask these to identify whether they\\nare adversarial or not. A second group is build on the agreement of ensembles on input [ 144,171,172]. In Sarkar et al .\\n[144] , this ensemble results indirectly from noising the input, but can also be build with a second, retrained version\\nof the original model on different styles [ 172] or augmentations [ 171]. Finally, and as used for trigger generation, the\\nconsistency of a classifier’s output can also help to detect an attack [ 60,85]. While Gao et al . [60] superimpose images\\nto check the consistency, Javaheripi et al . [85] instead consider the consistency of noised images in the inner layers. As\\nshown in Table 4, test data sanitization has been tested only on trigger-based backdoor attacks. However, the latter\\ntwo strategies mentioned above do have the potential to also detect targeted poisoning attacks, as these lead to locally\\nimplausible behavior. A detection of indiscriminate attacks at test time is however not possible.\\n4.7 Current Limitations\\nAlthough there is a large body of work on defenses, there are still unresolved challenges, as detailed in the following.\\n4.7.1 Inconsistent Defense Settings. The assumptions on the defender’s knowledge and capabilities reflect what is\\nrequired to deploy a defense. In indiscriminate defenses, or robust training and training data sanitization in general,\\nthese are very homogeneous. When it comes to model inspection, trigger reconstruction, model sanitization, and\\ntest data sanitization, there is a larger variation in both the defender’s knowledge and capabilities. In particular, we\\nlack understanding on the effect of individual capabilities or knowledge, for example not having direct access to the\\nmodel when provided as a service and interacting via queries. More work is required that enables comparison across\\napproaches here, and that sheds light on the individual components of the defense setting.\\n4.7.2 Insufficient Defense Evaluations. In Table 4, we match poisoning attack strategies and defenses by reporting in\\neach cell the defense papers that evaluate against the corresponding attack strategy. In some cases, indicated with a\\ncross (✗), a defense of this strategy is not possible as there is no trigger to reconstruct (indiscriminate or targeted) or the\\ntest data is not altered by the attacker and can thus not be sanitized (indiscriminate attacks). Furthermore, Table 4 shows\\nthat the amount of defenses per attack strategies varies greatly. Whereas for backdoor attacks using patch triggers\\nthere are around fifty defenses, only eleven defenses have been considered against semantic triggers, one against\\nbilevel targeted attacks [ 58], one against bilevel patch backdoor attacks [ 195], and none against indiscriminate clean\\nlabel bilevel attacks. With only a few defenses [ 163,214], there is also a shortage of model inspection and sanitization\\ndefenses when no trigger manifests in the model.\\nBeyond this shortage, there is a need to thoroughly test existing defenses using adaptive attacks, which are depicted in\\nTable 5. Adaptive attacks are tailor to circumvent one or several defenses. In other words, the attack identifies essential', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a8db028d-e3f6-486b-b392-b5150811852d', embedding=None, metadata={'page_label': '24', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='24 Cinà, Grosse, et al.\\nTable 5. Attacks breaking defenses in the areas of indiscriminate, targeted, and backdoor attacks. We provide the reference for the\\nadaptive attack, which defenses are broken, and a high-level description of the strategy of the adaptive attack.\\nBroken Defenses\\nAttack Indiscriminate Targeted Backdoor Strategy\\nKoh et al. [93] [141, 157] constrain poison point’s features\\nShokri et al. [152] [28, 168, 173] regularize trigger pattern\\nTang et al. [163] [28, 37, 60, 173] add trigger images with correct label\\nLin et al. [105] [109, 173] add trigger images mixed from source and target\\ncomponents like for example a threshold within a defense and adapts the poisoning points to be below this threshold.\\nFor example, Koh et al . [93] constrain the indiscriminate poisons features so that several points are in close vicinity to\\navoid outlier detection. In the case of backdoors, Shokri et al . [152] regularize the trigger to be less detectable within\\nthe network. Tang et al . [163] and Lin et al . [105] employ different strategies to make training data with trigger more\\nsimilar to benign data. Yet, as visible in Table 5, current adaptive backdoor attacks tend to break the same defenses.\\nMore work is thus needed to understand all defenses’ limitations through adaptive attacks, even though systematizing\\nthe design of such attacks and automating the corresponding evaluations is not trivial. To this end, it may be interesting\\nto design indicators of failure that automate the identification of faulty, non-adaptive evaluations for poisoning attacks,\\nas recently shown in [136] for adversarial examples.\\n4.7.3 Overly-specialized Defenses. Furthermore, few defenses (only roughly one sixth) have been evaluated against\\ndifferent kinds of triggers. Only one defense in test data sanitization [ 200] and two defenses in trigger reconstruc-\\ntion [ 78,109] have been evaluated against more than one trigger type. There are three defenses for each training data\\nsanitization [ 75,149,186], model sanitization [ 100,199,200] and robust training [ 22,61,102]. In model inspection,\\nfive [ 71,151,155,163,193] defenses tests on more than one attack type. There are even more general defenses that\\nare able to handle multiple poisoning attacks, such as indiscriminate, targeted, and backdoor attacks, as for example\\nGeiping et al. [61] and Hong et al. [77] show.\\n5 POISONING ATTACKS AND DEFENSES IN OTHER DOMAINS\\nWhile in this survey we focus on poisoning ML in the context of supervised learning tasks, and mostly in computer-vision\\napplications, it is also worth remarking that several poisoning attacks and defense mechanisms have been developed also\\nin the area of federated learning [ 4,12,23,76,161,165,191,201–203,210], regression learning [ 46,54,83,106,123,177],\\nreinforcement learning [ 3,6,10,52,74,82,89,115,139,174,192,205], and unsupervised clustering [ 17,18,41,90,141]\\nor anomaly detection [ 43,141] algorithms. Furthermore, notable examples of poisoning attacks and defenses have also\\nbeen shown in computer-security applications dealing with ML, including spam filtering [ 13,46,58,126,133], network\\ntraffic analysis [ 141], and malware detection [ 134,147,162], audio [ 1,91,107,110,193] and video analysis [ 168,209],\\nnatural language processing [ 29,34,110,137,206], and even in graph-based ML applications [ 20,108,181,207,215].\\nWhile, for the sake of space, we do not give a more detailed description of such research findings in this survey, we do\\nbelieve that the systematization offered in our work provides a useful starting point for the interested reader to gain a\\nbetter understanding of the main contributions reported in these other research areas.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c05925e2-a49b-4009-92f4-afcbd1bf2290', embedding=None, metadata={'page_label': '25', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 25\\n6 RESOURCES: SOFTWARE LIBRARIES, IMPLEMENTATIONS, AND BENCHMARKS\\nUnified test frameworks play a huge role when evaluating and benchmarking both poisoning attacks and defenses. We\\nthus attempt to give an overview of available resources in this section. Libraries and available code ease the evaluation\\nand benchmarking of both attacks and defenses. Ignoring the many repositories containing individual attacks, to date,\\nonly a few libraries provide implementations of poisoning attacks and defenses.10The library with the largest number\\nof attacks and defenses is the adversarial robustness toolbox (ART) [ 130]. ART implements indiscriminate poisoning\\nattacks [ 16], targeted [ 2,62,148,169] and backdoor attacks [ 70,142], as well as an adaptive backdoor attack [ 152]. The\\nlibrary further provides a range of defenses [ 7,28,60,125,168,173]. Furthermore, SecML [ 122] provides indiscriminate\\npoisoning attacks against SVM, logistic, and ridge regression [ 16,45,188]. Finally, the library advBox [ 67] provides\\nboth indiscriminate and backdoor attacks on a toy problem.\\nBeyond the typical ML datasets that can be used for evaluation, there exists a large database from the NIST compe-\\ntition,11which contains a large number of models from image classification, object recognition, and reinforcement\\nlearning. Each model is labeled as poisoned or not. The module further allows to generate new datasets with poisoned\\nand unpoisoned models. Schwarzschild et al . [146] recently introduced a framework to compare different poisoning\\nattacks. They conclude that for many attacks, the success depends highly on the experimental setting. To conclude,\\nalbeit a huge number of attacks and defenses have been introduced, there is still a need of libraries that allow access to\\noff-the-shelf implementations to compare new approaches. In general, few works benchmark poisoning attacks and\\ndefenses or provide guidelines to evaluate poisoning attacks or defenses.\\n7 DEVELOPMENT, CHALLENGES, AND FUTURE RESEARCH DIRECTIONS\\nIn this section, we outline challenges and future research directions for poisoning attacks and defenses. We start\\nby discussing the intertwined historical development of attacks and defenses, and then highlight the corresponding\\nchallenges, open questions, and promising avenues for further research.\\n7.1 Development Timelines for Poisoning Attacks and Defenses\\nWe start by discussing the historical development of poisoning attacks (represented in Fig. 5), and afterwards that of\\ndefenses (depicted in Fig. 6). In both cases, we highlight the respective milestones and development over time.\\n7.1.1 Attack Timeline. The attack timeline is shown in Fig. 5. To the best of our knowledge, the first example of\\nindiscriminate poisoning was developed in 2006 by Perdisci et al . [134] , Barreno et al . [9], and Newsome et al . [127]\\nin the computer security area. Such attacks, as well as subsequent attacks in the same area [ 90,141], were based on\\nheuristic approaches to mislead application specific ML models, and there was not a unifying mathematical formulation\\ndescribing them. It was only later, in 2012, that indiscriminate poisoning against machine learning was formulated for\\nthe first time as a bilevel optimization [ 190], to compute optimal label-flip poisoning attacks. Since then, indiscriminate\\npoisoning has been studied under two distinct settings, i.e., assuming either (i) that a small fraction of training samples\\ncan be largely perturbed [16, 45, 124]; or (ii) that all training points can be slightly perturbed [53, 55, 121].\\nTargeted and backdoor poisoning attacks only appeared in 2017, and interestingly, they both started from different\\nstrategies. Targeted poisoning started with the bilevel formulation in Koh and Liang [92], but evolved in more heuristic\\napproaches, such as feature collision [ 72,148,212]. Only recently, targeted poisoning attacks were reformulated as\\n10Analysis carried out in June 2022.\\n11https://pages.nist.gov/trojai/docs/index.html', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b47cc470-d8cb-458a-95c8-7391313ba912', embedding=None, metadata={'page_label': '26', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='26 Cinà, Grosse, et al.\\nFig. 5. Timeline for indiscriminate (blue), targeted (red) and backdoor (green) data poisoning attacks on machine learning. Related\\nwork is highlighted with markers of the same color and connected with dashed lines to highlight independent (but related) findings.\\nbilevel problems, given the limitation of the aforementioned heuristic approaches [ 62,80]. Backdoor poisoning started\\nwith the adoption of patch [ 70,110] and functional [ 111,128] triggers. However, in the last years, such heuristic choices\\nhave been put aside, and backdoor attacks are getting closer and closer to the idea of formulating them in terms of a\\nbilevel optimization, not only to enhance their effectiveness, but also their ability to bypass detection [142, 156].\\nThe historical development of the three types of attacks is primarily aimed at solving or mitigating as much as\\npossible the challenges highlighted in Sect. 3.4, i.e., (i) considering more realistic threat models, and (ii) designing more\\neffective and scalable poisoning attacks. In particular, recent developments in attacks seek to improve the applicability\\nof their threat models, by tampering with the training data as little as possible (e.g., a few points altered with invisible\\nperturbations) to evade defenses, and by considering more practical settings (e.g, training-from-scratch ). Moreover, more\\nrecent poisoning attacks aim to tackle the computational complexity and time required to solve the bilevel problem, not\\nonly to improve attack scalability but also their ability to stay undetected against current defenses. In Sect. 7.2 we more\\nthoroughly discuss these challenges, along with some possible future research directions to address them.\\n7.1.2 Defense Timeline. The defense timeline is shown in Fig. 6. The first defenses, training data sanitization and robust\\ntraining variants, were introduced 2008 and 2009 in a security context [ 15,43,125]. Following works in training data\\nsanitization were based on outlier detection, and mitigated backdoor [ 168], indiscriminate [ 96] and targeted [ 58] attacks.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5ed6be81-d3f7-4a72-923d-6f9016de783b', embedding=None, metadata={'page_label': '27', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 27\\n2020:  Model-agnostic input transformations to \\nclean backdoored test data \\n- Li et al., arXiv, 2020 \\n- Villarreal-Vasquez and Bhargava, arXiv 2020 2021: Zhu et al., ICCV  2021, \\nfirst model sanitization for \\ntargeted attacks \\n2021: Zhu et al., ICCV  2021, \\nfirst model inspection for \\ntargeted attacks \\n2019: Wang et al.,  IEEE S&P  2019, \\nreconstruct trigger from a \\nbackdoored model 2018: first defense to clean the \\nmodel: fine-tuning \\n- Liu and Xie, ICCD  2017 \\n- Liu et al, RAID  2018 \\n2019: Wang et al.,  IEEE \\nS&P 2019, clean a model \\nby retraining on \\nreconstructed trigger \\n2018: Model-specific backdoor \\ndetection as outliers in internal \\nrepresentations \\n- Chen et al., arXiv 2018 \\n- Tang et al. arXiv  2019 2019: Wang et al.,  IEEE S&P  2019, \\nDetect backdoored models based on \\ntheir response to reconstructed \\ntrigger (model-specific) 2009: Pioneering work in classification \\nsecurity applications \\n- Nelson et al., ML  in Cyber Trust,  2009 \\n- Biggio et al., Mult. Classifier Systems , 2011 2011: Regularization as a defense \\n- Biggio et al., ACML,  2011 \\n- Carnereno-Cano et al., Security \\n                        and Safety of ML@ICML,  2021 Robust Training \\n2019: Differential privacy as a defense \\n- Ma et al., IJCAI  2019 \\n- Hong et al, arXiv 2020 \\nTrigger Reconstruction \\n2019:  GAN-based trigger generation \\n- Qiao et al., NeurIPS 2019 \\n- Zhu et al.,  ICME  2020 2020: Kolouri et al.,  ICCV  \\n2020, model-agnostic \\nclassifier to detect \\nbackdoored models Model Inspection \\nModel Sanitization \\n2020: Zeng et al, arXiv  2020, \\nretrain using data aug- \\nmentation \\nTest Data Sanitization \\n2017: Outlier detection on model’s \\nbehavior or interpretability \\n- Liu et al., ICCD  2017 \\n- Chou et al, S&P Workshops,  2018 2019: Wang et al.,  IEEE S&P  2019, \\nuse reconstructed trigger to \\nidentify backdoored test inputs Backdoor  Poisoning Indiscriminate Poisoning Targeted Poisoning                                     \\nTechnical/conceptual similarity between approaches (or same work) Training Data Sanitization \\n2016: Remove outlying poisoning samples \\n- Laishram and Phoha, arXiv  2016 \\n- Frederickson et al. , IJCNN 2018 \\n- Tran et al., NeurIPS, 2018 2019: Xiang et al., MLSP  2019, \\ndetect and remove reconstructed \\ntrigger from training data  \\n2008: Pioneering work in \\nsecurity applications \\n- Cretu et al., IEEE S&P 2008 \\nFig. 6. Timeline of the six kinds of defenses described in Sect. 4. The dots remark against which class of attacks defenses have been\\nintroduced, dashed lines denote related approaches, and thin gray lines connect the same work across defense groups.\\nTo train robustly, Biggio et al . [13] showed 2011 that regularization can serve as a defense, a finding recently confirmed\\nfor backdoors [ 25]. In 2019, differential privacy was shown to be able to mitigate poisoning attacks [ 77,116]. This\\nconnection to privacy underlines the need to study poisoning also in relation to other ML security issues, as we will\\ndiscuss in Sect. 7.2.4. The remaining kinds of defenses are characterized by more diverse threat models, as we discussed\\nin Sect. 4.7.1. The type of attack mitigated is however less diverse, and focuses mainly on backdoors, as explained in\\nSect. 4.7.3. We start with model inspection approaches, which were first introduced by Chen et al . [28] and were based\\non outlier detection on latent representations. In 2020, Kolouri et al . [94] generalized the backdoor inspections to be\\nmodel independent using a meta-classifier. Recently, Zhu et al . [214] introduced a search-based approach to determine\\nwhether a model suffers from targeted poisoning. The latter approach also proposed how to sanitize the model. The\\nfirst defenses for such model sanitization against backdoors were trigger agnostic and based on fine-tuning [ 107,112]\\nand later on data augmentation Zeng et al . [200] . Another possibility, introduced by Wang et al . [173] , is to retrain a\\nmodel based on a reconstructed trigger. Wang et al . [173] introduced the idea of reconstruction a trigger in 2019. They\\ngenerated a trigger based on optimization of a pattern that causes backdoor behavior, e.g., misclassification of many\\nsamples when added to them. More recent approaches improve trigger reconstruction by considering distributions\\nover triggers [ 138], not individual patterns. A reconstructed trigger can also serve to inspect a model [ 173], or serve to\\nsanitize test data [ 173]. However, the first approaches to sanitize test data in 2017 were based on outlier detection to', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fe3d8969-9903-42c8-80e7-25c31bd96573', embedding=None, metadata={'page_label': '28', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='28 Cinà, Grosse, et al.\\ninspect the model inspection and sanitize training data. Analogous to model inspection, initial works relied on latent,\\nmodel specific features [112] whereas later works from 2020 use model-agnostic input transformations [104].\\nOne historical development which is highly relevant but left out in both timeline figures is the study of adaptive\\nattacks against defenses to assess their robustness, as discussed in Sect. 4.7.2. We elaborate on this challenge in Sect. 7.2.3.\\n7.2 Challenges and Future Work\\nBuilding on the development timelines and the corresponding overview provided in Sect. 7.1, we formulate some future\\nresearch challenges for both poisoning attacks and defenses in the remainder of this section.\\n7.2.1 Considering Realistic Threat Models. One pertinent challenge arising from the discussion on poisoning attacks in\\nSect. 3.4.1 demands considering more realistic threat models and attack scenarios, as also recently pointed out in [ 150].\\nWhile assessing machine learning models in real-world settings is not straightforward [ 154], the need to develop\\nrealistic threat models is still an open question in machine learning security and has so far only received recognition\\nfor test-time attacks [ 63]. Here we define some guidelines that can serve as a basis for future work that wants to assess\\nthe real safety impact of poisoning versus real applications. First, limit the attacker’s knowledge of the target system\\nand their capacity to tamper during training. For example, an attack that assumes only a small percentage of control\\nover the training set can be broadly applied. Second, develop more stealthy poisoning strategies to avoid detection\\nagainst defenses. Some attack strategies, e.g., patch trigger or feature collision, are computationally efficient, but several\\ndefensive countermeasures exist to detect them (see Table 4). Finally, evaluating poisoning attacks against real-world\\napplications and making them adaptive to the presence of a defender. Therefore, we invite the research community\\nto evaluate poisoning attacks with more realistic or less favorable assumptions for the attacker, which also take into\\naccount the specific application domain.\\n7.2.2 Designing More Effective and Scalable Poisoning Attacks. The other challenge we highlighted in Sect. 3.4.2 is the\\ncomputational complexity of poisoning attacks when relying on bilevel optimization. However, the same limitation is\\nalso encountered in other research domains such as hyperparameter optimization and meta-learning which naturally\\nare formulated within the mathematical framework of bilevel programming [ 57]. More concretely, the former is the\\nprocess of determining the optimal combination of hyperparameters that maximizes the performance of an underlying\\nlearning algorithm. On the other hand, meta-learning encompasses feature selection, algorithm selection, learning\\nto learn, or ensemble learning, to which the same reasoning applies. Having formulated poisoning attacks within\\nthe bilevel framework (see Sect. 3.6) hints that strategies developed to speed up the optimization of bilevel programs\\ninvolved in meta-learning or hyperparameter optimization taks can be adapted to facilitate the development of novel\\nscalable attacks. In principle, by imagining poisoning samples as the attacker-controlled learning hyperparameters,\\nwe could apply the approaches proposed in these two fields to mount an attack. Notably, we find some initial works\\nconnecting these two fields with data poisoning. For example, Shen et al . [151] rely in their approach on a k-arms\\ntechnique, a technique similar to bandits, as done by Jones et al . [87] . Further, Muñoz-González et al . [124] exploited\\nthe back-gradient optimization technique proposed in [ 49,117], originally proposed for hyperparameter optimization,\\nand subsequently, Huang et al . [80] inherited the same approach making the attack more effective against deep neural\\nnetworks. Apart from the work just mentioned, the connection between the two fields and poisoning is still currently\\nunder-investigated, and other ideas could still be explored. For example, the optimization proposed by [ 114] can further\\nreduce run-time complexity and memory usage even when dealing with millions of hyperparameters. Or another\\nway might be to move away from gradient-based approaches and consider gradient-free approaches, thus overcoming', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8c9912ee-8e78-4616-b28d-06e53b26a08f', embedding=None, metadata={'page_label': '29', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 29\\nthe complexity of the inverting the Hessian matrix seen in Sect. 3.4.2. In the area of gradient-free methods, the most\\nstraightforward way is to use grid or random search [ 11], which can be sped up using reinforcement learning [ 98].\\nAlso, Bayesian optimization has been used, given a few sampled points from the objective and constraint functions,\\nto approximate the target function [ 87]. Last but not least, evolutionary algorithms [ 198] as well as particle swarm\\noptimization [113] have shown to be successful.\\nIn conclusion, we consider these two fields as possible future research directions to find more effective and scalable\\npoisoning attacks for assessing ML robustness in practice.\\n7.2.3 Systematizing and Improving Defense Evaluations. Regardless of future attacks, we need to systematize and\\nunderstand the limits of existing (and future) defenses better. As we have seen in Sect. 6, there is no coherent benchmark\\nfor defenses. Such a benchmark exposes flawed evaluations and assesses the robustness of a defense per se or in\\nrelation to other defenses (taking into account the defense’s setting, as discussed in Sect. 4.7.2). Jointly with benchmarks,\\nevaluation guidelines, as discussed for ML evasion by Carlini et al . [24] , help to improve defense evaluation. More\\nspecifically, these guidelines can encompass knowledge when attacks fail and why, similar to work on evasion attack\\nfailure [ 136]. Crucial in this context is also, as discussed in Sect. 4.7.2, to expand our understanding of adaptive attacks.\\nAn orthogonal question is how to increase existing knowledge about trade-offs between for example attack strength\\nand stealthiness for indiscriminate attacks [ 58] or backdoors [ 33,143,169]. Further trade-offs relate clean accuracy\\nand accuracy under the poisoning attack by hyperparameter tuning. More concretely, Demontis et al . [45] and Cinà\\net al. [40] showed that more regularized classifiers tend to resist better to poisoning attacks, at the cost of slightly\\nreducing clean accuracy. Ideally, impossibility results further increase our knowledge about hard limitations. To the best\\nof our knowledge, the only impossibility results provided thus far for subpopulation poisoning attacks can be found\\nin [84], showing that it is impossible to defend poisoning attacks that target only a fraction of the data. Expanding our\\nknowledge about trade-offs and impossibilities will help to design and configure effective defenses.\\n7.2.4 Designing Generic Defenses against Multiple Attacks. Such effective defenses also need to overcome, as discussed\\nin the Sect. 4.7.3, that they often specialize and to on one or several poisoning attacks (for example backdoor and\\ntargeted). Such one-sided evaluations introduce a bias, and the effect of this overfitting on biased datasets has been\\nrecognized in image recognition [ 166], but received relatively little attention in security so far. Some defenses, however,\\ndo evaluate several poisoning attacks [ 61,77,151], or even different ML security threats like backdoors and evasion [ 160]\\nor poisoning and privacy [77].\\nIn addition to creating more robust defenses, such interdisciplinary works also increase our understanding of how\\npoisoning interferes with non-poisoning ML attacks [ 27,178]. One attack is evasion, where a small perturbation is\\nadded to a sample at test time to force the model to misclassify an output. Evasion is closely related, but different\\nfrom backdoors, which add a fixed perturbation at training time , causing an upfront known vulnerability at test time.\\nOnly a few works study evasion and poisoning together. For example, Sun et al . [160] introduce a defense against both\\nbackdoors and adversarial examples. Furthermore, Fowl et al . [56] show that adversarial examples with the original\\nlabels are strong poisons at training time. In the opposite direction, Weng et al . [178] find that if backdoor accuracy is\\nhigh, evasion tends to be less successful and vice versa. Furthermore, Mehra et al . [120] study poisoning of certified\\nevasion defenses: using poisoning, they decrease the certified radius and accuracy. Two works, namely by Manoj and\\nBlum [118] and Goldwasser et al . [65] , relate evasion and backdoors in a theoretical way. Both share rigid assumptions,\\nhowever Manoj and Blum [118] show an impossibility results in terms of non-existence of backdoor for some natural\\nlearning problems. Goldwasser et al . [65] , on the other hand, show that backdoor detection might be impossible. In', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c70708b3-53b1-486f-804a-dca2e05d3249', embedding=None, metadata={'page_label': '30', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='30 Cinà, Grosse, et al.\\nrelation to privacy or intellectual property, poisoning can be used to increase the information leakage from training\\ndata at test time on collaborative learning [ 27]. Privacy can further be a defense against poisoning [ 22,77], or poisoning\\ncan be a tool to obtain [ 55]. Summarizing, there is little knowledge on how poisoning interacts with other attacks. More\\nwork is needed to understand this relationship and secure machine learning models in practice against several threats\\nat the same time.\\n8 CONCLUDING REMARKS\\nThe increasing adoption of data-driven models in production systems demands a rigorous analysis of their reliability\\nin the presence of malicious users aiming to compromise them. Within this survey, we systematize a broad spectrum\\nof data poisoning attacks and defenses according to our modeling framework, and we exploit such categorization\\nto match defenses with the corresponding attacks they prevent. Moreover, we provide a unified formalization for\\npoisoning attacks via bilevel programming, and we spotlight resources (e.g., software libraries, datasets) that may\\nbe exploited to benchmark attacks and defenses. Finally, we trace the historical development of data literature since\\nthe early developments dating back to more than 20 years ago and find the open challenges and possible research\\ndirections that can pave the way for future development. In conclusion, we believe our contribution can help clarify\\nwhat threats an ML system may encounter in adversarial settings and encourage further research developments in\\ndeploying trustworthy systems even in the presence of data poisoning threats.\\nACKNOWLEDGMENTS\\nThis work has been partly supported by: the PRIN 2017 project RexLearn (grant no. 2017TWNMH2), funded by the\\nItalian Ministry of Education, University and Research; the EU’s Horizon Europe research and innovation program\\nunder the project ELSA, grant agreement No 101070617; the project “TrustML: Towards Machine Learning that Humans\\nCan Trust”, CUP: F73C22001320007, funded by Fondazione di Sardegna; the NRRP MUR program funded by the EU -\\nNGEU under the project SERICS (PE00000014); and the COMET Programme managed by FFG in the COMET Module\\nS3AI, funded by BMK, BMDW, and the Province of Upper Austria.\\nREFERENCES\\n[1]Hojjat Aghakhani, Thorsten Eisenhofer, Lea Schönherr, Dorothea Kolossa, Thorsten Holz, Christopher Kruegel, and Giovanni Vigna. 2020.\\nVENOMAVE: Clean-Label Poisoning Against Speech Recognition. arXiv:2010.10682 (2020).\\n[2]Hojjat Aghakhani, Dongyu Meng, Yu-Xiang Wang, Christopher Kruegel, and Giovanni Vigna. 2021. Bullseye polytope: A scalable clean-label\\npoisoning attack with improved transferability. In EuroS&P . 159–178.\\n[3] Chace Ashcraft and Kiran Karra. 2021. Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers. arXiv:2106.07798 (2021).\\n[4] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. 2020. How To Backdoor Federated Learning. In The 23rd\\nInt. Conf. on AI and Statistics, AISTATS . PMLR, 2938–2948.\\n[5] Peter Bajcsy and Michael Majurski. 2021. Baseline Pruning-Based Approach to Trojan Detection in Neural Networks. arXiv:2101.12016 (2021).\\n[6]Kiarash Banihashem, Adish Singla, and Goran Radanovic. 2021. Defense Against Reward Poisoning Attacks in Reinforcement Learning.\\narXiv:2102.05776 (2021).\\n[7]Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, Amir Safavi, and Rui Zhang. 2018. Detecting poisoning attacks on ML in iot environments. In\\nIEEE Int. congress on internet of things, ICIOT 2018 . IEEE, 57–64.\\n[8]Mauro Barni, Kassem Kallas, and Benedetta Tondi. 2019. A New Backdoor Attack in CNNS by Training Set Corruption Without Label Poisoning. In\\n2019 IEEE Int. Conf. on Image Proc., ICIP 2019 . IEEE, 101–105.\\n[9]Marco Barreno, Blaine Nelson, Russell Sears, Anthony D. Joseph, and J. D. Tygar. 2006. Can ML be secure?. In ACM Symposium on Inf., Computer\\nand Communications Security, ASIACCS . ACM, 16–25.\\n[10] Vahid Behzadan and Arslan Munir. 2017. Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks. In ML and Data Mining in\\nPattern Recognition - 13th Int. Conf., MLDM 2017 . Springer, 262–275.\\n[11] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of ML research 13, 2 (2012).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='00d7ef5f-c0f2-4940-b70b-f9191e5b321c', embedding=None, metadata={'page_label': '31', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 31\\n[12] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin B. Calo. 2019. Analyzing Federated Learning through an Adversarial Lens.\\nIn36th Int. Conf. on ML, ICML 2019 . PMLR, 634–643.\\n[13] Battista Biggio, Igino Corona, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli. 2011. Bagging classifiers for fighting poisoning attacks in\\nadversarial classification tasks. In Int. workshop on multiple classifier Sys. Springer, 350–359.\\n[14] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion Attacks\\nagainst ML at Test Time. In ML and Knowl. Disc. in Databases - Eur. Conf., ECML PKDD 2013 . Springer, 387–402.\\n[15] Battista Biggio, Blaine Nelson, and Pavel Laskov. 2011. Support Vector Machines Under Adversarial Label Noise. In ACML2011 . 97–112.\\n[16] Battista Biggio, Blaine Nelson, and Pavel Laskov. 2012. Poisoning Attacks against Support Vector Machines. In ICML 2012 . icml.cc / Omnipress.\\n[17] Battista Biggio, Ignazio Pillai, Samuel Rota Bulò, Davide Ariu, Marcello Pelillo, and Fabio Roli. 2013. Is data clustering in adversarial settings\\nsecure?. In 6th ACM Workshop on Art. Intell. and Sec., AISec 2013 . ACM, 87–98.\\n[18] Battista Biggio, Konrad Rieck, Davide Ariu, Christian Wressnegger, Igino Corona, Giorgio Giacinto, and Fabio Roli. 2014. Poisoning behavioral\\nmalware clustering. In 7th ACM Workshop on Art. Intell. and Sec., AISec 2014 . ACM, 27–36.\\n[19] Battista Biggio and Fabio Roli. 2018. Wild patterns: Ten years after the rise of adversarial ML. Pattern Recognition 84 (2018), 317–331.\\n[20] Aleksandar Bojchevski and Stephan Günnemann. 2019. Adversarial Attacks on Node Embeddings via Graph Poisoning. In ICML . 695–704.\\n[21] Eitan Borgnia, Valeriia Cherepanova, Liam Fowl, Amin Ghiasi, Jonas Geiping, Micah Goldblum, Tom Goldstein, and Arjun Gupta. 2021. Strong\\ndata augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff. In IEEE ICASSP 2021 . IEEE, 3855–3859.\\n[22] Eitan Borgnia, Jonas Geiping, Valeriia Cherepanova, Liam Fowl, Arjun Gupta, Amin Ghiasi, Furong Huang, Micah Goldblum, and Tom Goldstein.\\n2021. DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations. arXiv:2103.02079 (2021).\\n[23] Di Cao, Shan Chang, Zhijian Lin, Guohua Liu, and Donghong Sun. 2019. Understanding Distributed Poisoning Attack in Federated Learning. In\\nIEEE Int. Conf. on Parallel and Distributed Sys. 233–239.\\n[24] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and\\nAlexey Kurakin. 2019. On evaluating adversarial robustness. arXiv:1902.06705 (2019).\\n[25] Javier Carnerero-Cano, Luis Muñoz-González, Phillippa Spencer, and Emil C Lupu. 2021. Regularization Can Help Mitigate Poisoning Attacks...\\nwith the Right Hyperparameters. arXiv:2105.10948 (2021).\\n[26] Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep Mukhopadhyay. 2018. Adversarial attacks and defences: A\\nsurvey. arXiv:1810.00069 (2018).\\n[27] Melissa Chase, Esha Ghosh, and Saeed Mahloujifar. 2021. Property Inference From Poisoning. arXiv:2101.11073 (2021).\\n[28] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. 2018.\\nDetecting Backdoor Attacks on Deep Neural Networks by Activation Clustering. arXiv:1811.03728 (2018).\\n[29] Chuanshuai Chen and Jiazhu Dai. 2020. Mitigating backdoor attacks in LSTM-based Text Classification Sys. by Backdoor Keyword Identification.\\narXiv:2007.12070 (2020).\\n[30] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. 2019. DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep\\nNeural Networks. In Int. Joint Conf. on AI, IJCAI 2019 . 4658–4664.\\n[31] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. 2017. ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep\\nNeural Networks Without Training Substitute Models. In 10th ACM Workshop on AI and Security (AISec ’17) . 15–26.\\n[32] Ruoxin Chen, Zenan Li, Jie Li, Junchi Yan, and Chentao Wu. 2022. On Collective Robustness of Bagging Against Data Poisoning. In ICML .\\n[33] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted backdoor attacks on deep learning systems using data poisoning.\\narXiv:1712.05526 (2017).\\n[34] Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, and Yang Zhang. 2021. Badnl: Backdoor attacks against nlp models. In ICML Workshop on\\nAdversarial ML (2021) .\\n[35] Hao Cheng, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Pu Zhao, and Xue Lin. 2020. Defending against backdoor attack on deep neural networks.\\narXiv:2002.12162 (2020).\\n[36] Siyuan Cheng, Yingqi Liu, Shiqing Ma, and Xiangyu Zhang. 2021. Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification.\\nInThirty-Fifth AAAI Conf. on AI 2021 . AAAI Press, 1148–1156.\\n[37] Edward Chou, Florian Tramer, and Giancarlo Pellegrino. 2020. Sentinet: Detecting localized universal attacks against deep learning systems. In\\nIEEE Security and Privacy Workshops, SPW 2020 . IEEE, 48–54.\\n[38] Antonio Emanuele Cinà, Ambra Demontis, Battista Biggio, Fabio Roli, and Marcello Pelillo. 2022. Energy-Latency Attacks via Sponge Poisoning.\\narXiv:2203.08147 (2022).\\n[39] Antonio Emanuele Cinà, Kathrin Grosse, Ambra Demontis, Battista Biggio, Fabio Roli, and Marcello Pelillo. 2022. Machine Learning Security\\nagainst Data Poisoning: Are We There Yet? CoRR abs/2204.05986 (2022).\\n[40] Antonio Emanuele Cinà, Kathrin Grosse, Sebastiano Vascon, Ambra Demontis, Battista Biggio, Fabio Roli, and Marcello Pelillo. 2021. Backdoor\\nLearning Curves: Explaining Backdoor Poisoning Beyond Influence Functions. arXiv:2106.07214 (2021).\\n[41] Antonio Emanuele Cinà, Alessandro Torcinovich, and Marcello Pelillo. 2022. A black-box adversarial attack for poisoning clustering. Pattern\\nRecognition 122 (2022), 108306.\\n[42] Antonio Emanuele Cinà, Sebastiano Vascon, Ambra Demontis, Battista Biggio, Fabio Roli, and Marcello Pelillo. 2021. The Hammer and the Nut: Is\\nBilevel Optimization Really Needed to Poison Linear Classifiers?. In Int. Joint Conf. on Neural Networks, IJCNN 2021 . IEEE, 1–8.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6e77c931-055f-4dc0-979e-c34faf5eeb52', embedding=None, metadata={'page_label': '32', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='32 Cinà, Grosse, et al.\\n[43] G.F. Cretu, A. Stavrou, M.E. Locasto, S.J. Stolfo, and A.D. Keromytis. 2008. Casting out Demons: Sanitizing Training Data for Anomaly Sensors. In\\nSecurity and Privacy, 2008. SP 2008. IEEE Symposium on . 81 –95.\\n[44] Ambra Demontis, Battista Biggio, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli. 2017. Infinity-Norm Support Vector Machines Against\\nAdversarial Label Contamination. In ITASEC (CEUR Workshop Proceedings, Vol. 1816) . CEUR-WS.org, 106–115.\\n[45] Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio, Alina Oprea, Cristina Nita-Rotaru, and Fabio Roli. 2019. Why Do\\nAdversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks. In USENIX Sec. Symp. USENIX Association, 321–338.\\n[46] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. 2019. Sever: A robust meta-algorithm for\\nstochastic optimization. In Int. Conf. on ML . PMLR, 1596–1606.\\n[47] Bao Gia Doan, Ehsan Abbasnejad, and Damith C Ranasinghe. 2020. Februus: Input purification defense against trojan attacks on deep neural\\nnetwork systems. In Computer Security Applications Conf. 897–912.\\n[48] Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. 2021. LIRA: Learnable, Imperceptible and Robust Backdoor Attacks. In IEEE ICCV . 11966–11976.\\n[49] Justin Domke. 2012. Generic Methods for Optimization-Based Modeling. In 15th Int. Conf. on Art. Intell. and Statistics, AISTATS 2012 . JMLR, 318–326.\\n[50] Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, and Jun Zhu. 2021. Black-box Detection of Backdoor Attacks with\\nLimited Information and Data. In ICCV .\\n[51] Min Du, Ruoxi Jia, and Dawn Song. 2020. Robust anomaly detection and backdoor attack detection via differential privacy. In ICLR, 2020 .\\n[52] Tom Everitt, Victoria Krakovna, Laurent Orseau, and Shane Legg. 2017. Reinforcement learning with a corrupted reward channel. In 26th Int. Joint\\nConf. on AI, IJCAI 2017 . 4705–4713.\\n[53] Ji Feng, Qi-Zhi Cai, and Zhi-Hua Zhou. 2019. Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder. In NeurIPS .\\n[54] Jiashi Feng, Huan Xu, Shie Mannor, and Shuicheng Yan. 2014. Robust Logistic Regression and Classification. In Advances in Neural Inf. Proc. Sys.,\\nNIPS . 253–261.\\n[55] Liam Fowl, Ping-yeh Chiang, Micah Goldblum, Jonas Geiping, Arpit Bansal, Wojtek Czaja, and Tom Goldstein. 2021. Preventing unauthorized use\\nof proprietary data: Poisoning for secure dataset release. arXiv:2103.02683 (2021).\\n[56] Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojtek Czaja, and Tom Goldstein. 2021. Adversarial Examples Make Strong Poisons.\\narXiv:2106.10807 .\\n[57] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. 2018. Bilevel Programming for Hyperparameter\\nOptimization and Meta-Learning. In ICML , Vol. 80. PMLR, 1563–1572.\\n[58] Christopher Frederickson, Michael Moore, Glenn Dawson, and Robi Polikar. 2018. Attack strength vs. detectability dilemma in adversarial ML. In\\nInt. Joint Conf. on Neural Networks, IJCNN 2018 . IEEE, 1–8.\\n[59] Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jiliang Zhang, Anmin Fu, Surya Nepal, and Hyoungshick Kim. 2020. Backdoor attacks and\\ncountermeasures on deep learning: A comprehensive review. arXiv:2007.10760 (2020).\\n[60] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. 2019. Strip: A defence against trojan attacks on\\ndeep neural networks. In Computer Security Applications Conf. 113–125.\\n[61] Jonas Geiping, Liam Fowl, Gowthami Somepalli, Micah Goldblum, Michael Moeller, and Tom Goldstein. 2021. What Doesn’t Kill You Makes You\\nRobust (er): Adversarial Training against Poisons and Backdoors. arXiv:2102.13624 (2021).\\n[62] Jonas Geiping, Liam H. Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, and Tom Goldstein. 2021. Witches’ Brew: Industrial\\nScale Data Poisoning via Gradient Matching. In ICLR 2021 . OpenReview.\\n[63] Justin Gilmer, Ryan P Adams, Ian Goodfellow, David Andersen, and George E Dahl. 2018. Motivating the rules of the game for adversarial example\\nresearch. arXiv:1807.06732 (2018).\\n[64] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, and Tom Goldstein. 2022.\\nDataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses. IEEE Transactions on PAMI (2022), 1–1.\\n[65] Shafi Goldwasser, Michael P Kim, Vinod Vaikuntanathan, and Or Zamir. 2022. Planting undetectable backdoors in machine learning models.\\narXiv:2204.06974 (2022).\\n[66] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples. In ICLR 2015 .\\n[67] Dou Goodman, Hao Xin, Wang Yang, Wu Yuesheng, Xiong Junfeng, and Zhang Huan. 2020. Advbox: a toolbox to generate adversarial examples\\nthat fool neural networks. arXiv:2001.05574 (2020).\\n[68] Kathrin Grosse, Lukas Bieringer, Tarek Richard Besold, Battista Biggio, and Katharina Krombholz. 2023. Machine Learning Security in Industry: A\\nQuantitative Survey. IEEE Transactions on Information Forensics and Security (2023).\\n[69] Kathrin Grosse, Taesung Lee, Battista Biggio, Youngja Park, Michael Backes, and Ian Molloy. 2022. Backdoor Smoothing: Demystifying Backdoor\\nAttacks on Deep Neural Networks. Computers & Security (2022), 102814.\\n[70] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifying vulnerabilities in the ML model supply chain. arXiv (2017).\\n[71] Junfeng Guo, Ang Li, and Cong Liu. 2022. AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis. In ICLR,2022 .\\n[72] Junfeng Guo and Cong Liu. 2020. Practical Poisoning Attacks on Neural Networks. In 16th Eur. Conf. on Com. Vis., ECCV 2020 . Springer, 142–158.\\n[73] Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song. 2019. Tabor: A highly accurate approach to inspecting and restoring trojan\\nbackdoors in AI Systems. arXiv:1908.01763 (2019).\\n[74] Yi Han, David Hubczenko, Paul Montague, Olivier De Vel, Tamas Abraham, Benjamin IP Rubinstein, Christopher Leckie, Tansu Alpcan, and Sarah\\nErfani. 2020. Adversarial Reinforcement Learning under Partial Observability in Autonomous Computer Network Defence. In Int. Joint Conf. on', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5c680f85-ee02-409f-8f8b-1d614279a0be', embedding=None, metadata={'page_label': '33', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 33\\nNeural Networks, IJCNN 2020 . IEEE, 1–8.\\n[75] Jonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. 2021. SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics.\\nInInt. Conf. on ML, ICML 2020 . PMLR, 4129–4139.\\n[76] Jamie Hayes and Olga Ohrimenko. 2018. Contamination Attacks and Mitigation in Multi-Party ML. Advances in Neural Inf. Proc. Sys., NIPS 31\\n(2018), 6604–6615.\\n[77] Sanghyun Hong, Varun Chandrasekaran, Yiğitcan Kaya, Tudor Dumitraş, and Nicolas Papernot. 2020. On the effectiveness of mitigating data\\npoisoning attacks with gradient shaping. arXiv:2002.11497 (2020).\\n[78] Xiaoling Hu, Xiao Lin, Michael Cogswell, Yi Yao, Susmit Jha, and Chao Chen. 2022. Trigger Hunting with a Topological Prior for Trojan Detection.\\nInICLR, 2022 .\\n[79] Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. 2022. Backdoor Defense via Decoupling the Training Process. In ICLR, 2022 .\\n[80] W. Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein. 2020. MetaPoison: Practical General-purpose Clean-label Data\\nPoisoning. In Advances in Neural Inf. Proc. Sys., NeurIPS .\\n[81] Xijie Huang, Moustafa Alzantot, and Mani Srivastava. 2019. Neuroninspect: Detecting backdoors in neural networks via output explanations.\\narXiv:1911.07399 (2019).\\n[82] Yunhan Huang and Quanyan Zhu. 2019. Deceptive reinforcement learning under adversarial manipulations on cost signals. In Int. Conf. on Decision\\nand Game Theory for Security . Springer, 217–237.\\n[83] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. 2018. Manipulating ML: Poisoning attacks and\\ncountermeasures for regression learning. In IEEE Symposium on Security and Privacy, S&P 2018 . IEEE, 19–35.\\n[84] Matthew Jagielski, Giorgio Severi, Niklas Pousette Harger, and Alina Oprea. 2021. Subpopulation data poisoning attacks. In ACM SIGSAC Conf. on\\nComputer and Communications Security, CCS 2021 . 3104–3122.\\n[85] Mojan Javaheripi, Mohammad Samragh, Gregory Fields, Tara Javidi, and Farinaz Koushanfar. 2020. CLEANN: Accelerated trojan shield for\\nembedded neural networks. In IEEE/ACM Int. Conf. On Computer Aided Design, ICCAD 2020 . IEEE, 1–9.\\n[86] Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong. 2020. Certified robustness of nearest neighbors against data poisoning attacks. arXiv (2020).\\n[87] Donald R Jones, Matthias Schonlau, and William J Welch. 1998. Efficient global optimization of expensive black-box functions. Journal of Global\\noptimization 13, 4 (1998), 455–492.\\n[88] Sara Kaviani and Insoo Sohn. 2021. Defense against neural trojan attacks: A survey. Neurocomputing 423 (2021), 651–667.\\n[89] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. 2020. TrojDRL: evaluation of backdoor attacks on deep reinforcement learning.\\nInACM/IEEE Design Automation Conf., DAC 2020 . IEEE, 1–6.\\n[90] Marius Kloft and Pavel Laskov. 2010. Online Anomaly Detection under Adversarial Impact. In AISTATS 2010 . JMLR, 405–412.\\n[91] Stefanos Koffas, Jing Xu, Mauro Conti, and Stjepan Picek. 2021. Can You Hear It? Backdoor Attacks via Ultrasonic Triggers. arXiv (2021).\\n[92] Pang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In Int. Conf. on ML, ICML . PMLR, 1885–1894.\\n[93] Pang Wei Koh, Jacob Steinhardt, and Percy Liang. 2022. Stronger data poisoning attacks break data sanitization defenses. Machine Learning 111\\n(2022), 1–47.\\n[94] Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. 2020. Universal litmus patterns: Revealing backdoor attacks in cnns. In\\nIEEE/CVF Int. Conf. on Computer Vision, ICCV 2021 . 301–310.\\n[95] Ram Shankar Siva Kumar, Magnus Nyström, John Lambert, Andrew Marshall, Mario Goertzel, Andi Comissoneru, Matt Swann, and Sharon Xia.\\n2020. Adversarial ML-industry perspectives. (2020), 69–75.\\n[96] Ricky Laishram and Vir Virander Phoha. 2016. Curie: A method for protecting SVM Classifier from Poisoning Attack. arXiv:1606.01584 (2016).\\n[97] Alexander Levine and Soheil Feizi. 2021. Deep Partition Aggregation: Provable Defenses against General Poisoning Attacks. In ICLR 2021 .\\n[98] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2017. Hyperband: A novel bandit-based approach to\\nhyperparameter optimization. The Journal of ML Research 18, 1 (2017), 6765–6816.\\n[99] Shaofeng Li, Minhui Xue, Benjamin Zhao, Haojin Zhu, and Xinpeng Zhang. 2020. Invisible Backdoor Attacks on Deep Neural Networks via\\nSteganography and Regularization. IEEE Trans. on Dependable and Secure Computing PP (09 2020), 1–1.\\n[100] Yige Li, Nodens Koren, Lingjuan Lyu, Xixiang Lyu, Bo Li, and Xingjun Ma. 2021. Neural Attention Distillation: Erasing Backdoor Triggers from\\nDeep Neural Networks. ICLR, 2021 .\\n[101] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. 2021. Invisible Backdoor Attack With Sample-Specific Triggers. In\\nIEEE/CVF Int. Conf. on Computer Vision, ICCV 2021 . 16463–16472.\\n[102] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. 2021. Anti-backdoor learning: Training clean models on poisoned data.\\nAdvances in Neural Inf. Proc. Sys., NeurIPS 34.\\n[103] Yiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. 2020. Backdoor learning: A survey. arXiv:2007.08745 (2020).\\n[104] Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shutao Xia. 2020. Rethinking the trigger of backdoor attack. arXiv (2020).\\n[105] Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang. 2020. Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features.\\nInSIGSAC Conf. on Computer and Communications Security, CCS 2020 . 113–131.\\n[106] Chang Liu, Bo Li, Yevgeniy Vorobeychik, and Alina Oprea. 2017. Robust Linear Regression Against Training Data Poisoning. In 10th ACM Workshop\\non Art. Intell. and Sec., AISec 2017 . ACM, 91–102.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b1942535-0fc4-4840-938e-0fbd1ba5ba97', embedding=None, metadata={'page_label': '34', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='34 Cinà, Grosse, et al.\\n[107] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2018. Fine-pruning: Defending against backdooring attacks on deep neural networks. In Int.\\nSymposium on Research in Attacks, Intrusions, and Defenses . Springer, 273–294.\\n[108] Xuanqing Liu, Si Si, Jerry Zhu, Yang Li, and Cho-Jui Hsieh. 2019. A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised\\nLearning. In Advances in Neural Inf. Proc. Sys., NeurIPS 2019 . 9777–9787.\\n[109] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. 2019. ABS: Scanning Neural Networks for Back-doors\\nby Artificial Brain Stimulation. In ACM SIGSAC Conf. on Computer and Communications Security, CCS 2019 . ACM, 1265–1282.\\n[110] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. 2018. Trojaning Attack on Neural Networks.\\nInNetwork and Distributed System Sec. Symp., NDSS . 45–48.\\n[111] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. 2020. Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks. In Computer\\nVision - ECCV 2020 - 16th Eur. Conf. Springer, 182–199.\\n[112] Yuntao Liu, Yang Xie, and Ankur Srivastava. 2017. Neural Trojans. In IEEE Int. Conf. on Computer Design, ICCD 2017 . 45–48.\\n[113] Pablo Ribalta Lorenzo, Jakub Nalepa, Michal Kawulok, Luciano Sanchez Ramos, and José Ranilla Pastor. 2017. Particle swarm optimization for\\nhyper-parameter selection in deep neural networks. In the genetic and evolutionary computation conference . 481–488.\\n[114] Jonathan Lorraine, Paul Vicol, and David Duvenaud. 2020. Optimizing millions of hyperparameters by implicit differentiation. In Int. Conf. on AI\\nand Statistics . PMLR, 1540–1552.\\n[115] Yuzhe Ma, Kwang-Sung Jun, Lihong Li, and Xiaojin Zhu. 2018. Data poisoning attacks in contextual bandits. In Int. Conf. on Decision and Game\\nTheory for Security . Springer, 186–204.\\n[116] Yuzhe Ma, Xiaojin Zhu, and Justin Hsu. 2019. Data Poisoning against Differentially-Private Learners: Attacks and Defenses. In IJCAI . 4732–4738.\\n[117] Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. 2015. Gradient-based Hyperparameter Optimization through Reversible Learning. In\\n32nd Int. Conf. on ML, ICML 2015 . JMLR, 2113–2122.\\n[118] Naren Manoj and Avrim Blum. 2021. Excess capacity and backdoor poisoning. Advances in Neural Inf. Proc. Sys., NeurIPS 34 (2021), 20373–20384.\\n[119] Sean McGregor. 2020. Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database. arXiv:2011.08512 (2020).\\n[120] Akshay Mehra, Bhavya Kailkhura, Pin-Yu Chen, and Jihun Hamm. 2021. How Robust are Randomized Smoothing based Defenses to Data Poisoning?.\\nInIEEE/CVF Int. Conf. on Computer Vision, ICCV 2021 . 13244–13253.\\n[121] Shike Mei and Xiaojin Zhu. 2015. Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners. In AAAI . 2871–2877.\\n[122] Marco Melis, Ambra Demontis, Maura Pintor, Angelo Sotgiu, and Battista Biggio. 2019. secml: A Python Library for Secure and Explainable ML.\\narXiv:1912.10013 (2019).\\n[123] Nicolas M. Müller, Daniel Kowatsch, and Konstantin Böttinger. 2020. Data Poisoning Attacks on Regression Learning and Corresponding Defenses.\\nIn25th IEEE Pacific Rim Int. Symposium on Dependable Computing, PRDC 2020 . IEEE, 80–89.\\n[124] Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C. Lupu, and Fabio Roli. 2017. Towards\\nPoisoning of Deep Learning Algorithms with Back-gradient Optimization. In 10th ACM Workshop on Art. Intell. and Sec., AISec 2017 . ACM, 27–38.\\n[125] Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP Rubinstein, Udam Saini, Charles Sutton, JD Tygar, and Kai Xia.\\n2009. Misleading learners: Co-opting your spam filter. In ML in Cyber Trust . Springer, 17–51.\\n[126] Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D. Joseph, Benjamin I. P. Rubinstein, Udam Saini, Charles Sutton, J. Doug Tygar, and Kai\\nXia. 2008. Exploiting ML to Subvert Your Spam Filter. In USENIX Workshop on Large-Scale Exploits and Emergent Threats, LEET 2008 . USENIX, 1–9.\\n[127] James Newsome, Brad Karp, and Dawn Xiaodong Song. 2006. Paragraph: Thwarting Signature Learning by Training Maliciously. In RAID 2006\\n(Lecture Notes in Computer Science, Vol. 4219) . Springer, 81–105.\\n[128] Tuan Anh Nguyen and Anh Tran. 2020. Input-Aware Dynamic Backdoor Attack. In Advances in Neural Inf. Proc. Sys., NeurIPS .\\n[129] Tuan Anh Nguyen and Anh Tuan Tran. 2021. WaNet - Imperceptible Warping-based Backdoor Attack. In ICLR, 2021 . OpenReview.net.\\n[130] Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser, Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo,\\nBryant Chen, Heiko Ludwig, Ian Molloy, and Ben Edwards. 2018. Adversarial Robustness Toolbox v1.2.0. CoRR 1807.01069 (2018).\\n[131] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. 2016. Transferability in ML: from phenomena to black-box attacks using adversarial\\nsamples. arXiv:1605.07277 (2016).\\n[132] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. 2017. Practical Black-Box Attacks\\nagainst ML. In ACM Asia Conf. on Computer and Communications Security, AsiaCCS 2017 . ACM, 506–519.\\n[133] Andrea Paudice, Luis Muñoz-González, and Emil C Lupu. 2018. Label sanitization against label flipping poisoning attacks. In Joint Eur. Conf. on ML\\nand Knowledge Discovery in Databases . Springer, 5–15.\\n[134] R. Perdisci, D. Dagon, Wenke Lee, P. Fogla, and M. Sharif. 2006. Misleading worm signature generators using deliberate noise injection. In IEEE\\nSymposium on Security and Privacy, S& P 2006 . 15 pp.–31.\\n[135] Neehar Peri, Neal Gupta, W Ronny Huang, Liam Fowl, Chen Zhu, Soheil Feizi, Tom Goldstein, and John P Dickerson. 2020. Deep k-NN defense\\nagainst clean-label data poisoning attacks. In Eur. Conf. on Computer Vision . Springer, 55–70.\\n[136] Maura Pintor, Luca Demetrio, Angelo Sotgiu, Giovanni Manca, Ambra Demontis, Nicholas Carlini, Battista Biggio, and Fabio Roli. 2021. Indicators\\nof Attack Failure: Debugging and Improving Optimization of Adversarial Examples. arXiv preprint arXiv:2106.09947 (2021).\\n[137] Fanchao Qi, Yangyi Chen, Mukai Li, Zhiyuan Liu, and Maosong Sun. 2020. ONION: A Simple and Effective Defense Against Textual Backdoor\\nAttacks. arXiv:2011.10369 (2020).\\n[138] Ximing Qiao, Yukun Yang, and Hai Li. 2019. Defending neural backdoors via generative distribution modeling. In NeurIPS . 14004–14013.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1110253f-3229-4c95-b147-74352b4bedec', embedding=None, metadata={'page_label': '35', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 35\\n[139] Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. 2020. Policy teaching via environment poisoning: Training-time\\nadversarial attacks against reinforcement learning. In Int. Conf. on ML, ICML 2020 . PMLR, 7974–7984.\\n[140] Elan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and Zico Kolter. 2020. Certified robustness to label-flipping attacks via randomized smoothing.\\nInICML . PMLR, 8230–8241.\\n[141] Benjamin IP Rubinstein, Blaine Nelson, Ling Huang, Anthony D Joseph, Shing-hon Lau, Satish Rao, Nina Taft, and J Doug Tygar. 2009. Antidote:\\nunderstanding and defending against poisoning of anomaly detectors. In 9th ACM SIGCOMM Conf. on Internet Measurement . 1–14.\\n[142] Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. 2020. Hidden Trigger Backdoor Attacks. In AAAI . AAAI Press, 11957–11965.\\n[143] Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang. 2020. Dynamic backdoor attacks against ML models. arXiv (2020).\\n[144] Esha Sarkar, Yousif Alkindi, and Michail Maniatakos. 2020. Backdoor suppression in neural networks using input fuzzing and majority voting.\\nIEEE Design & Test 37, 2 (2020), 103–110.\\n[145] Esha Sarkar, Hadjer Benkraouda, and Michail Maniatakos. 2020. FaceHack: Triggering backdoored facial recognition Sys. using facial characteristics.\\narXive:2006.11623 (2020).\\n[146] Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. 2021. Just how toxic is data poisoning? a unified\\nbenchmark for backdoor and data poisoning attacks. In Int. Conf. on ML, ICML 2021 . PMLR, 9389–9398.\\n[147] Giorgio Severi, Jim Meyer, Scott Coull, and Alina Oprea. 2021. Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers. In\\nUSENIX Sec. Symp.\\n[148] Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. 2018. Poison Frogs! Targeted\\nClean-Label Poisoning Attacks on Neural Networks. In Advances in Neural Inf. Proc. Sys., NeurIPS 2018 . 6106–6116.\\n[149] Shawn Shan, Arjun Nitin Bhagoji, Haitao Zheng, and Ben Y Zhao. 2022. Traceback of Targeted Data Poisoning Attacks in Neural Networks. In\\nUSENIX Sec. Symp. USENIX Association.\\n[150] Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, and Daniel Ramage. 2022. Back to the Drawing Board: A Critical Evaluation of Poisoning\\nAttacks on Federated Learning. In IEEE Symposium on Security and Privacy .\\n[151] Guangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An, Qiuling Xu, Siyuan Cheng, Shiqing Ma, and Xiangyu Zhang. 2021. Backdoor Scanning\\nfor Deep Neural Networks through K-Arm Optimization. arXiv:2102.05123 (2021).\\n[152] Reza Shokri et al. 2020. Bypassing Backdoor Detection Algorithms in Deep Learning. In IEEE Euro S&P 2020 . IEEE, 175–183.\\n[153] David Solans, Battista Biggio, and Carlos Castillo. 2020. Poisoning Attacks on Algorithmic Fairness. In ECML PKDD . Springer, 162–177.\\n[154] Robin Sommer and Vern Paxson. 2010. Outside the closed world: On using ML for network intrusion detection. In IEEE S&P 2010 . IEEE, 305–316.\\n[155] Ezekiel Soremekun, Sakshi Udeshi, Sudipta Chattopadhyay, and Andreas Zeller. 2020. Exposing backdoors in robust ML models. arXiv (2020).\\n[156] Hossein Souri, Micah Goldblum, Liam Fowl, Rama Chellappa, and Tom Goldstein. 2021. Sleeper Agent: Scalable Hidden Trigger Backdoors for\\nNeural Networks Trained from Scratch. arXiv:2106.08970 (2021).\\n[157] Jacob Steinhardt, Pang Wei Koh, and Percy Liang. 2017. Certified Defenses for Data Poisoning Attacks. In Neural Inf. Proc. Sys., NIPS . 3517–3529.\\n[158] Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor Dumitras. 2018. When does ML {FAIL}? generalized transferability for\\nevasion and poisoning attacks. In USENIX Sec. Symp. 1299–1316.\\n[159] Lichao Sun, Yingtong Dou, Carl Yang, Ji Wang, Philip S Yu, Lifang He, and Bo Li. 2018. Adversarial attack and defense on graph data: A survey.\\narXiv:1812.10528 (2018).\\n[160] Mingjie Sun, Zichao Li, Chaowei Xiao, Haonan Qiu, Bhavya Kailkhura, Mingyan Liu, and Bo Li. 2021. Can Shape Structure Features Improve\\nModel Robustness under Diverse Adversarial Settings?. In IEEE CVF Int. Conf. on Computer Vision . 7526–7535.\\n[161] Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. 2019. Can you really backdoor federated learning? arXiv (2019).\\n[162] Rahim Taheri, Reza Javidan, Mohammad Shojafar, Zahra Pooranian, Ali Miri, and Mauro Conti. 2020. On defending against label flipping attacks\\non malware detection system. Neural Computing and Applications (2020), 1–20.\\n[163] Di Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang. 2021. Demon in the Variant: Statistical Analysis of {DNNs}for Robust Backdoor\\nContamination Detection. (2021), 1541–1558.\\n[164] Zhiyi Tian, Lei Cui, Jie Liang, and Shui Yu. 2022. A Comprehensive Survey on Poisoning Attacks and Countermeasures in Machine Learning.\\nComput. Surveys (2022).\\n[165] Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. 2020. Data Poisoning Attacks Against Federated Learning Sytems. In Eur.\\nSymposium on Research in Computer Security, ESORICS . Springer, 480–501.\\n[166] Antonio Torralba and Alexei A Efros. 2011. Unbiased look at dataset bias. In CVPR 2011 . IEEE, 1521–1528.\\n[167] Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart. 2016. Stealing ML Models via Prediction APIs. In USENIX Sec.\\nSymp. 601–618.\\n[168] Brandon Tran, Jerry Li, and Aleksander Mądry. 2018. Spectral signatures in backdoor attacks. In Conf. on Neural Inf. Proc. Sys., NIPS 2018 . 8011–8021.\\n[169] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. 2019. Label-consistent backdoor attacks. arXiv:1912.02771 (2019).\\n[170] Sakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell Loh, Louth Rawshan, and Sudipta Chattopadhyay. 2019. Model agnostic defence against\\nbackdoor attacks in ML. arXiv:1908.02203 (2019).\\n[171] Akshaj Kumar Veldanda, Kang Liu, Benjamin Tan, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Brendan Dolan-Gavitt, and\\nSiddharth Garg. 2021. NNoculation: Catching BadNets in the Wild. In 14th ACM Workshop on AI and Security . 49–60.\\n[172] Miguel Villarreal-Vasquez and Bharat Bhargava. 2020. Confoc: Content-focus protection against trojan attacks on neural networks. arXiv (2020).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f9a09cf2-15c8-4db0-ae9b-6de11b14d329', embedding=None, metadata={'page_label': '36', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='36 Cinà, Grosse, et al.\\n[173] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. 2019. Neural cleanse: Identifying and\\nmitigating backdoor attacks in neural networks. In IEEE Symposium on Security and Privacy, S&P 2019 . IEEE, 707–723.\\n[174] Jingkang Wang, Yang Liu, and Bo Li. 2020. Reinforcement learning with perturbed rewards. In AAAI Conf. on Art. Intell. AAAI Press, 6202–6209.\\n[175] Wenxiao Wang, Alexander J Levine, and Soheil Feizi. 2022. Improved Certified Defenses against Data Poisoning with (Deterministic) Finite\\nAggregation. In ICML . PMLR, 22769–22783.\\n[176] Maurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, and Bo Li. 2020. Rab: Provable robustness against backdoor attacks. arXiv:2003.08904 (2020).\\n[177] Jialin Wen, Benjamin Zi Hao Zhao, Minhui Xue, Alina Oprea, and Haifeng Qian. 2021. With Great Dispersion Comes Greater Resilience: Efficient\\nPoisoning Attacks and Defenses for Linear Regression Models. IEEE Trans. on Inf. Forensics and Security (2021).\\n[178] Cheng-Hsin Weng, Yan-Ting Lee, and Shan-Hung Brandon Wu. 2020. On the Trade-off between Adversarial and Backdoor Robustness. Neural Inf.\\nProc. Sys., NeurIPS (2020).\\n[179] Emily Wenger, Josephine Passananti, Arjun Nitin Bhagoji, Yuanshun Yao, Haitao Zheng, and Ben Y. Zhao. 2021. Backdoor Attacks Against Deep\\nLearning Sys. in the Physical World. IEEE/CVF Int. Conf. on Computer Vision, ICCV 2021 (2021), 6202–6211.\\n[180] Dongxian Wu and Yisen Wang. 2021. Adversarial Neuron Pruning Purifies Backdoored Deep Models. In NeurIPS .\\n[181] Zhaohan Xi, Ren Pang, Shouling Ji, and Ting Wang. 2021. Graph Backdoor. In USENIX Sec. Symp. 1523–1540.\\n[182] Zhen Xiang, David Miller, and George Kesidis. 2022. Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios. In\\nICLR,2022 .\\n[183] Zhen Xiang, David J Miller, and George Kesidis. 2019. A benchmark study of backdoor data poisoning defenses for deep neural network classifiers\\nand a novel defense. In IEEE 29th Int. Workshop on ML for Signal Proc., MLSP 2019 . IEEE, 1–6.\\n[184] Zhen Xiang, David J Miller, and George Kesidis. 2020. Detection of Backdoors in Trained Classifiers Without Access to the Training Set. IEEE\\nTrans. on Neural Networks and Learning Sys. (2020).\\n[185] Zhen Xiang, David J Miller, and George Kesidis. 2021. L-Red: Efficient Post-Training Detection of Imperceptible Backdoor Attacks Without Access\\nto the Training Set. In IEEE Int. Conf. on Acoustics, Speech and Signal Proc., ICASSP 2021 . IEEE, 3745–3749.\\n[186] Zhen Xiang, David J Miller, and George Kesidis. 2021. Reverse engineering imperceptible backdoor attacks on deep neural networks for detection\\nand training set cleansing. Computers & Security 106 (2021), 102280.\\n[187] Chaowei Xiao, Xinlei Pan, Warren He, Jian Peng, Mingjie Sun, Jinfeng Yi, Mingyan Liu, Bo Li, and Dawn Song. 2019. Characterizing attacks on\\ndeep reinforcement learning. arXiv:1907.09470 (2019).\\n[188] Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and Fabio Roli. 2015. Is Feature Selection Secure against Training Data\\nPoisoning?. In 32nd Int. Conf. on ML, ICML 2015 . JMLR, 1689–1698.\\n[189] Huang Xiao, Battista Biggio, Blaine Nelson, Han Xiao, Claudia Eckert, and Fabio Roli. 2015. Support vector machines under adversarial label\\ncontamination. Neurocomputing 160 (2015), 53–62.\\n[190] Han Xiao, Huang Xiao, and Claudia Eckert. 2012. Adversarial Label Flips Attack on Support Vector Machines. In ECAI 2012 - 20th Eur. Conf. on AI.\\nIncluding Prestigious Applications of AI, PAIS-2012 . IOS Press, 870–875.\\n[191] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. 2020. DBA: Distributed Backdoor Attacks against Federated Learning. In ICLR, 2020 . OpenReview.\\n[192] Hang Xu, Rundong Wang, Lev Raizman, and Zinovi Rabinovich. 2021. Transferable Environment Poisoning: Training-time Attack on Reinforcement\\nLearning. In 20th Int. Conf. on Autonomous Agents and MultiAgent Sys. 1398–1406.\\n[193] Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A. Gunter, and Bo Li. 2021. Detecting AI Trojans Using Meta Neural Analysis. In IEEE\\nSymposium on Security and Privacy, S&P . IEEE, 103–120.\\n[194] Chaofei Yang, Qing Wu, Hai Li, and Yiran Chen. 2017. Generative Poisoning Attack Method Against Neural Networks. CoRR abs/1703.01340 (2017).\\n[195] Yu Yang, Tian Yu Liu, and Baharan Mirzasoleiman. 2022. Not All Poisons are Created Equal: Robust Training against Data Poisoning. In ICML .\\nPMLR, 25154–25165.\\n[196] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. 2019. Latent backdoor attacks on deep neural networks. In ACM SIGSAC Conf. on\\nComputer and Communications Security, CCS 2019 . 2041–2055.\\n[197] Kota Yoshida and Takeshi Fujino. 2020. Disabling Backdoor and Identifying Poison Data by using Knowledge Distillation in Backdoor Attacks on\\nDeep Neural Networks. In 13th ACM Workshop on AI and Security . 117–127.\\n[198] Steven R Young, Derek C Rose, Thomas P Karnowski, Seung-Hwan Lim, and Robert M Patton. 2015. Optimizing deep learning hyper-parameters\\nthrough an evolutionary algorithm. In Workshop on ML in High-Performance Computing Environments . 1–5.\\n[199] Yi Zeng, Si Chen, Won Park, Zhuoqing Mao, Ming Jin, and Ruoxi Jia. 2022. Adversarial Unlearning of Backdoors via Implicit Hypergradient. In\\nICLR, 2022 .\\n[200] Yi Zeng, Han Qiu, Shangwei Guo, Tianwei Zhang, Meikang Qiu, and Bhavani Thuraisingham. 2020. DeepSweep: An Evaluation Framework for\\nMitigating DNN Backdoor Attacks using Data Augmentation. arXiv:2012.07006 (2020).\\n[201] Jiale Zhang, Junjun Chen, Di Wu, Bing Chen, and Shui Yu. 2019. Poisoning Attack in Federated Learning using Generative Adversarial Nets. In\\nIEEE Int. Conf. On Trust, Security and Privacy . IEEE, 374–380.\\n[202] Jiale Zhang, Di Wu, Chengyong Liu, and Bing Chen. 2020. Defending Poisoning Attacks in Federated Learning via Adversarial Training Method. In\\nInt. Conf. on Frontiers in Cyber Security . Springer, 83–94.\\n[203] Rui Zhang and Quanyan Zhu. 2017. A game-theoretic analysis of label flipping attacks on distributed support vector machines. In Conf. on Inf.\\nSciences and Sys., CISS 2017 . IEEE, 1–6.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c9d096a8-54e8-42b4-bafc-406b58d66031', embedding=None, metadata={'page_label': '37', 'file_name': 'survey-poisoning-attacks.pdf', 'file_path': '/content/data/survey-poisoning-attacks.pdf', 'file_type': 'application/pdf', 'file_size': 3074995, 'creation_date': '2024-05-22', 'last_modified_date': '2024-05-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Wild Patterns Reloaded 37\\n[204] Xinqiao Zhang, Huili Chen, and Farinaz Koushanfar. 2021. TAD: Trigger Approximation based Black-box Trojan Detection for AI. arXiv (2021).\\n[205] Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. 2020. Adaptive reward-poisoning attacks against reinforcement learning. In Int. Conf.\\non ML, ICML 2020 . PMLR, 11225–11234.\\n[206] Xinyang Zhang, Zheng Zhang, and Ting Wang. 2020. Trojaning Language Models for Fun and Profit. CoRR abs/2008.00312 (2020).\\n[207] Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. 2021. Backdoor Attacks to Graph Neural Networks. In SACMAT ’21: The 26th\\nACM Symposium on Access Control Models and Technologies . ACM, 15–26.\\n[208] Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, and Xue Lin. 2020. Bridging mode connectivity in loss landscapes and\\nadversarial robustness. In ICLR, 2021 .\\n[209] Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. 2020. Clean-Label Backdoor Attacks on Video Recognition\\nModels. In IEEE/CVF Int. Conf. on Computer Vision, ICCV 2020 . IEEE, 14431–14440.\\n[210] Ying Zhao, Junjun Chen, Jiale Zhang, Di Wu, Jian Teng, and Shui Yu. 2019. PDGAN: a novel poisoning defense method in federated learning using\\ngenerative adversarial network. In International Conference on Algorithms and Architectures for Parallel Processing . Springer, 595–609.\\n[211] Haoti Zhong, Cong Liao, Anna Cinzia Squicciarini, Sencun Zhu, and David J. Miller. 2020. Backdoor Embedding in Convolutional Neural Network\\nModels via Invisible Perturbation. In CODASPY ’20: Tenth ACM Conf. on Data and Application Security and Privacy 2020 . ACM, 97–108.\\n[212] Chen Zhu, W. Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2019. Transferable Clean-Label Poisoning Attacks\\non Deep Neural Nets. In 36th Int. Conf. on ML, ICML 2019 . PMLR, 7614–7623.\\n[213] Liuwan Zhu, Rui Ning, Cong Wang, Chunsheng Xin, and Hongyi Wu. 2020. Gangsweep: Sweep out neural backdoors by gan. In 28th ACM Int.\\nConf. on Multimedia . 3173–3181.\\n[214] Liuwan Zhu, Rui Ning, Chunsheng Xin, Chonggang Wang, and Hongyi Wu. 2021. CLEAR: Clean-Up Sample-Targeted Backdoor in Neural\\nNetworks. In IEEE/CVF Int. Conf. on Computer Vision, ICCV 2021 . 16453–16462.\\n[215] Daniel Zügner and Stephan Günnemann. 2019. Adversarial Attacks on Graph Neural Networks via Meta Learning. In ICLR, 2019 . OpenReview.net.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are an AI QA assistant. Please answer all questions as accurately\n",
        "as possible based on the insstructions and context provided.\n",
        "\"\"\"\n",
        "# Defaualt format suppoted by llama2\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISSTANT|>\")"
      ],
      "metadata": {
        "id": "rQzZXs7eLB5o"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGGjjEMFL7mf",
        "outputId": "9f65c4c0-51a2-4d1f-dd8f-18a0644ec07e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|',\n",
              " '    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|',\n",
              " '    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|',\n",
              " '    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|',\n",
              " '    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|',\n",
              " '',\n",
              " '    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .',\n",
              " 'Enter your token (input will not be visible): ',\n",
              " 'Add token as git credential? (Y/n) y',\n",
              " 'Token is valid (permission: read).',\n",
              " '\\x1b[1m\\x1b[31mCannot authenticate through git-credential as no helper is defined on your machine.',\n",
              " 'You might have to re-authenticate when pushing to the Hugging Face Hub.',\n",
              " \"Run the following command in your terminal in case you want to set the 'store' credential helper as default.\",\n",
              " '',\n",
              " 'git config --global credential.helper store',\n",
              " '',\n",
              " 'Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\\x1b[0m',\n",
              " 'Token has not been saved to git credential helper.',\n",
              " 'Your token has been saved to /root/.cache/huggingface/token',\n",
              " 'Login successful']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={'temperature':0.0, 'do_sample':False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name='meta-llama/Llama-2-7b-chat-hf',\n",
        "    model_name='meta-llama/Llama-2-7b-chat-hf',\n",
        "    device_map='auto',\n",
        "    model_kwargs={'torch_dtype':torch.float16,\"load_in_8bit\":True}\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "FmSRLOusMPeZ",
        "outputId": "ff7b4905-b33c-4aad-f030-4c0dfdac17ec"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n403 Client Error. (Request ID: Root=1-664de203-1ac1fdec50fd5af03f96f351;87134ccb-0738-4ac1-b512-472997a5ef07)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\nYour request to access model meta-llama/Llama-2-7b-chat-hf is awaiting a review from the repo authors.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1222\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hf_file_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1723\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEntryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhttp_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1646\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    320\u001b[0m             )\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGatedRepoError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 403 Client Error. (Request ID: Root=1-664de203-1ac1fdec50fd5af03f96f351;87134ccb-0738-4ac1-b512-472997a5ef07)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\nYour request to access model meta-llama/Llama-2-7b-chat-hf is awaiting a review from the repo authors.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-c4b7a415ce2d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m llm = HuggingFaceLLM(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcontext_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'temperature'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'do_sample'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/llms/huggingface/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, context_window, max_new_tokens, query_wrapper_prompt, tokenizer_name, model_name, model, tokenizer, device_map, stopping_ids, tokenizer_kwargs, tokenizer_outputs_to_remove, model_kwargs, generate_kwargs, is_chat_model, callback_manager, system_prompt, messages_to_prompt, completion_to_prompt, pydantic_program_mode, output_parser)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;34m\"\"\"Initialize params.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         self._model = model or AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    524\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    690\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n403 Client Error. (Request ID: Root=1-664de203-1ac1fdec50fd5af03f96f351;87134ccb-0738-4ac1-b512-472997a5ef07)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\nYour request to access model meta-llama/Llama-2-7b-chat-hf is awaiting a review from the repo authors."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BBs5ltdtOTq8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}